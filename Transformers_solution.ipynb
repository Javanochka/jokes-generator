{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "F1MP6D-ioE-y",
    "outputId": "3f515fa3-ab67-4733-d9ba-5baf05f6feaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as opt\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZbmUctl3_eQ"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "bptt = 40\n",
    "\n",
    "freq_list = Counter()\n",
    "word_to_id_dict = {}\n",
    "id_to_word_dict = {}\n",
    "\n",
    "word_to_id_dict[\"<pad>\"] = 0\n",
    "id_to_word_dict[0] = \"<pad>\"\n",
    "\n",
    "next_id = 1\n",
    "train = []\n",
    "\n",
    "# Tokenize sentence\n",
    "def generate_dataset(filename, word_to_id_dict, id_to_word_dict, next_id):\n",
    "  result = []\n",
    "  lang_model = spacy.load('en', disable=['tagger', 'parser', 'ner'])\n",
    "  was = set()\n",
    "  with open(filename, 'r') as file:\n",
    "    for i, sentence in enumerate(file):\n",
    "      sentence = sentence.lower().rstrip('\\n')\n",
    "      sentence = [tok.text for tok in lang_model.tokenizer(sentence) if not tok.text.isspace()]\n",
    "      lemmzed = \" \".join(sorted([wordnet_lemmatizer.lemmatize(x) for x in sentence]))\n",
    "      if lemmzed in was:\n",
    "          continue\n",
    "      was.add(lemmzed)\n",
    "      sentence = [\"<sos>\"] + sentence\n",
    "      if len(sentence) > bptt or len(sentence) < 7:\n",
    "        continue\n",
    "      sentence += ['<pad>'] * max(0, bptt - len(sentence))\n",
    "      \n",
    "      for word in sentence:\n",
    "        if not word in word_to_id_dict:\n",
    "          word_to_id_dict[word] = next_id\n",
    "          id_to_word_dict[next_id] = word\n",
    "          next_id += 1\n",
    "      \n",
    "      sentence = [word_to_id_dict[word] for word in sentence]\n",
    "      freq_list.update(sentence)\n",
    "      \n",
    "      sentence_segments = [np.array(sentence[i:i + bptt]) for i in range(len(sentence) - bptt + 1)]\n",
    "\n",
    "      result.append(sentence_segments)\n",
    "  return result, next_id\n",
    "\n",
    "train, next_id = generate_dataset(path_to_data + 'train.txt', word_to_id_dict, id_to_word_dict, next_id)\n",
    "validate, next_id = generate_dataset(path_to_data + 'valid.txt', word_to_id_dict, id_to_word_dict, next_id)\n",
    "test, next_id = generate_dataset(path_to_data + 'test.txt', word_to_id_dict, id_to_word_dict, next_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ToRK5FY_1e-_"
   },
   "outputs": [],
   "source": [
    "train = np.array(list(map(np.array, train)))\n",
    "np.random.shuffle(train)\n",
    "test = np.array(sum(test, []))[:100]\n",
    "validate = np.array(sum(validate, []))[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "aCujpHVHl_17",
    "outputId": "a0e4009e-e56b-4e26-a623-c74b779528b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5401, -0.9919,  0.0966,  ..., -0.2318,  0.6757, -0.7559],\n",
      "        [-0.4823,  0.6175,  0.9280,  ..., -0.3485, -0.0172,  0.4641],\n",
      "        [ 0.4532,  0.0598, -0.1058,  ...,  0.5324, -0.2510,  0.6255],\n",
      "        ...,\n",
      "        [ 1.2031, -0.4003,  0.0740,  ...,  1.3262, -0.3325,  0.8198],\n",
      "        [-0.4364,  0.6623,  0.9707,  ...,  0.0869,  0.9431,  0.8322],\n",
      "        [ 0.7192, -0.9104,  0.9343,  ..., -0.1853,  0.3997,  0.3091]])\n"
     ]
    }
   ],
   "source": [
    "dim = 50\n",
    "\n",
    "word_to_glove = {}\n",
    "with open(\"/content/drive/My Drive/Colab Notebooks/glove.6B.50d.txt\", 'r') as glove:\n",
    "  for line in glove:\n",
    "    tokens = line.rstrip().split()\n",
    "    word = tokens[0]\n",
    "    if word not in word_to_glove and word in word_to_id_dict:\n",
    "      emb = torch.from_numpy(np.array([list(map(float, tokens[1:]))])).float()\n",
    "      word_to_glove[word] = emb\n",
    "\n",
    "emb_tensor = torch.tensor([]).float()\n",
    "for i, word in id_to_word_dict.items():\n",
    "  if word in word_to_glove:\n",
    "    emb = word_to_glove[word]\n",
    "  else:\n",
    "    emb = torch.from_numpy(np.random.uniform(-1., 1., size=dim).reshape(1, -1)).float()\n",
    "  emb_tensor = torch.cat((emb_tensor, emb))\n",
    "\n",
    "print(emb_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KvCDRJoNks4Y",
    "outputId": "8765282c-cc23-42b9-80c3-c17e2e2f1946"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47167"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "UqANZMjbsXDh",
    "outputId": "6540a068-aae4-4bfd-ba45-cfc83573154c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30234, 50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30234"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(emb_tensor.shape)\n",
    "len(word_to_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2SrqtyhrmZM"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.2, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EKvhol6Kl9-b"
   },
   "outputs": [],
   "source": [
    "class TransformetGenerator(nn.Module):\n",
    "  def __init__(self, emb_size, src_dict_len, tgt_dict_len, max_len, embs=None):\n",
    "    super(TransformetGenerator, self).__init__()\n",
    "    self.src_emb = nn.Embedding(src_dict_len, emb_size)\n",
    "    if embs is not None:\n",
    "      self.src_emb.from_pretrained(embs, freeze=False)\n",
    "    #self.tgt_emb = nn.Embedding(tgt_dict_len, emb_size)\n",
    "\n",
    "    self.d_model = emb_size\n",
    "    self.pos_enc = PositionalEncoding(emb_size, max_len=max_len)\n",
    "    self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(emb_size, 10, dropout=0.1), 6)\n",
    "    self.linear = nn.Linear(emb_size, tgt_dict_len)\n",
    "    self.softmax = nn.Softmax(dim=2)\n",
    "    self.src_mask = None\n",
    "\n",
    "    \"\"\"\n",
    "    initrange = 0.1\n",
    "    self.src_emb.weight.data.uniform_(-initrange, initrange)\n",
    "    #self.tgt_emb.weight.data.uniform_(-initrange, initrange)\n",
    "    self.linear.bias.data.zero_()\n",
    "    self.linear.weight.data.uniform_(-initrange, initrange)\"\"\"\n",
    "\n",
    "  def forward(self, src, tgt):\n",
    "    # src & tgt dimensions (batch_size, words)\n",
    "\n",
    "    src_padding_mask = (src == 0).to(src.device)\n",
    "    #tgt_padding_mask = (tgt == 0).to(src.device)\n",
    "    memory_mask = src_padding_mask.clone().to(src.device)\n",
    "\n",
    "    #tgt_mask = (torch.triu(torch.ones(tgt.shape[1], tgt.shape[1])) == 1).float()\n",
    "    #tgt_mask = tgt_mask.masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0)).transpose(0, 1).to(src.device)\n",
    "    \n",
    "    if self.src_mask is None or self.src_mask.shape[0] != src.shape[1]:\n",
    "      src_mask = (torch.triu(torch.ones(src.shape[1], src.shape[1])) == 1).float()\n",
    "      self.src_mask = src_mask.masked_fill(src_mask == 0, float('-inf')).masked_fill(src_mask == 1, float(0.0)).transpose(0, 1).to(src.device)\n",
    "    \n",
    "    src = src.transpose(1, 0)\n",
    "    #tgt = tgt.transpose(1, 0)\n",
    "    # src & tgt dimensions (words, batch_size)\n",
    "    src = self.src_emb(src)\n",
    "    #tgt = self.tgt_emb(tgt)\n",
    "    # src & tgt dimensions (words, batch_size, embs)\n",
    "    src = self.pos_enc(src * math.sqrt(self.d_model))\n",
    "    #tgt = self.pos_enc(src * math.sqrt(self.d_model))\n",
    "    output = self.transformer(src, #tgt,\n",
    "                              mask=self.src_mask,\n",
    "    #                          tgt_mask=tgt_mask,\n",
    "    #                          memory_key_padding_mask=memory_mask,\n",
    "                              src_key_padding_mask=src_padding_mask,\n",
    "    #                          tgt_key_padding_mask=tgt_padding_mask\n",
    "                              )\n",
    "    #output shape: (words, batch_size, emb)\n",
    "    output = output.transpose(1, 0)\n",
    "    # output shape: (batch_size, words, emb)\n",
    "    # embeddings to probabilities:\n",
    "    output = self.linear(output)\n",
    "    output = self.softmax(output)\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UagjyE4uiiGz"
   },
   "outputs": [],
   "source": [
    "def learn(device, model, loss_function, epoches, batch_size, optimizer):\n",
    "    train_losses = []\n",
    "    validate_losses = []\n",
    "    iters_per_epoch = len(train)\n",
    "\n",
    "    v = torch.from_numpy(validate).to(device)\n",
    "    t = torch.from_numpy(test).to(device)\n",
    "    \n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        np.random.shuffle(train)\n",
    "        optimizer.zero_grad()\n",
    "        # np.random.shuffle(train)\n",
    "        cnt = 0\n",
    "        while True:\n",
    "            batch = np.array([])\n",
    "            while batch.shape[0] < batch_size:\n",
    "              if cnt >= len(train):\n",
    "                break\n",
    "              if batch.shape[0]:\n",
    "                batch = np.concatenate((batch, train[cnt]))\n",
    "              else:\n",
    "                batch = train[cnt]\n",
    "              cnt += 1\n",
    "            if batch.shape[0] < batch_size:\n",
    "              break\n",
    "\n",
    "            batch = torch.from_numpy(batch).to(device)\n",
    "\n",
    "            train_error    = loss_function(model.forward(batch, batch), batch)\n",
    "            validate_error = loss_function(model.forward(v, v), v)\n",
    "\n",
    "            train_losses.append(float(train_error))\n",
    "            validate_losses.append(float(validate_error))\n",
    "\n",
    "            print(\"Epoch {}/{}. Iteration {}/{} Losses: train: {}, validate: {}\"\n",
    "              .format(epoch, epoches, cnt, iters_per_epoch, train_losses[-1], validate_losses[-1]))\n",
    "\n",
    "            train_error.backward()   \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    test_loss = float(loss_function(model.forward(t, t), t))\n",
    "    return train_losses, validate_losses, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XiIMKxZ2vg2S",
    "outputId": "c93229a5-0620-4e36-e30c-5287b1f627b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10. Iteration 100/47167 Losses: train: 10.16590404510498, validate: 10.13668441772461\n",
      "Epoch 0/10. Iteration 200/47167 Losses: train: 8.677658081054688, validate: 8.64410400390625\n",
      "Epoch 0/10. Iteration 300/47167 Losses: train: 8.289255142211914, validate: 8.380843162536621\n",
      "Epoch 0/10. Iteration 400/47167 Losses: train: 8.11864948272705, validate: 8.28111457824707\n",
      "Epoch 0/10. Iteration 500/47167 Losses: train: 8.062396049499512, validate: 8.218608856201172\n",
      "Epoch 0/10. Iteration 600/47167 Losses: train: 8.1345853805542, validate: 8.15282154083252\n",
      "Epoch 0/10. Iteration 700/47167 Losses: train: 8.057779312133789, validate: 8.087459564208984\n",
      "Epoch 0/10. Iteration 800/47167 Losses: train: 8.002664566040039, validate: 8.012243270874023\n",
      "Epoch 0/10. Iteration 900/47167 Losses: train: 7.971149444580078, validate: 7.936307430267334\n",
      "Epoch 0/10. Iteration 1000/47167 Losses: train: 7.8749189376831055, validate: 7.860538005828857\n",
      "Epoch 0/10. Iteration 1100/47167 Losses: train: 7.752090930938721, validate: 7.781806468963623\n",
      "Epoch 0/10. Iteration 1200/47167 Losses: train: 7.6877241134643555, validate: 7.70943546295166\n",
      "Epoch 0/10. Iteration 1300/47167 Losses: train: 7.747360706329346, validate: 7.628853797912598\n",
      "Epoch 0/10. Iteration 1400/47167 Losses: train: 7.378829002380371, validate: 7.551845550537109\n",
      "Epoch 0/10. Iteration 1500/47167 Losses: train: 7.48592472076416, validate: 7.474240779876709\n",
      "Epoch 0/10. Iteration 1600/47167 Losses: train: 7.404122352600098, validate: 7.4014058113098145\n",
      "Epoch 0/10. Iteration 1700/47167 Losses: train: 7.095108509063721, validate: 7.319614410400391\n",
      "Epoch 0/10. Iteration 1800/47167 Losses: train: 7.081836700439453, validate: 7.242586612701416\n",
      "Epoch 0/10. Iteration 1900/47167 Losses: train: 7.207158088684082, validate: 7.162317752838135\n",
      "Epoch 0/10. Iteration 2000/47167 Losses: train: 7.083165168762207, validate: 7.086872100830078\n",
      "Epoch 0/10. Iteration 2100/47167 Losses: train: 7.091085910797119, validate: 7.004824161529541\n",
      "Epoch 0/10. Iteration 2200/47167 Losses: train: 6.758927822113037, validate: 6.924500465393066\n",
      "Epoch 0/10. Iteration 2300/47167 Losses: train: 6.728946208953857, validate: 6.846467971801758\n",
      "Epoch 0/10. Iteration 2400/47167 Losses: train: 6.60978364944458, validate: 6.761984825134277\n",
      "Epoch 0/10. Iteration 2500/47167 Losses: train: 6.450561046600342, validate: 6.686143398284912\n",
      "Epoch 0/10. Iteration 2600/47167 Losses: train: 6.6550116539001465, validate: 6.60472297668457\n",
      "Epoch 0/10. Iteration 2700/47167 Losses: train: 6.460608005523682, validate: 6.527917861938477\n",
      "Epoch 0/10. Iteration 2800/47167 Losses: train: 6.424291133880615, validate: 6.447412490844727\n",
      "Epoch 0/10. Iteration 2900/47167 Losses: train: 6.279230117797852, validate: 6.368519306182861\n",
      "Epoch 0/10. Iteration 3000/47167 Losses: train: 6.150541305541992, validate: 6.287095546722412\n",
      "Epoch 0/10. Iteration 3100/47167 Losses: train: 6.042311191558838, validate: 6.2117695808410645\n",
      "Epoch 0/10. Iteration 3200/47167 Losses: train: 6.060900688171387, validate: 6.133230209350586\n",
      "Epoch 0/10. Iteration 3300/47167 Losses: train: 6.002904415130615, validate: 6.054577827453613\n",
      "Epoch 0/10. Iteration 3400/47167 Losses: train: 6.070122241973877, validate: 5.981205940246582\n",
      "Epoch 0/10. Iteration 3500/47167 Losses: train: 5.779325008392334, validate: 5.901970863342285\n",
      "Epoch 0/10. Iteration 3600/47167 Losses: train: 5.720054626464844, validate: 5.830345153808594\n",
      "Epoch 0/10. Iteration 3700/47167 Losses: train: 5.76239013671875, validate: 5.75298547744751\n",
      "Epoch 0/10. Iteration 3800/47167 Losses: train: 5.407153129577637, validate: 5.676334381103516\n",
      "Epoch 0/10. Iteration 3900/47167 Losses: train: 5.388890266418457, validate: 5.609251499176025\n",
      "Epoch 0/10. Iteration 4000/47167 Losses: train: 5.516469955444336, validate: 5.541967868804932\n",
      "Epoch 0/10. Iteration 4100/47167 Losses: train: 5.278130531311035, validate: 5.469362258911133\n",
      "Epoch 0/10. Iteration 4200/47167 Losses: train: 5.153110980987549, validate: 5.404964447021484\n",
      "Epoch 0/10. Iteration 4300/47167 Losses: train: 5.281199932098389, validate: 5.3378400802612305\n",
      "Epoch 0/10. Iteration 4400/47167 Losses: train: 5.208110332489014, validate: 5.27218770980835\n",
      "Epoch 0/10. Iteration 4500/47167 Losses: train: 5.114089488983154, validate: 5.209860801696777\n",
      "Epoch 0/10. Iteration 4600/47167 Losses: train: 4.854280948638916, validate: 5.14853572845459\n",
      "Epoch 0/10. Iteration 4700/47167 Losses: train: 4.91768741607666, validate: 5.092586517333984\n",
      "Epoch 0/10. Iteration 4800/47167 Losses: train: 4.888492584228516, validate: 5.035358428955078\n",
      "Epoch 0/10. Iteration 4900/47167 Losses: train: 4.857848167419434, validate: 4.9813127517700195\n",
      "Epoch 0/10. Iteration 5000/47167 Losses: train: 4.867858409881592, validate: 4.929859638214111\n",
      "Epoch 0/10. Iteration 5100/47167 Losses: train: 4.6901445388793945, validate: 4.87734317779541\n",
      "Epoch 0/10. Iteration 5200/47167 Losses: train: 4.695240497589111, validate: 4.827787399291992\n",
      "Epoch 0/10. Iteration 5300/47167 Losses: train: 4.687886714935303, validate: 4.780147075653076\n",
      "Epoch 0/10. Iteration 5400/47167 Losses: train: 4.721547603607178, validate: 4.7367095947265625\n",
      "Epoch 0/10. Iteration 5500/47167 Losses: train: 4.541807651519775, validate: 4.693414688110352\n",
      "Epoch 0/10. Iteration 5600/47167 Losses: train: 4.481135368347168, validate: 4.649959564208984\n",
      "Epoch 0/10. Iteration 5700/47167 Losses: train: 4.509282112121582, validate: 4.613095283508301\n",
      "Epoch 0/10. Iteration 5800/47167 Losses: train: 4.408055782318115, validate: 4.573882579803467\n",
      "Epoch 0/10. Iteration 5900/47167 Losses: train: 4.389801025390625, validate: 4.532525062561035\n",
      "Epoch 0/10. Iteration 6000/47167 Losses: train: 4.543361186981201, validate: 4.490379810333252\n",
      "Epoch 0/10. Iteration 6100/47167 Losses: train: 4.2326741218566895, validate: 4.443136692047119\n",
      "Epoch 0/10. Iteration 6200/47167 Losses: train: 4.411803245544434, validate: 4.397150039672852\n",
      "Epoch 0/10. Iteration 6300/47167 Losses: train: 4.21486759185791, validate: 4.351770401000977\n",
      "Epoch 0/10. Iteration 6400/47167 Losses: train: 4.144976615905762, validate: 4.322187900543213\n",
      "Epoch 0/10. Iteration 6500/47167 Losses: train: 4.206482410430908, validate: 4.295199394226074\n",
      "Epoch 0/10. Iteration 6600/47167 Losses: train: 4.115783214569092, validate: 4.261316299438477\n",
      "Epoch 0/10. Iteration 6700/47167 Losses: train: 3.9756085872650146, validate: 4.225586891174316\n",
      "Epoch 0/10. Iteration 6800/47167 Losses: train: 3.9088516235351562, validate: 4.199020862579346\n",
      "Epoch 0/10. Iteration 6900/47167 Losses: train: 3.92370867729187, validate: 4.168710231781006\n",
      "Epoch 0/10. Iteration 7000/47167 Losses: train: 3.8766367435455322, validate: 4.144133567810059\n",
      "Epoch 0/10. Iteration 7100/47167 Losses: train: 3.9879417419433594, validate: 4.1203107833862305\n",
      "Epoch 0/10. Iteration 7200/47167 Losses: train: 3.6684751510620117, validate: 4.09236478805542\n",
      "Epoch 0/10. Iteration 7300/47167 Losses: train: 3.8959543704986572, validate: 4.0724077224731445\n",
      "Epoch 0/10. Iteration 7400/47167 Losses: train: 3.979090690612793, validate: 4.050457000732422\n",
      "Epoch 0/10. Iteration 7500/47167 Losses: train: 3.699430465698242, validate: 4.031452178955078\n",
      "Epoch 0/10. Iteration 7600/47167 Losses: train: 3.8113760948181152, validate: 4.013115882873535\n",
      "Epoch 0/10. Iteration 7700/47167 Losses: train: 3.8312811851501465, validate: 3.996661901473999\n",
      "Epoch 0/10. Iteration 7800/47167 Losses: train: 3.8982980251312256, validate: 3.9797940254211426\n",
      "Epoch 0/10. Iteration 7900/47167 Losses: train: 3.88442325592041, validate: 3.9608967304229736\n",
      "Epoch 0/10. Iteration 8000/47167 Losses: train: 3.6644299030303955, validate: 3.948359966278076\n",
      "Epoch 0/10. Iteration 8100/47167 Losses: train: 3.906860828399658, validate: 3.9286019802093506\n",
      "Epoch 0/10. Iteration 8200/47167 Losses: train: 3.683380126953125, validate: 3.916931629180908\n",
      "Epoch 0/10. Iteration 8300/47167 Losses: train: 3.841912031173706, validate: 3.9024202823638916\n",
      "Epoch 0/10. Iteration 8400/47167 Losses: train: 3.720500946044922, validate: 3.888960599899292\n",
      "Epoch 0/10. Iteration 8500/47167 Losses: train: 3.9734485149383545, validate: 3.8789992332458496\n",
      "Epoch 0/10. Iteration 8600/47167 Losses: train: 3.8864078521728516, validate: 3.8652713298797607\n",
      "Epoch 0/10. Iteration 8700/47167 Losses: train: 3.7834150791168213, validate: 3.8543310165405273\n",
      "Epoch 0/10. Iteration 8800/47167 Losses: train: 3.775830030441284, validate: 3.843668222427368\n",
      "Epoch 0/10. Iteration 8900/47167 Losses: train: 3.7952725887298584, validate: 3.8297536373138428\n",
      "Epoch 0/10. Iteration 9000/47167 Losses: train: 3.8932783603668213, validate: 3.821272373199463\n",
      "Epoch 0/10. Iteration 9100/47167 Losses: train: 3.527909278869629, validate: 3.811572313308716\n",
      "Epoch 0/10. Iteration 9200/47167 Losses: train: 3.6066172122955322, validate: 3.802504301071167\n",
      "Epoch 0/10. Iteration 9300/47167 Losses: train: 3.57423996925354, validate: 3.791125535964966\n",
      "Epoch 0/10. Iteration 9400/47167 Losses: train: 3.7455997467041016, validate: 3.7831943035125732\n",
      "Epoch 0/10. Iteration 9500/47167 Losses: train: 3.5835790634155273, validate: 3.7747769355773926\n",
      "Epoch 0/10. Iteration 9600/47167 Losses: train: 3.640812635421753, validate: 3.7650930881500244\n",
      "Epoch 0/10. Iteration 9700/47167 Losses: train: 3.577732801437378, validate: 3.7566349506378174\n",
      "Epoch 0/10. Iteration 9800/47167 Losses: train: 3.370108127593994, validate: 3.750452995300293\n",
      "Epoch 0/10. Iteration 9900/47167 Losses: train: 3.5959341526031494, validate: 3.7395384311676025\n",
      "Epoch 0/10. Iteration 10000/47167 Losses: train: 3.4363818168640137, validate: 3.733006477355957\n",
      "Epoch 0/10. Iteration 10100/47167 Losses: train: 4.0457444190979, validate: 3.7255218029022217\n",
      "Epoch 0/10. Iteration 10200/47167 Losses: train: 3.3874170780181885, validate: 3.721062421798706\n",
      "Epoch 0/10. Iteration 10300/47167 Losses: train: 3.529326915740967, validate: 3.712097406387329\n",
      "Epoch 0/10. Iteration 10400/47167 Losses: train: 3.8324954509735107, validate: 3.706719398498535\n",
      "Epoch 0/10. Iteration 10500/47167 Losses: train: 3.6598219871520996, validate: 3.6972973346710205\n",
      "Epoch 0/10. Iteration 10600/47167 Losses: train: 3.4446189403533936, validate: 3.6939234733581543\n",
      "Epoch 0/10. Iteration 10700/47167 Losses: train: 3.482077121734619, validate: 3.6900036334991455\n",
      "Epoch 0/10. Iteration 10800/47167 Losses: train: 3.5156145095825195, validate: 3.680659532546997\n",
      "Epoch 0/10. Iteration 10900/47167 Losses: train: 3.446962594985962, validate: 3.6773016452789307\n",
      "Epoch 0/10. Iteration 11000/47167 Losses: train: 3.6047494411468506, validate: 3.6693902015686035\n",
      "Epoch 0/10. Iteration 11100/47167 Losses: train: 3.3679230213165283, validate: 3.666672468185425\n",
      "Epoch 0/10. Iteration 11200/47167 Losses: train: 3.424684762954712, validate: 3.6618130207061768\n",
      "Epoch 0/10. Iteration 11300/47167 Losses: train: 3.6606385707855225, validate: 3.6555540561676025\n",
      "Epoch 0/10. Iteration 11400/47167 Losses: train: 3.4393250942230225, validate: 3.650519847869873\n",
      "Epoch 0/10. Iteration 11500/47167 Losses: train: 3.368236541748047, validate: 3.6469919681549072\n",
      "Epoch 0/10. Iteration 11600/47167 Losses: train: 3.744788885116577, validate: 3.6424646377563477\n",
      "Epoch 0/10. Iteration 11700/47167 Losses: train: 3.547436237335205, validate: 3.63861346244812\n",
      "Epoch 0/10. Iteration 11800/47167 Losses: train: 3.4081385135650635, validate: 3.635565996170044\n",
      "Epoch 0/10. Iteration 11900/47167 Losses: train: 3.361004590988159, validate: 3.630253314971924\n",
      "Epoch 0/10. Iteration 12000/47167 Losses: train: 3.337223529815674, validate: 3.628082275390625\n",
      "Epoch 0/10. Iteration 12100/47167 Losses: train: 3.5545132160186768, validate: 3.6247830390930176\n",
      "Epoch 0/10. Iteration 12200/47167 Losses: train: 3.4314353466033936, validate: 3.620556354522705\n",
      "Epoch 0/10. Iteration 12300/47167 Losses: train: 3.4927830696105957, validate: 3.6150755882263184\n",
      "Epoch 0/10. Iteration 12400/47167 Losses: train: 3.561511278152466, validate: 3.6139867305755615\n",
      "Epoch 0/10. Iteration 12500/47167 Losses: train: 3.235978603363037, validate: 3.6098270416259766\n",
      "Epoch 0/10. Iteration 12600/47167 Losses: train: 3.5472817420959473, validate: 3.602013349533081\n",
      "Epoch 0/10. Iteration 12700/47167 Losses: train: 3.4123265743255615, validate: 3.6047983169555664\n",
      "Epoch 0/10. Iteration 12800/47167 Losses: train: 3.7003602981567383, validate: 3.60199236869812\n",
      "Epoch 0/10. Iteration 12900/47167 Losses: train: 3.5513105392456055, validate: 3.595179557800293\n",
      "Epoch 0/10. Iteration 13000/47167 Losses: train: 3.4274654388427734, validate: 3.59456205368042\n",
      "Epoch 0/10. Iteration 13100/47167 Losses: train: 3.3385260105133057, validate: 3.592351198196411\n",
      "Epoch 0/10. Iteration 13200/47167 Losses: train: 3.4860801696777344, validate: 3.585860013961792\n",
      "Epoch 0/10. Iteration 13300/47167 Losses: train: 3.4601402282714844, validate: 3.5832300186157227\n",
      "Epoch 0/10. Iteration 13400/47167 Losses: train: 3.56022572517395, validate: 3.583225727081299\n",
      "Epoch 0/10. Iteration 13500/47167 Losses: train: 3.3804445266723633, validate: 3.576677083969116\n",
      "Epoch 0/10. Iteration 13600/47167 Losses: train: 3.536057472229004, validate: 3.576213836669922\n",
      "Epoch 0/10. Iteration 13700/47167 Losses: train: 3.5355618000030518, validate: 3.5673582553863525\n",
      "Epoch 0/10. Iteration 13800/47167 Losses: train: 3.4733335971832275, validate: 3.5697362422943115\n",
      "Epoch 0/10. Iteration 13900/47167 Losses: train: 3.4555866718292236, validate: 3.564211845397949\n",
      "Epoch 0/10. Iteration 14000/47167 Losses: train: 3.4581737518310547, validate: 3.564096212387085\n",
      "Epoch 0/10. Iteration 14100/47167 Losses: train: 3.494133710861206, validate: 3.5593972206115723\n",
      "Epoch 0/10. Iteration 14200/47167 Losses: train: 3.495598554611206, validate: 3.559119701385498\n",
      "Epoch 0/10. Iteration 14300/47167 Losses: train: 3.3821640014648438, validate: 3.5590109825134277\n",
      "Epoch 0/10. Iteration 14400/47167 Losses: train: 3.453967809677124, validate: 3.559197187423706\n",
      "Epoch 0/10. Iteration 14500/47167 Losses: train: 3.527367115020752, validate: 3.5552620887756348\n",
      "Epoch 0/10. Iteration 14600/47167 Losses: train: 3.517756223678589, validate: 3.55014967918396\n",
      "Epoch 0/10. Iteration 14700/47167 Losses: train: 3.5355117321014404, validate: 3.5483028888702393\n",
      "Epoch 0/10. Iteration 14800/47167 Losses: train: 3.3561770915985107, validate: 3.5451900959014893\n",
      "Epoch 0/10. Iteration 14900/47167 Losses: train: 3.429738759994507, validate: 3.5413050651550293\n",
      "Epoch 0/10. Iteration 15000/47167 Losses: train: 3.2696878910064697, validate: 3.5423014163970947\n",
      "Epoch 0/10. Iteration 15100/47167 Losses: train: 3.5506131649017334, validate: 3.539395809173584\n",
      "Epoch 0/10. Iteration 15200/47167 Losses: train: 3.4803946018218994, validate: 3.533454656600952\n",
      "Epoch 0/10. Iteration 15300/47167 Losses: train: 3.530263662338257, validate: 3.5324342250823975\n",
      "Epoch 0/10. Iteration 15400/47167 Losses: train: 3.6386237144470215, validate: 3.5351815223693848\n",
      "Epoch 0/10. Iteration 15500/47167 Losses: train: 3.583893299102783, validate: 3.531129837036133\n",
      "Epoch 0/10. Iteration 15600/47167 Losses: train: 3.337946653366089, validate: 3.534508228302002\n",
      "Epoch 0/10. Iteration 15700/47167 Losses: train: 3.2928218841552734, validate: 3.526373863220215\n",
      "Epoch 0/10. Iteration 15800/47167 Losses: train: 3.3590855598449707, validate: 3.53662109375\n",
      "Epoch 0/10. Iteration 15900/47167 Losses: train: 3.4635839462280273, validate: 3.524097204208374\n",
      "Epoch 0/10. Iteration 16000/47167 Losses: train: 3.423626661300659, validate: 3.5275542736053467\n",
      "Epoch 0/10. Iteration 16100/47167 Losses: train: 3.5892088413238525, validate: 3.522095203399658\n",
      "Epoch 0/10. Iteration 16200/47167 Losses: train: 3.28586483001709, validate: 3.5219388008117676\n",
      "Epoch 0/10. Iteration 16300/47167 Losses: train: 3.2612130641937256, validate: 3.5194287300109863\n",
      "Epoch 0/10. Iteration 16400/47167 Losses: train: 3.564990997314453, validate: 3.518157958984375\n",
      "Epoch 0/10. Iteration 16500/47167 Losses: train: 3.318753480911255, validate: 3.5141239166259766\n",
      "Epoch 0/10. Iteration 16600/47167 Losses: train: 3.411334276199341, validate: 3.5131144523620605\n",
      "Epoch 0/10. Iteration 16700/47167 Losses: train: 3.429739236831665, validate: 3.50883412361145\n",
      "Epoch 0/10. Iteration 16800/47167 Losses: train: 3.3898537158966064, validate: 3.5113158226013184\n",
      "Epoch 0/10. Iteration 16900/47167 Losses: train: 3.4387826919555664, validate: 3.507078170776367\n",
      "Epoch 0/10. Iteration 17000/47167 Losses: train: 3.483828544616699, validate: 3.503427505493164\n",
      "Epoch 0/10. Iteration 17100/47167 Losses: train: 3.4070568084716797, validate: 3.5013163089752197\n",
      "Epoch 0/10. Iteration 17200/47167 Losses: train: 3.3000283241271973, validate: 3.499631404876709\n",
      "Epoch 0/10. Iteration 17300/47167 Losses: train: 3.3404622077941895, validate: 3.499243974685669\n",
      "Epoch 0/10. Iteration 17400/47167 Losses: train: 3.221518039703369, validate: 3.4976439476013184\n",
      "Epoch 0/10. Iteration 17500/47167 Losses: train: 3.2477505207061768, validate: 3.496581792831421\n",
      "Epoch 0/10. Iteration 17600/47167 Losses: train: 3.296809673309326, validate: 3.496162176132202\n",
      "Epoch 0/10. Iteration 17700/47167 Losses: train: 3.3052947521209717, validate: 3.4937899112701416\n",
      "Epoch 0/10. Iteration 17800/47167 Losses: train: 3.3539724349975586, validate: 3.495511531829834\n",
      "Epoch 0/10. Iteration 17900/47167 Losses: train: 3.3865091800689697, validate: 3.4929165840148926\n",
      "Epoch 0/10. Iteration 18000/47167 Losses: train: 3.1922450065612793, validate: 3.4946322441101074\n",
      "Epoch 0/10. Iteration 18100/47167 Losses: train: 3.3475265502929688, validate: 3.4893429279327393\n",
      "Epoch 0/10. Iteration 18200/47167 Losses: train: 3.2943646907806396, validate: 3.4895877838134766\n",
      "Epoch 0/10. Iteration 18300/47167 Losses: train: 3.403888463973999, validate: 3.4872586727142334\n",
      "Epoch 0/10. Iteration 18400/47167 Losses: train: 3.177260398864746, validate: 3.483060359954834\n",
      "Epoch 0/10. Iteration 18500/47167 Losses: train: 3.4955029487609863, validate: 3.4824836254119873\n",
      "Epoch 0/10. Iteration 18600/47167 Losses: train: 3.3740038871765137, validate: 3.4781692028045654\n",
      "Epoch 0/10. Iteration 18700/47167 Losses: train: 3.2710044384002686, validate: 3.479330062866211\n",
      "Epoch 0/10. Iteration 18800/47167 Losses: train: 3.5798230171203613, validate: 3.477522134780884\n",
      "Epoch 0/10. Iteration 18900/47167 Losses: train: 3.289088010787964, validate: 3.4792776107788086\n",
      "Epoch 0/10. Iteration 19000/47167 Losses: train: 3.491374969482422, validate: 3.477971315383911\n",
      "Epoch 0/10. Iteration 19100/47167 Losses: train: 3.2656173706054688, validate: 3.473128080368042\n",
      "Epoch 0/10. Iteration 19200/47167 Losses: train: 3.509775400161743, validate: 3.4755961894989014\n",
      "Epoch 0/10. Iteration 19300/47167 Losses: train: 3.5221641063690186, validate: 3.4736504554748535\n",
      "Epoch 0/10. Iteration 19400/47167 Losses: train: 3.31485652923584, validate: 3.472277879714966\n",
      "Epoch 0/10. Iteration 19500/47167 Losses: train: 3.630669355392456, validate: 3.471975564956665\n",
      "Epoch 0/10. Iteration 19600/47167 Losses: train: 3.1111021041870117, validate: 3.4678521156311035\n",
      "Epoch 0/10. Iteration 19700/47167 Losses: train: 3.1895744800567627, validate: 3.4708831310272217\n",
      "Epoch 0/10. Iteration 19800/47167 Losses: train: 3.358350992202759, validate: 3.468782901763916\n",
      "Epoch 0/10. Iteration 19900/47167 Losses: train: 3.237257719039917, validate: 3.4648101329803467\n",
      "Epoch 0/10. Iteration 20000/47167 Losses: train: 3.48899507522583, validate: 3.465729236602783\n",
      "Epoch 0/10. Iteration 20100/47167 Losses: train: 3.3138608932495117, validate: 3.4643914699554443\n",
      "Epoch 0/10. Iteration 20200/47167 Losses: train: 3.398235559463501, validate: 3.466508626937866\n",
      "Epoch 0/10. Iteration 20300/47167 Losses: train: 3.5218238830566406, validate: 3.4556195735931396\n",
      "Epoch 0/10. Iteration 20400/47167 Losses: train: 3.2234046459198, validate: 3.4584996700286865\n",
      "Epoch 0/10. Iteration 20500/47167 Losses: train: 3.344207763671875, validate: 3.459951162338257\n",
      "Epoch 0/10. Iteration 20600/47167 Losses: train: 3.3590481281280518, validate: 3.4577362537384033\n",
      "Epoch 0/10. Iteration 20700/47167 Losses: train: 3.3325705528259277, validate: 3.4523699283599854\n",
      "Epoch 0/10. Iteration 20800/47167 Losses: train: 3.193121910095215, validate: 3.449413299560547\n",
      "Epoch 0/10. Iteration 20900/47167 Losses: train: 3.3694844245910645, validate: 3.450493097305298\n",
      "Epoch 0/10. Iteration 21000/47167 Losses: train: 3.561471939086914, validate: 3.452908754348755\n",
      "Epoch 0/10. Iteration 21100/47167 Losses: train: 3.523658037185669, validate: 3.4479174613952637\n",
      "Epoch 0/10. Iteration 21200/47167 Losses: train: 3.2658965587615967, validate: 3.446074962615967\n",
      "Epoch 0/10. Iteration 21300/47167 Losses: train: 3.142357349395752, validate: 3.450071334838867\n",
      "Epoch 0/10. Iteration 21400/47167 Losses: train: 3.5056962966918945, validate: 3.4472057819366455\n",
      "Epoch 0/10. Iteration 21500/47167 Losses: train: 3.072762966156006, validate: 3.4527649879455566\n",
      "Epoch 0/10. Iteration 21600/47167 Losses: train: 3.2544355392456055, validate: 3.440192699432373\n",
      "Epoch 0/10. Iteration 21700/47167 Losses: train: 3.170602560043335, validate: 3.4493248462677\n",
      "Epoch 0/10. Iteration 21800/47167 Losses: train: 3.315401554107666, validate: 3.444272518157959\n",
      "Epoch 0/10. Iteration 21900/47167 Losses: train: 3.571584701538086, validate: 3.4380314350128174\n",
      "Epoch 0/10. Iteration 22000/47167 Losses: train: 3.2813267707824707, validate: 3.4330239295959473\n",
      "Epoch 0/10. Iteration 22100/47167 Losses: train: 3.2207489013671875, validate: 3.4381139278411865\n",
      "Epoch 0/10. Iteration 22200/47167 Losses: train: 3.1640331745147705, validate: 3.4392173290252686\n",
      "Epoch 0/10. Iteration 22300/47167 Losses: train: 3.24045991897583, validate: 3.4304087162017822\n",
      "Epoch 0/10. Iteration 22400/47167 Losses: train: 3.355987310409546, validate: 3.4295437335968018\n",
      "Epoch 0/10. Iteration 22500/47167 Losses: train: 3.060580253601074, validate: 3.427706480026245\n",
      "Epoch 0/10. Iteration 22600/47167 Losses: train: 3.3719115257263184, validate: 3.431943893432617\n",
      "Epoch 0/10. Iteration 22700/47167 Losses: train: 3.208529233932495, validate: 3.423816442489624\n",
      "Epoch 0/10. Iteration 22800/47167 Losses: train: 3.401988983154297, validate: 3.4266014099121094\n",
      "Epoch 0/10. Iteration 22900/47167 Losses: train: 3.1868181228637695, validate: 3.4253737926483154\n",
      "Epoch 0/10. Iteration 23000/47167 Losses: train: 3.064073085784912, validate: 3.4271514415740967\n",
      "Epoch 0/10. Iteration 23100/47167 Losses: train: 3.3925695419311523, validate: 3.431924343109131\n",
      "Epoch 0/10. Iteration 23200/47167 Losses: train: 3.2690165042877197, validate: 3.416990280151367\n",
      "Epoch 0/10. Iteration 23300/47167 Losses: train: 3.341740369796753, validate: 3.4171695709228516\n",
      "Epoch 0/10. Iteration 23400/47167 Losses: train: 3.180198907852173, validate: 3.4137606620788574\n",
      "Epoch 0/10. Iteration 23500/47167 Losses: train: 3.352083206176758, validate: 3.4152884483337402\n",
      "Epoch 0/10. Iteration 23600/47167 Losses: train: 3.2563047409057617, validate: 3.4274866580963135\n",
      "Epoch 0/10. Iteration 23700/47167 Losses: train: 3.423689842224121, validate: 3.414252758026123\n",
      "Epoch 0/10. Iteration 23800/47167 Losses: train: 3.558112382888794, validate: 3.4140257835388184\n",
      "Epoch 0/10. Iteration 23900/47167 Losses: train: 3.314251661300659, validate: 3.411339282989502\n",
      "Epoch 0/10. Iteration 24000/47167 Losses: train: 3.1929335594177246, validate: 3.4067251682281494\n",
      "Epoch 0/10. Iteration 24100/47167 Losses: train: 3.2296016216278076, validate: 3.41259503364563\n",
      "Epoch 0/10. Iteration 24200/47167 Losses: train: 3.155590534210205, validate: 3.4129793643951416\n",
      "Epoch 0/10. Iteration 24300/47167 Losses: train: 3.143813371658325, validate: 3.4048373699188232\n",
      "Epoch 0/10. Iteration 24400/47167 Losses: train: 3.2780215740203857, validate: 3.404853343963623\n",
      "Epoch 0/10. Iteration 24500/47167 Losses: train: 3.162653684616089, validate: 3.3993496894836426\n",
      "Epoch 0/10. Iteration 24600/47167 Losses: train: 3.2069005966186523, validate: 3.4026405811309814\n",
      "Epoch 0/10. Iteration 24700/47167 Losses: train: 3.265226125717163, validate: 3.3985655307769775\n",
      "Epoch 0/10. Iteration 24800/47167 Losses: train: 3.1225428581237793, validate: 3.4013688564300537\n",
      "Epoch 0/10. Iteration 24900/47167 Losses: train: 3.4643096923828125, validate: 3.3994693756103516\n",
      "Epoch 0/10. Iteration 25000/47167 Losses: train: 3.26208233833313, validate: 3.395841598510742\n",
      "Epoch 0/10. Iteration 25100/47167 Losses: train: 3.163957357406616, validate: 3.392608404159546\n",
      "Epoch 0/10. Iteration 25200/47167 Losses: train: 3.116595506668091, validate: 3.3921220302581787\n",
      "Epoch 0/10. Iteration 25300/47167 Losses: train: 3.18481183052063, validate: 3.3941707611083984\n",
      "Epoch 0/10. Iteration 25400/47167 Losses: train: 3.3792264461517334, validate: 3.394202470779419\n",
      "Epoch 0/10. Iteration 25500/47167 Losses: train: 3.2203903198242188, validate: 3.3940110206604004\n",
      "Epoch 0/10. Iteration 25600/47167 Losses: train: 3.4058678150177, validate: 3.3879220485687256\n",
      "Epoch 0/10. Iteration 25700/47167 Losses: train: 3.1183242797851562, validate: 3.3861796855926514\n",
      "Epoch 0/10. Iteration 25800/47167 Losses: train: 3.2598929405212402, validate: 3.391897439956665\n",
      "Epoch 0/10. Iteration 25900/47167 Losses: train: 3.2695772647857666, validate: 3.3864388465881348\n",
      "Epoch 0/10. Iteration 26000/47167 Losses: train: 3.1364896297454834, validate: 3.3829710483551025\n",
      "Epoch 0/10. Iteration 26100/47167 Losses: train: 3.2141036987304688, validate: 3.382429599761963\n",
      "Epoch 0/10. Iteration 26200/47167 Losses: train: 3.115248680114746, validate: 3.379857063293457\n",
      "Epoch 0/10. Iteration 26300/47167 Losses: train: 3.261908531188965, validate: 3.3777740001678467\n",
      "Epoch 0/10. Iteration 26400/47167 Losses: train: 3.342864990234375, validate: 3.3783249855041504\n",
      "Epoch 0/10. Iteration 26500/47167 Losses: train: 3.2772068977355957, validate: 3.3767313957214355\n",
      "Epoch 0/10. Iteration 26600/47167 Losses: train: 3.2711586952209473, validate: 3.3803932666778564\n",
      "Epoch 0/10. Iteration 26700/47167 Losses: train: 3.4908223152160645, validate: 3.380009651184082\n",
      "Epoch 0/10. Iteration 26800/47167 Losses: train: 3.349942207336426, validate: 3.37536358833313\n",
      "Epoch 0/10. Iteration 26900/47167 Losses: train: 3.127589225769043, validate: 3.3807621002197266\n",
      "Epoch 0/10. Iteration 27000/47167 Losses: train: 3.24064040184021, validate: 3.377197027206421\n",
      "Epoch 0/10. Iteration 27100/47167 Losses: train: 3.0734646320343018, validate: 3.3719732761383057\n",
      "Epoch 0/10. Iteration 27200/47167 Losses: train: 2.875615119934082, validate: 3.3717775344848633\n",
      "Epoch 0/10. Iteration 27300/47167 Losses: train: 3.045184850692749, validate: 3.364746332168579\n",
      "Epoch 0/10. Iteration 27400/47167 Losses: train: 3.2017886638641357, validate: 3.364461660385132\n",
      "Epoch 0/10. Iteration 27500/47167 Losses: train: 3.3829774856567383, validate: 3.364044189453125\n",
      "Epoch 0/10. Iteration 27600/47167 Losses: train: 3.332695722579956, validate: 3.3612496852874756\n",
      "Epoch 0/10. Iteration 27700/47167 Losses: train: 3.357553005218506, validate: 3.356541156768799\n",
      "Epoch 0/10. Iteration 27800/47167 Losses: train: 3.111586809158325, validate: 3.357736349105835\n",
      "Epoch 0/10. Iteration 27900/47167 Losses: train: 3.1348390579223633, validate: 3.358734369277954\n",
      "Epoch 0/10. Iteration 28000/47167 Losses: train: 3.1299147605895996, validate: 3.3525404930114746\n",
      "Epoch 0/10. Iteration 28100/47167 Losses: train: 2.9729437828063965, validate: 3.354982376098633\n",
      "Epoch 0/10. Iteration 28200/47167 Losses: train: 3.3775346279144287, validate: 3.356001377105713\n",
      "Epoch 0/10. Iteration 28300/47167 Losses: train: 3.106658458709717, validate: 3.349919557571411\n",
      "Epoch 0/10. Iteration 28400/47167 Losses: train: 3.2258450984954834, validate: 3.352125883102417\n",
      "Epoch 0/10. Iteration 28500/47167 Losses: train: 3.217392683029175, validate: 3.352221965789795\n",
      "Epoch 0/10. Iteration 28600/47167 Losses: train: 3.2807717323303223, validate: 3.3442397117614746\n",
      "Epoch 0/10. Iteration 28700/47167 Losses: train: 3.3254282474517822, validate: 3.344028949737549\n",
      "Epoch 0/10. Iteration 28800/47167 Losses: train: 3.156052827835083, validate: 3.3485870361328125\n",
      "Epoch 0/10. Iteration 28900/47167 Losses: train: 3.5141448974609375, validate: 3.3485352993011475\n",
      "Epoch 0/10. Iteration 29000/47167 Losses: train: 3.1642632484436035, validate: 3.340975522994995\n",
      "Epoch 0/10. Iteration 29100/47167 Losses: train: 3.21036958694458, validate: 3.347310781478882\n",
      "Epoch 0/10. Iteration 29200/47167 Losses: train: 3.2874059677124023, validate: 3.3405561447143555\n",
      "Epoch 0/10. Iteration 29300/47167 Losses: train: 3.204669952392578, validate: 3.3349249362945557\n",
      "Epoch 0/10. Iteration 29400/47167 Losses: train: 3.0714473724365234, validate: 3.338998794555664\n",
      "Epoch 0/10. Iteration 29500/47167 Losses: train: 3.1925294399261475, validate: 3.336568832397461\n",
      "Epoch 0/10. Iteration 29600/47167 Losses: train: 3.254164695739746, validate: 3.3227851390838623\n",
      "Epoch 0/10. Iteration 29700/47167 Losses: train: 3.2223243713378906, validate: 3.33065128326416\n",
      "Epoch 0/10. Iteration 29800/47167 Losses: train: 3.2086470127105713, validate: 3.327321767807007\n",
      "Epoch 0/10. Iteration 29900/47167 Losses: train: 3.263291358947754, validate: 3.3262200355529785\n",
      "Epoch 0/10. Iteration 30000/47167 Losses: train: 3.1686253547668457, validate: 3.319952964782715\n",
      "Epoch 0/10. Iteration 30100/47167 Losses: train: 3.057054281234741, validate: 3.3383092880249023\n",
      "Epoch 0/10. Iteration 30200/47167 Losses: train: 3.065427303314209, validate: 3.3259143829345703\n",
      "Epoch 0/10. Iteration 30300/47167 Losses: train: 3.121211290359497, validate: 3.3158044815063477\n",
      "Epoch 0/10. Iteration 30400/47167 Losses: train: 3.125239133834839, validate: 3.3217568397521973\n",
      "Epoch 0/10. Iteration 30500/47167 Losses: train: 3.1362977027893066, validate: 3.309515953063965\n",
      "Epoch 0/10. Iteration 30600/47167 Losses: train: 3.2694082260131836, validate: 3.3243355751037598\n",
      "Epoch 0/10. Iteration 30700/47167 Losses: train: 3.108072519302368, validate: 3.3063910007476807\n",
      "Epoch 0/10. Iteration 30800/47167 Losses: train: 3.140805244445801, validate: 3.304044246673584\n",
      "Epoch 0/10. Iteration 30900/47167 Losses: train: 3.0581603050231934, validate: 3.3021442890167236\n",
      "Epoch 0/10. Iteration 31000/47167 Losses: train: 3.0513908863067627, validate: 3.306831121444702\n",
      "Epoch 0/10. Iteration 31100/47167 Losses: train: 3.102736473083496, validate: 3.304419755935669\n",
      "Epoch 0/10. Iteration 31200/47167 Losses: train: 3.1947388648986816, validate: 3.2959845066070557\n",
      "Epoch 0/10. Iteration 31300/47167 Losses: train: 3.197798728942871, validate: 3.3016579151153564\n",
      "Epoch 0/10. Iteration 31400/47167 Losses: train: 3.2653820514678955, validate: 3.294682741165161\n",
      "Epoch 0/10. Iteration 31500/47167 Losses: train: 3.2536041736602783, validate: 3.292728900909424\n",
      "Epoch 0/10. Iteration 31600/47167 Losses: train: 3.1960880756378174, validate: 3.30033278465271\n",
      "Epoch 0/10. Iteration 31700/47167 Losses: train: 3.1456212997436523, validate: 3.293630599975586\n",
      "Epoch 0/10. Iteration 31800/47167 Losses: train: 3.1259443759918213, validate: 3.2921266555786133\n",
      "Epoch 0/10. Iteration 31900/47167 Losses: train: 3.167948007583618, validate: 3.2846481800079346\n",
      "Epoch 0/10. Iteration 32000/47167 Losses: train: 3.320366621017456, validate: 3.282054901123047\n",
      "Epoch 0/10. Iteration 32100/47167 Losses: train: 3.135139226913452, validate: 3.2781381607055664\n",
      "Epoch 0/10. Iteration 32200/47167 Losses: train: 3.2213492393493652, validate: 3.2754552364349365\n",
      "Epoch 0/10. Iteration 32300/47167 Losses: train: 3.143747568130493, validate: 3.2816126346588135\n",
      "Epoch 0/10. Iteration 32400/47167 Losses: train: 3.162808418273926, validate: 3.2792439460754395\n",
      "Epoch 0/10. Iteration 32500/47167 Losses: train: 3.018902063369751, validate: 3.2717463970184326\n",
      "Epoch 0/10. Iteration 32600/47167 Losses: train: 3.1402151584625244, validate: 3.2779603004455566\n",
      "Epoch 0/10. Iteration 32700/47167 Losses: train: 3.0546042919158936, validate: 3.2763307094573975\n",
      "Epoch 0/10. Iteration 32800/47167 Losses: train: 3.096381664276123, validate: 3.265674352645874\n",
      "Epoch 0/10. Iteration 32900/47167 Losses: train: 3.097022533416748, validate: 3.2675697803497314\n",
      "Epoch 0/10. Iteration 33000/47167 Losses: train: 3.2584784030914307, validate: 3.2626020908355713\n",
      "Epoch 0/10. Iteration 33100/47167 Losses: train: 2.8848557472229004, validate: 3.2603952884674072\n",
      "Epoch 0/10. Iteration 33200/47167 Losses: train: 3.32321834564209, validate: 3.258514404296875\n",
      "Epoch 0/10. Iteration 33300/47167 Losses: train: 3.261209487915039, validate: 3.2585737705230713\n",
      "Epoch 0/10. Iteration 33400/47167 Losses: train: 3.127720832824707, validate: 3.2559800148010254\n",
      "Epoch 0/10. Iteration 33500/47167 Losses: train: 3.1225123405456543, validate: 3.255458116531372\n",
      "Epoch 0/10. Iteration 33600/47167 Losses: train: 3.28576922416687, validate: 3.2510597705841064\n",
      "Epoch 0/10. Iteration 33700/47167 Losses: train: 3.2658896446228027, validate: 3.250108480453491\n",
      "Epoch 0/10. Iteration 33800/47167 Losses: train: 3.2089011669158936, validate: 3.2493996620178223\n",
      "Epoch 0/10. Iteration 33900/47167 Losses: train: 3.3412976264953613, validate: 3.245591402053833\n",
      "Epoch 0/10. Iteration 34000/47167 Losses: train: 3.068236827850342, validate: 3.2440567016601562\n",
      "Epoch 0/10. Iteration 34100/47167 Losses: train: 2.9692904949188232, validate: 3.2405641078948975\n",
      "Epoch 0/10. Iteration 34200/47167 Losses: train: 3.299866199493408, validate: 3.2512929439544678\n",
      "Epoch 0/10. Iteration 34300/47167 Losses: train: 3.199784994125366, validate: 3.2428791522979736\n",
      "Epoch 0/10. Iteration 34400/47167 Losses: train: 2.9833102226257324, validate: 3.241971731185913\n",
      "Epoch 0/10. Iteration 34500/47167 Losses: train: 3.009528875350952, validate: 3.2379238605499268\n",
      "Epoch 0/10. Iteration 34600/47167 Losses: train: 3.146883964538574, validate: 3.231813907623291\n",
      "Epoch 0/10. Iteration 34700/47167 Losses: train: 2.842679262161255, validate: 3.234063148498535\n",
      "Epoch 0/10. Iteration 34800/47167 Losses: train: 3.301257610321045, validate: 3.238802194595337\n",
      "Epoch 0/10. Iteration 34900/47167 Losses: train: 3.106295585632324, validate: 3.230539560317993\n",
      "Epoch 0/10. Iteration 35000/47167 Losses: train: 3.1729865074157715, validate: 3.2281317710876465\n",
      "Epoch 0/10. Iteration 35100/47167 Losses: train: 3.04565167427063, validate: 3.2327589988708496\n",
      "Epoch 0/10. Iteration 35200/47167 Losses: train: 3.2141644954681396, validate: 3.2324819564819336\n",
      "Epoch 0/10. Iteration 35300/47167 Losses: train: 2.9105279445648193, validate: 3.22705078125\n",
      "Epoch 0/10. Iteration 35400/47167 Losses: train: 2.9583539962768555, validate: 3.220980167388916\n",
      "Epoch 0/10. Iteration 35500/47167 Losses: train: 2.989712953567505, validate: 3.2195074558258057\n",
      "Epoch 0/10. Iteration 35600/47167 Losses: train: 3.080824613571167, validate: 3.2208359241485596\n",
      "Epoch 0/10. Iteration 35700/47167 Losses: train: 3.2429468631744385, validate: 3.2237417697906494\n",
      "Epoch 0/10. Iteration 35800/47167 Losses: train: 3.088550329208374, validate: 3.217205047607422\n",
      "Epoch 0/10. Iteration 35900/47167 Losses: train: 2.9263219833374023, validate: 3.213740110397339\n",
      "Epoch 0/10. Iteration 36000/47167 Losses: train: 3.089261293411255, validate: 3.2127089500427246\n",
      "Epoch 0/10. Iteration 36100/47167 Losses: train: 3.2449254989624023, validate: 3.216057300567627\n",
      "Epoch 0/10. Iteration 36200/47167 Losses: train: 3.286153554916382, validate: 3.220048666000366\n",
      "Epoch 0/10. Iteration 36300/47167 Losses: train: 2.926375389099121, validate: 3.2073166370391846\n",
      "Epoch 0/10. Iteration 36400/47167 Losses: train: 3.117650270462036, validate: 3.2114524841308594\n",
      "Epoch 0/10. Iteration 36500/47167 Losses: train: 3.2224996089935303, validate: 3.206679105758667\n",
      "Epoch 0/10. Iteration 36600/47167 Losses: train: 3.0588536262512207, validate: 3.2032687664031982\n",
      "Epoch 0/10. Iteration 36700/47167 Losses: train: 3.3026084899902344, validate: 3.2061829566955566\n",
      "Epoch 0/10. Iteration 36800/47167 Losses: train: 3.1200954914093018, validate: 3.2036895751953125\n",
      "Epoch 0/10. Iteration 36900/47167 Losses: train: 3.0374350547790527, validate: 3.203906536102295\n",
      "Epoch 0/10. Iteration 37000/47167 Losses: train: 3.0691325664520264, validate: 3.1980464458465576\n",
      "Epoch 0/10. Iteration 37100/47167 Losses: train: 3.0046846866607666, validate: 3.195113182067871\n",
      "Epoch 0/10. Iteration 37200/47167 Losses: train: 3.056896686553955, validate: 3.1949868202209473\n",
      "Epoch 0/10. Iteration 37300/47167 Losses: train: 3.061912775039673, validate: 3.199129581451416\n",
      "Epoch 0/10. Iteration 37400/47167 Losses: train: 2.864067792892456, validate: 3.198146104812622\n",
      "Epoch 0/10. Iteration 37500/47167 Losses: train: 2.817505359649658, validate: 3.199063539505005\n",
      "Epoch 0/10. Iteration 37600/47167 Losses: train: 3.1399717330932617, validate: 3.192547559738159\n",
      "Epoch 0/10. Iteration 37700/47167 Losses: train: 3.1752536296844482, validate: 3.1910552978515625\n",
      "Epoch 0/10. Iteration 37800/47167 Losses: train: 2.9586126804351807, validate: 3.197594165802002\n",
      "Epoch 0/10. Iteration 37900/47167 Losses: train: 3.1330370903015137, validate: 3.18533992767334\n",
      "Epoch 0/10. Iteration 38000/47167 Losses: train: 3.035451650619507, validate: 3.1928598880767822\n",
      "Epoch 0/10. Iteration 38100/47167 Losses: train: 2.8885295391082764, validate: 3.187103748321533\n",
      "Epoch 0/10. Iteration 38200/47167 Losses: train: 3.003288507461548, validate: 3.181319236755371\n",
      "Epoch 0/10. Iteration 38300/47167 Losses: train: 2.977213144302368, validate: 3.185373306274414\n",
      "Epoch 0/10. Iteration 38400/47167 Losses: train: 3.1395630836486816, validate: 3.1810405254364014\n",
      "Epoch 0/10. Iteration 38500/47167 Losses: train: 3.0595269203186035, validate: 3.182425022125244\n",
      "Epoch 0/10. Iteration 38600/47167 Losses: train: 2.922177314758301, validate: 3.1761412620544434\n",
      "Epoch 0/10. Iteration 38700/47167 Losses: train: 3.0730180740356445, validate: 3.1846132278442383\n",
      "Epoch 0/10. Iteration 38800/47167 Losses: train: 2.9395151138305664, validate: 3.1815922260284424\n",
      "Epoch 0/10. Iteration 38900/47167 Losses: train: 3.1116585731506348, validate: 3.1776785850524902\n",
      "Epoch 0/10. Iteration 39000/47167 Losses: train: 2.890394687652588, validate: 3.1781256198883057\n",
      "Epoch 0/10. Iteration 39100/47167 Losses: train: 3.0002694129943848, validate: 3.1695237159729004\n",
      "Epoch 0/10. Iteration 39200/47167 Losses: train: 3.0710480213165283, validate: 3.1707897186279297\n",
      "Epoch 0/10. Iteration 39300/47167 Losses: train: 3.070828437805176, validate: 3.1725895404815674\n",
      "Epoch 0/10. Iteration 39400/47167 Losses: train: 3.0025784969329834, validate: 3.1711182594299316\n",
      "Epoch 0/10. Iteration 39500/47167 Losses: train: 3.057356357574463, validate: 3.170382022857666\n",
      "Epoch 0/10. Iteration 39600/47167 Losses: train: 3.132941484451294, validate: 3.167999029159546\n",
      "Epoch 0/10. Iteration 39700/47167 Losses: train: 3.0598855018615723, validate: 3.161893129348755\n",
      "Epoch 0/10. Iteration 39800/47167 Losses: train: 3.455678701400757, validate: 3.165140151977539\n",
      "Epoch 0/10. Iteration 39900/47167 Losses: train: 3.040585517883301, validate: 3.161742687225342\n",
      "Epoch 0/10. Iteration 40000/47167 Losses: train: 2.9359734058380127, validate: 3.1595962047576904\n",
      "Epoch 0/10. Iteration 40100/47167 Losses: train: 3.0296552181243896, validate: 3.152798891067505\n",
      "Epoch 0/10. Iteration 40200/47167 Losses: train: 2.8570947647094727, validate: 3.153489828109741\n",
      "Epoch 0/10. Iteration 40300/47167 Losses: train: 2.894968271255493, validate: 3.154609441757202\n",
      "Epoch 0/10. Iteration 40400/47167 Losses: train: 2.912781000137329, validate: 3.153256416320801\n",
      "Epoch 0/10. Iteration 40500/47167 Losses: train: 2.82228422164917, validate: 3.149331569671631\n",
      "Epoch 0/10. Iteration 40600/47167 Losses: train: 3.110586643218994, validate: 3.1567933559417725\n",
      "Epoch 0/10. Iteration 40700/47167 Losses: train: 3.085055112838745, validate: 3.155629873275757\n",
      "Epoch 0/10. Iteration 40800/47167 Losses: train: 3.193387985229492, validate: 3.145289421081543\n",
      "Epoch 0/10. Iteration 40900/47167 Losses: train: 3.0885233879089355, validate: 3.13999080657959\n",
      "Epoch 0/10. Iteration 41000/47167 Losses: train: 3.2544913291931152, validate: 3.1366193294525146\n",
      "Epoch 0/10. Iteration 41100/47167 Losses: train: 3.117997646331787, validate: 3.1403770446777344\n",
      "Epoch 0/10. Iteration 41200/47167 Losses: train: 3.056293249130249, validate: 3.1423041820526123\n",
      "Epoch 0/10. Iteration 41300/47167 Losses: train: 3.217512369155884, validate: 3.141385078430176\n",
      "Epoch 0/10. Iteration 41400/47167 Losses: train: 2.9792871475219727, validate: 3.1378657817840576\n",
      "Epoch 0/10. Iteration 41500/47167 Losses: train: 3.0204837322235107, validate: 3.1382815837860107\n",
      "Epoch 0/10. Iteration 41600/47167 Losses: train: 2.83817720413208, validate: 3.138197422027588\n",
      "Epoch 0/10. Iteration 41700/47167 Losses: train: 3.026144504547119, validate: 3.1298699378967285\n",
      "Epoch 0/10. Iteration 41800/47167 Losses: train: 2.7590579986572266, validate: 3.1273462772369385\n",
      "Epoch 0/10. Iteration 41900/47167 Losses: train: 3.05279541015625, validate: 3.1235785484313965\n",
      "Epoch 0/10. Iteration 42000/47167 Losses: train: 2.975839853286743, validate: 3.134382963180542\n",
      "Epoch 0/10. Iteration 42100/47167 Losses: train: 3.198392868041992, validate: 3.1280996799468994\n",
      "Epoch 0/10. Iteration 42200/47167 Losses: train: 3.245507001876831, validate: 3.123478651046753\n",
      "Epoch 0/10. Iteration 42300/47167 Losses: train: 2.938952684402466, validate: 3.126772403717041\n",
      "Epoch 0/10. Iteration 42400/47167 Losses: train: 2.9645397663116455, validate: 3.1216371059417725\n",
      "Epoch 0/10. Iteration 42500/47167 Losses: train: 3.237239360809326, validate: 3.1257145404815674\n",
      "Epoch 0/10. Iteration 42600/47167 Losses: train: 3.093827247619629, validate: 3.12631893157959\n",
      "Epoch 0/10. Iteration 42700/47167 Losses: train: 2.9998605251312256, validate: 3.1223247051239014\n",
      "Epoch 0/10. Iteration 42800/47167 Losses: train: 2.8751306533813477, validate: 3.123277187347412\n",
      "Epoch 0/10. Iteration 42900/47167 Losses: train: 3.0181217193603516, validate: 3.1244852542877197\n",
      "Epoch 0/10. Iteration 43000/47167 Losses: train: 3.068760395050049, validate: 3.121082305908203\n",
      "Epoch 0/10. Iteration 43100/47167 Losses: train: 2.9092600345611572, validate: 3.121910333633423\n",
      "Epoch 0/10. Iteration 43200/47167 Losses: train: 3.0817792415618896, validate: 3.1210925579071045\n",
      "Epoch 0/10. Iteration 43300/47167 Losses: train: 3.1164777278900146, validate: 3.1267971992492676\n",
      "Epoch 0/10. Iteration 43400/47167 Losses: train: 2.8539655208587646, validate: 3.1130549907684326\n",
      "Epoch 0/10. Iteration 43500/47167 Losses: train: 3.0628583431243896, validate: 3.1117355823516846\n",
      "Epoch 0/10. Iteration 43600/47167 Losses: train: 3.18863844871521, validate: 3.119415760040283\n",
      "Epoch 0/10. Iteration 43700/47167 Losses: train: 3.0240464210510254, validate: 3.1277239322662354\n",
      "Epoch 0/10. Iteration 43800/47167 Losses: train: 2.8705193996429443, validate: 3.1189658641815186\n",
      "Epoch 0/10. Iteration 43900/47167 Losses: train: 2.9110236167907715, validate: 3.108994722366333\n",
      "Epoch 0/10. Iteration 44000/47167 Losses: train: 2.686361789703369, validate: 3.104491949081421\n",
      "Epoch 0/10. Iteration 44100/47167 Losses: train: 2.7390823364257812, validate: 3.1071972846984863\n",
      "Epoch 0/10. Iteration 44200/47167 Losses: train: 2.8401994705200195, validate: 3.1059484481811523\n",
      "Epoch 0/10. Iteration 44300/47167 Losses: train: 3.1504127979278564, validate: 3.107816696166992\n",
      "Epoch 0/10. Iteration 44400/47167 Losses: train: 2.913250684738159, validate: 3.103860855102539\n",
      "Epoch 0/10. Iteration 44500/47167 Losses: train: 2.970280885696411, validate: 3.1087288856506348\n",
      "Epoch 0/10. Iteration 44600/47167 Losses: train: 2.8544888496398926, validate: 3.102046012878418\n",
      "Epoch 0/10. Iteration 44700/47167 Losses: train: 3.037418842315674, validate: 3.0984458923339844\n",
      "Epoch 0/10. Iteration 44800/47167 Losses: train: 2.9346158504486084, validate: 3.0990216732025146\n",
      "Epoch 0/10. Iteration 44900/47167 Losses: train: 2.918954610824585, validate: 3.1042139530181885\n",
      "Epoch 0/10. Iteration 45000/47167 Losses: train: 3.029304027557373, validate: 3.1078124046325684\n",
      "Epoch 0/10. Iteration 45100/47167 Losses: train: 3.0375356674194336, validate: 3.102220296859741\n",
      "Epoch 0/10. Iteration 45200/47167 Losses: train: 2.921064615249634, validate: 3.0990216732025146\n",
      "Epoch 0/10. Iteration 45300/47167 Losses: train: 3.114914655685425, validate: 3.102271795272827\n",
      "Epoch 0/10. Iteration 45400/47167 Losses: train: 2.885228395462036, validate: 3.0955193042755127\n",
      "Epoch 0/10. Iteration 45500/47167 Losses: train: 2.841362714767456, validate: 3.096813678741455\n",
      "Epoch 0/10. Iteration 45600/47167 Losses: train: 3.2239580154418945, validate: 3.0945582389831543\n",
      "Epoch 0/10. Iteration 45700/47167 Losses: train: 2.845534324645996, validate: 3.090320348739624\n",
      "Epoch 0/10. Iteration 45800/47167 Losses: train: 3.0229551792144775, validate: 3.0899298191070557\n",
      "Epoch 0/10. Iteration 45900/47167 Losses: train: 2.840693712234497, validate: 3.0951356887817383\n",
      "Epoch 0/10. Iteration 46000/47167 Losses: train: 2.925598382949829, validate: 3.0908186435699463\n",
      "Epoch 0/10. Iteration 46100/47167 Losses: train: 2.9339940547943115, validate: 3.0873730182647705\n",
      "Epoch 0/10. Iteration 46200/47167 Losses: train: 2.901841878890991, validate: 3.087932586669922\n",
      "Epoch 0/10. Iteration 46300/47167 Losses: train: 2.931736707687378, validate: 3.0820846557617188\n",
      "Epoch 0/10. Iteration 46400/47167 Losses: train: 3.0161571502685547, validate: 3.0760276317596436\n",
      "Epoch 0/10. Iteration 46500/47167 Losses: train: 2.9802002906799316, validate: 3.0864648818969727\n",
      "Epoch 0/10. Iteration 46600/47167 Losses: train: 2.905327796936035, validate: 3.085419178009033\n",
      "Epoch 0/10. Iteration 46700/47167 Losses: train: 3.102046012878418, validate: 3.081373453140259\n",
      "Epoch 0/10. Iteration 46800/47167 Losses: train: 2.75699520111084, validate: 3.072364568710327\n",
      "Epoch 0/10. Iteration 46900/47167 Losses: train: 3.2087748050689697, validate: 3.0702104568481445\n",
      "Epoch 0/10. Iteration 47000/47167 Losses: train: 2.975991725921631, validate: 3.0769472122192383\n",
      "Epoch 0/10. Iteration 47100/47167 Losses: train: 3.037381172180176, validate: 3.07399320602417\n",
      "Epoch 1/10. Iteration 100/47167 Losses: train: 2.9538490772247314, validate: 3.0740504264831543\n",
      "Epoch 1/10. Iteration 200/47167 Losses: train: 2.835841417312622, validate: 3.090833902359009\n",
      "Epoch 1/10. Iteration 300/47167 Losses: train: 2.9501285552978516, validate: 3.0835840702056885\n",
      "Epoch 1/10. Iteration 400/47167 Losses: train: 2.913055419921875, validate: 3.0708744525909424\n",
      "Epoch 1/10. Iteration 500/47167 Losses: train: 2.87803316116333, validate: 3.0624654293060303\n",
      "Epoch 1/10. Iteration 600/47167 Losses: train: 3.047208309173584, validate: 3.068296432495117\n",
      "Epoch 1/10. Iteration 700/47167 Losses: train: 3.0195369720458984, validate: 3.066753387451172\n",
      "Epoch 1/10. Iteration 800/47167 Losses: train: 2.866875410079956, validate: 3.0668559074401855\n",
      "Epoch 1/10. Iteration 900/47167 Losses: train: 2.99288010597229, validate: 3.068418025970459\n",
      "Epoch 1/10. Iteration 1000/47167 Losses: train: 3.006458282470703, validate: 3.0668253898620605\n",
      "Epoch 1/10. Iteration 1100/47167 Losses: train: 2.8603360652923584, validate: 3.0596985816955566\n",
      "Epoch 1/10. Iteration 1200/47167 Losses: train: 3.1354446411132812, validate: 3.0660510063171387\n",
      "Epoch 1/10. Iteration 1300/47167 Losses: train: 2.9099738597869873, validate: 3.0692248344421387\n",
      "Epoch 1/10. Iteration 1400/47167 Losses: train: 2.7093613147735596, validate: 3.0643632411956787\n",
      "Epoch 1/10. Iteration 1500/47167 Losses: train: 3.0235700607299805, validate: 3.062047243118286\n",
      "Epoch 1/10. Iteration 1600/47167 Losses: train: 2.9128100872039795, validate: 3.0620102882385254\n",
      "Epoch 1/10. Iteration 1700/47167 Losses: train: 2.9883100986480713, validate: 3.064793586730957\n",
      "Epoch 1/10. Iteration 1800/47167 Losses: train: 2.9011757373809814, validate: 3.06815242767334\n",
      "Epoch 1/10. Iteration 1900/47167 Losses: train: 2.9728827476501465, validate: 3.0595178604125977\n",
      "Epoch 1/10. Iteration 2000/47167 Losses: train: 3.1176037788391113, validate: 3.054765462875366\n",
      "Epoch 1/10. Iteration 2100/47167 Losses: train: 2.8851919174194336, validate: 3.0604166984558105\n",
      "Epoch 1/10. Iteration 2200/47167 Losses: train: 3.2151615619659424, validate: 3.056675672531128\n",
      "Epoch 1/10. Iteration 2300/47167 Losses: train: 2.7369120121002197, validate: 3.0464580059051514\n",
      "Epoch 1/10. Iteration 2400/47167 Losses: train: 2.944772720336914, validate: 3.0522611141204834\n",
      "Epoch 1/10. Iteration 2500/47167 Losses: train: 2.775270938873291, validate: 3.043764591217041\n",
      "Epoch 1/10. Iteration 2600/47167 Losses: train: 2.853119134902954, validate: 3.056774616241455\n",
      "Epoch 1/10. Iteration 2700/47167 Losses: train: 2.7989327907562256, validate: 3.0473544597625732\n",
      "Epoch 1/10. Iteration 2800/47167 Losses: train: 2.925565481185913, validate: 3.044471263885498\n",
      "Epoch 1/10. Iteration 2900/47167 Losses: train: 2.8245491981506348, validate: 3.045747756958008\n",
      "Epoch 1/10. Iteration 3000/47167 Losses: train: 2.8721225261688232, validate: 3.047065019607544\n",
      "Epoch 1/10. Iteration 3100/47167 Losses: train: 2.7735719680786133, validate: 3.044278621673584\n",
      "Epoch 1/10. Iteration 3200/47167 Losses: train: 2.845393180847168, validate: 3.0525660514831543\n",
      "Epoch 1/10. Iteration 3300/47167 Losses: train: 2.8489627838134766, validate: 3.060607433319092\n",
      "Epoch 1/10. Iteration 3400/47167 Losses: train: 3.102687120437622, validate: 3.048879384994507\n",
      "Epoch 1/10. Iteration 3500/47167 Losses: train: 3.2557690143585205, validate: 3.044796943664551\n",
      "Epoch 1/10. Iteration 3600/47167 Losses: train: 2.8624985218048096, validate: 3.0409646034240723\n",
      "Epoch 1/10. Iteration 3700/47167 Losses: train: 2.7890520095825195, validate: 3.0438928604125977\n",
      "Epoch 1/10. Iteration 3800/47167 Losses: train: 2.9679651260375977, validate: 3.0426952838897705\n",
      "Epoch 1/10. Iteration 3900/47167 Losses: train: 2.8262600898742676, validate: 3.042353630065918\n",
      "Epoch 1/10. Iteration 4000/47167 Losses: train: 2.7646806240081787, validate: 3.050058126449585\n",
      "Epoch 1/10. Iteration 4100/47167 Losses: train: 2.913099765777588, validate: 3.0363144874572754\n",
      "Epoch 1/10. Iteration 4200/47167 Losses: train: 3.012089490890503, validate: 3.0349435806274414\n",
      "Epoch 1/10. Iteration 4300/47167 Losses: train: 2.9067068099975586, validate: 3.0277915000915527\n",
      "Epoch 1/10. Iteration 4400/47167 Losses: train: 2.86222767829895, validate: 3.027329921722412\n",
      "Epoch 1/10. Iteration 4500/47167 Losses: train: 2.8396756649017334, validate: 3.0196428298950195\n",
      "Epoch 1/10. Iteration 4600/47167 Losses: train: 2.8845105171203613, validate: 3.0302300453186035\n",
      "Epoch 1/10. Iteration 4700/47167 Losses: train: 2.8378403186798096, validate: 3.026172161102295\n",
      "Epoch 1/10. Iteration 4800/47167 Losses: train: 2.7047739028930664, validate: 3.029465675354004\n",
      "Epoch 1/10. Iteration 4900/47167 Losses: train: 2.845479726791382, validate: 3.030433177947998\n",
      "Epoch 1/10. Iteration 5000/47167 Losses: train: 2.8410677909851074, validate: 3.031409740447998\n",
      "Epoch 1/10. Iteration 5100/47167 Losses: train: 2.9883997440338135, validate: 3.0203893184661865\n",
      "Epoch 1/10. Iteration 5200/47167 Losses: train: 2.8782997131347656, validate: 3.0241167545318604\n",
      "Epoch 1/10. Iteration 5300/47167 Losses: train: 2.7476353645324707, validate: 3.014159679412842\n",
      "Epoch 1/10. Iteration 5400/47167 Losses: train: 2.874141216278076, validate: 3.0227205753326416\n",
      "Epoch 1/10. Iteration 5500/47167 Losses: train: 2.8643064498901367, validate: 3.022676706314087\n",
      "Epoch 1/10. Iteration 5600/47167 Losses: train: 2.699641227722168, validate: 3.0149762630462646\n",
      "Epoch 1/10. Iteration 5700/47167 Losses: train: 2.8043415546417236, validate: 3.016724109649658\n",
      "Epoch 1/10. Iteration 5800/47167 Losses: train: 3.001400947570801, validate: 3.022279977798462\n",
      "Epoch 1/10. Iteration 5900/47167 Losses: train: 2.8136887550354004, validate: 3.013850212097168\n",
      "Epoch 1/10. Iteration 6000/47167 Losses: train: 2.8560845851898193, validate: 3.013747215270996\n",
      "Epoch 1/10. Iteration 6100/47167 Losses: train: 2.8840270042419434, validate: 3.007413864135742\n",
      "Epoch 1/10. Iteration 6200/47167 Losses: train: 3.131352663040161, validate: 3.008340835571289\n",
      "Epoch 1/10. Iteration 6300/47167 Losses: train: 2.9328322410583496, validate: 3.011775493621826\n",
      "Epoch 1/10. Iteration 6400/47167 Losses: train: 2.740335464477539, validate: 3.0191290378570557\n",
      "Epoch 1/10. Iteration 6500/47167 Losses: train: 2.8299202919006348, validate: 3.0105905532836914\n",
      "Epoch 1/10. Iteration 6600/47167 Losses: train: 3.0849218368530273, validate: 3.0111944675445557\n",
      "Epoch 1/10. Iteration 6700/47167 Losses: train: 2.826493740081787, validate: 3.013915777206421\n",
      "Epoch 1/10. Iteration 6800/47167 Losses: train: 2.797834873199463, validate: 3.0032219886779785\n",
      "Epoch 1/10. Iteration 6900/47167 Losses: train: 2.725006341934204, validate: 3.007439374923706\n",
      "Epoch 1/10. Iteration 7000/47167 Losses: train: 2.8212814331054688, validate: 3.0041534900665283\n",
      "Epoch 1/10. Iteration 7100/47167 Losses: train: 2.7444376945495605, validate: 3.009735107421875\n",
      "Epoch 1/10. Iteration 7200/47167 Losses: train: 2.7153964042663574, validate: 3.008500814437866\n",
      "Epoch 1/10. Iteration 7300/47167 Losses: train: 2.7887368202209473, validate: 3.016127824783325\n",
      "Epoch 1/10. Iteration 7400/47167 Losses: train: 2.945518732070923, validate: 3.005540609359741\n",
      "Epoch 1/10. Iteration 7500/47167 Losses: train: 2.819967031478882, validate: 3.0005202293395996\n",
      "Epoch 1/10. Iteration 7600/47167 Losses: train: 2.8649253845214844, validate: 3.003405809402466\n",
      "Epoch 1/10. Iteration 7700/47167 Losses: train: 2.8894970417022705, validate: 3.004610538482666\n",
      "Epoch 1/10. Iteration 7800/47167 Losses: train: 3.0268592834472656, validate: 3.002917766571045\n",
      "Epoch 1/10. Iteration 7900/47167 Losses: train: 2.6429131031036377, validate: 3.0031116008758545\n",
      "Epoch 1/10. Iteration 8000/47167 Losses: train: 2.9393951892852783, validate: 3.0084285736083984\n",
      "Epoch 1/10. Iteration 8100/47167 Losses: train: 2.7685606479644775, validate: 3.0055112838745117\n",
      "Epoch 1/10. Iteration 8200/47167 Losses: train: 2.835097074508667, validate: 3.0060746669769287\n",
      "Epoch 1/10. Iteration 8300/47167 Losses: train: 2.6435678005218506, validate: 2.9951207637786865\n",
      "Epoch 1/10. Iteration 8400/47167 Losses: train: 3.0950982570648193, validate: 3.001507520675659\n",
      "Epoch 1/10. Iteration 8500/47167 Losses: train: 2.9697375297546387, validate: 2.9972896575927734\n",
      "Epoch 1/10. Iteration 8600/47167 Losses: train: 3.0638973712921143, validate: 2.9958696365356445\n",
      "Epoch 1/10. Iteration 8700/47167 Losses: train: 2.7748754024505615, validate: 2.9983227252960205\n",
      "Epoch 1/10. Iteration 8800/47167 Losses: train: 2.8257038593292236, validate: 2.9974007606506348\n",
      "Epoch 1/10. Iteration 8900/47167 Losses: train: 2.8160829544067383, validate: 2.9895310401916504\n",
      "Epoch 1/10. Iteration 9000/47167 Losses: train: 2.8422741889953613, validate: 2.9943764209747314\n",
      "Epoch 1/10. Iteration 9100/47167 Losses: train: 2.825979232788086, validate: 2.9896485805511475\n",
      "Epoch 1/10. Iteration 9200/47167 Losses: train: 3.0871853828430176, validate: 2.986647844314575\n",
      "Epoch 1/10. Iteration 9300/47167 Losses: train: 2.7673447132110596, validate: 2.9910950660705566\n",
      "Epoch 1/10. Iteration 9400/47167 Losses: train: 2.8736867904663086, validate: 2.9869675636291504\n",
      "Epoch 1/10. Iteration 9500/47167 Losses: train: 3.0725183486938477, validate: 2.9897475242614746\n",
      "Epoch 1/10. Iteration 9600/47167 Losses: train: 3.1188511848449707, validate: 2.9887349605560303\n",
      "Epoch 1/10. Iteration 9700/47167 Losses: train: 3.115480422973633, validate: 2.9954617023468018\n",
      "Epoch 1/10. Iteration 9800/47167 Losses: train: 2.9010112285614014, validate: 2.981191635131836\n",
      "Epoch 1/10. Iteration 9900/47167 Losses: train: 2.779829263687134, validate: 2.9818999767303467\n",
      "Epoch 1/10. Iteration 10000/47167 Losses: train: 2.80953049659729, validate: 2.9896774291992188\n",
      "Epoch 1/10. Iteration 10100/47167 Losses: train: 2.5884296894073486, validate: 2.9902713298797607\n",
      "Epoch 1/10. Iteration 10200/47167 Losses: train: 2.898578643798828, validate: 2.9898698329925537\n",
      "Epoch 1/10. Iteration 10300/47167 Losses: train: 2.902107000350952, validate: 2.985325813293457\n",
      "Epoch 1/10. Iteration 10400/47167 Losses: train: 2.7692642211914062, validate: 2.9858222007751465\n",
      "Epoch 1/10. Iteration 10500/47167 Losses: train: 2.939513683319092, validate: 2.983480930328369\n",
      "Epoch 1/10. Iteration 10600/47167 Losses: train: 2.766063690185547, validate: 2.9873046875\n",
      "Epoch 1/10. Iteration 10700/47167 Losses: train: 2.8362343311309814, validate: 2.9810729026794434\n",
      "Epoch 1/10. Iteration 10800/47167 Losses: train: 2.906409502029419, validate: 2.9870622158050537\n",
      "Epoch 1/10. Iteration 10900/47167 Losses: train: 2.719088077545166, validate: 2.982574224472046\n",
      "Epoch 1/10. Iteration 11000/47167 Losses: train: 2.606868267059326, validate: 2.9786276817321777\n",
      "Epoch 1/10. Iteration 11100/47167 Losses: train: 2.9269721508026123, validate: 2.988374710083008\n",
      "Epoch 1/10. Iteration 11200/47167 Losses: train: 2.8829445838928223, validate: 2.9849555492401123\n",
      "Epoch 1/10. Iteration 11300/47167 Losses: train: 2.8134734630584717, validate: 2.984776258468628\n",
      "Epoch 1/10. Iteration 11400/47167 Losses: train: 3.0655152797698975, validate: 2.9870729446411133\n",
      "Epoch 1/10. Iteration 11500/47167 Losses: train: 2.662285566329956, validate: 2.986560583114624\n",
      "Epoch 1/10. Iteration 11600/47167 Losses: train: 2.898789882659912, validate: 2.981985092163086\n",
      "Epoch 1/10. Iteration 11700/47167 Losses: train: 2.887448787689209, validate: 2.9766619205474854\n",
      "Epoch 1/10. Iteration 11800/47167 Losses: train: 2.8017325401306152, validate: 2.9777750968933105\n",
      "Epoch 1/10. Iteration 11900/47167 Losses: train: 2.6633973121643066, validate: 2.9858694076538086\n",
      "Epoch 1/10. Iteration 12000/47167 Losses: train: 3.0179502964019775, validate: 2.9713480472564697\n",
      "Epoch 1/10. Iteration 12100/47167 Losses: train: 2.851484775543213, validate: 2.979473829269409\n",
      "Epoch 1/10. Iteration 12200/47167 Losses: train: 2.8692595958709717, validate: 2.9774041175842285\n",
      "Epoch 1/10. Iteration 12300/47167 Losses: train: 2.7431955337524414, validate: 2.9784576892852783\n",
      "Epoch 1/10. Iteration 12400/47167 Losses: train: 2.707010507583618, validate: 2.981337308883667\n",
      "Epoch 1/10. Iteration 12500/47167 Losses: train: 2.767254114151001, validate: 2.97424054145813\n",
      "Epoch 1/10. Iteration 12600/47167 Losses: train: 2.7593894004821777, validate: 2.9643959999084473\n",
      "Epoch 1/10. Iteration 12700/47167 Losses: train: 2.8252928256988525, validate: 2.9671027660369873\n",
      "Epoch 1/10. Iteration 12800/47167 Losses: train: 2.800407886505127, validate: 2.9738681316375732\n",
      "Epoch 1/10. Iteration 12900/47167 Losses: train: 2.686649799346924, validate: 2.976644992828369\n",
      "Epoch 1/10. Iteration 13000/47167 Losses: train: 2.8632864952087402, validate: 2.96822190284729\n",
      "Epoch 1/10. Iteration 13100/47167 Losses: train: 2.7634189128875732, validate: 2.9754910469055176\n",
      "Epoch 1/10. Iteration 13200/47167 Losses: train: 2.982950210571289, validate: 2.969594717025757\n",
      "Epoch 1/10. Iteration 13300/47167 Losses: train: 2.7074568271636963, validate: 2.966740608215332\n",
      "Epoch 1/10. Iteration 13400/47167 Losses: train: 2.8292596340179443, validate: 2.977229595184326\n",
      "Epoch 1/10. Iteration 13500/47167 Losses: train: 2.733020544052124, validate: 2.979205846786499\n",
      "Epoch 1/10. Iteration 13600/47167 Losses: train: 2.9281136989593506, validate: 2.9676616191864014\n",
      "Epoch 1/10. Iteration 13700/47167 Losses: train: 3.01973819732666, validate: 2.9637818336486816\n",
      "Epoch 1/10. Iteration 13800/47167 Losses: train: 2.862464189529419, validate: 2.970021963119507\n",
      "Epoch 1/10. Iteration 13900/47167 Losses: train: 2.686188220977783, validate: 2.9676125049591064\n",
      "Epoch 1/10. Iteration 14000/47167 Losses: train: 2.9362869262695312, validate: 2.964454174041748\n",
      "Epoch 1/10. Iteration 14100/47167 Losses: train: 2.7574589252471924, validate: 2.9628593921661377\n",
      "Epoch 1/10. Iteration 14200/47167 Losses: train: 3.032947540283203, validate: 2.973128080368042\n",
      "Epoch 1/10. Iteration 14300/47167 Losses: train: 2.6306798458099365, validate: 2.965237617492676\n",
      "Epoch 1/10. Iteration 14400/47167 Losses: train: 2.9286515712738037, validate: 2.9576025009155273\n",
      "Epoch 1/10. Iteration 14500/47167 Losses: train: 2.7565956115722656, validate: 2.9609127044677734\n",
      "Epoch 1/10. Iteration 14600/47167 Losses: train: 2.872152328491211, validate: 2.972487449645996\n",
      "Epoch 1/10. Iteration 14700/47167 Losses: train: 2.707294225692749, validate: 2.961313486099243\n",
      "Epoch 1/10. Iteration 14800/47167 Losses: train: 2.913250684738159, validate: 2.959523916244507\n",
      "Epoch 1/10. Iteration 14900/47167 Losses: train: 2.832162380218506, validate: 2.9637043476104736\n",
      "Epoch 1/10. Iteration 15000/47167 Losses: train: 3.071718692779541, validate: 2.9540774822235107\n",
      "Epoch 1/10. Iteration 15100/47167 Losses: train: 2.956718921661377, validate: 2.95017671585083\n",
      "Epoch 1/10. Iteration 15200/47167 Losses: train: 2.9581639766693115, validate: 2.9589648246765137\n",
      "Epoch 1/10. Iteration 15300/47167 Losses: train: 3.046926259994507, validate: 2.95660400390625\n",
      "Epoch 1/10. Iteration 15400/47167 Losses: train: 2.674229383468628, validate: 2.951814889907837\n",
      "Epoch 1/10. Iteration 15500/47167 Losses: train: 2.9292049407958984, validate: 2.9453628063201904\n",
      "Epoch 1/10. Iteration 15600/47167 Losses: train: 2.62485408782959, validate: 2.9516525268554688\n",
      "Epoch 1/10. Iteration 15700/47167 Losses: train: 2.7621307373046875, validate: 2.9511611461639404\n",
      "Epoch 1/10. Iteration 15800/47167 Losses: train: 2.7777366638183594, validate: 2.9506945610046387\n",
      "Epoch 1/10. Iteration 15900/47167 Losses: train: 2.741121768951416, validate: 2.9504055976867676\n",
      "Epoch 1/10. Iteration 16000/47167 Losses: train: 2.7751693725585938, validate: 2.939429998397827\n",
      "Epoch 1/10. Iteration 16100/47167 Losses: train: 2.813793182373047, validate: 2.9460432529449463\n",
      "Epoch 1/10. Iteration 16200/47167 Losses: train: 2.860773801803589, validate: 2.940852642059326\n",
      "Epoch 1/10. Iteration 16300/47167 Losses: train: 2.6777586936950684, validate: 2.9441049098968506\n",
      "Epoch 1/10. Iteration 16400/47167 Losses: train: 2.862828016281128, validate: 2.94773268699646\n",
      "Epoch 1/10. Iteration 16500/47167 Losses: train: 2.8382201194763184, validate: 2.942115306854248\n",
      "Epoch 1/10. Iteration 16600/47167 Losses: train: 2.756758451461792, validate: 2.942192792892456\n",
      "Epoch 1/10. Iteration 16700/47167 Losses: train: 2.8197689056396484, validate: 2.939709186553955\n",
      "Epoch 1/10. Iteration 16800/47167 Losses: train: 2.6738803386688232, validate: 2.940843105316162\n",
      "Epoch 1/10. Iteration 16900/47167 Losses: train: 2.861576557159424, validate: 2.9373698234558105\n",
      "Epoch 1/10. Iteration 17000/47167 Losses: train: 2.963233709335327, validate: 2.939382791519165\n",
      "Epoch 1/10. Iteration 17100/47167 Losses: train: 2.842944383621216, validate: 2.9382827281951904\n",
      "Epoch 1/10. Iteration 17200/47167 Losses: train: 2.8170316219329834, validate: 2.9364864826202393\n",
      "Epoch 1/10. Iteration 17300/47167 Losses: train: 2.832149028778076, validate: 2.941253900527954\n",
      "Epoch 1/10. Iteration 17400/47167 Losses: train: 2.5957746505737305, validate: 2.938758373260498\n",
      "Epoch 1/10. Iteration 17500/47167 Losses: train: 2.8043458461761475, validate: 2.9460935592651367\n",
      "Epoch 1/10. Iteration 17600/47167 Losses: train: 2.98246431350708, validate: 2.9406020641326904\n",
      "Epoch 1/10. Iteration 17700/47167 Losses: train: 2.756357192993164, validate: 2.939741373062134\n",
      "Epoch 1/10. Iteration 17800/47167 Losses: train: 2.8211536407470703, validate: 2.9284298419952393\n",
      "Epoch 1/10. Iteration 17900/47167 Losses: train: 2.9711689949035645, validate: 2.930504322052002\n",
      "Epoch 1/10. Iteration 18000/47167 Losses: train: 2.708040952682495, validate: 2.9435551166534424\n",
      "Epoch 1/10. Iteration 18100/47167 Losses: train: 2.756334066390991, validate: 2.9444756507873535\n",
      "Epoch 1/10. Iteration 18200/47167 Losses: train: 2.872318744659424, validate: 2.9350414276123047\n",
      "Epoch 1/10. Iteration 18300/47167 Losses: train: 2.5621538162231445, validate: 2.939990997314453\n",
      "Epoch 1/10. Iteration 18400/47167 Losses: train: 2.766352891921997, validate: 2.9443018436431885\n",
      "Epoch 1/10. Iteration 18500/47167 Losses: train: 2.7838776111602783, validate: 2.9409170150756836\n",
      "Epoch 1/10. Iteration 18600/47167 Losses: train: 2.74080228805542, validate: 2.932650566101074\n",
      "Epoch 1/10. Iteration 18700/47167 Losses: train: 3.014836549758911, validate: 2.9303648471832275\n",
      "Epoch 1/10. Iteration 18800/47167 Losses: train: 2.747072219848633, validate: 2.92820405960083\n",
      "Epoch 1/10. Iteration 18900/47167 Losses: train: 2.8895702362060547, validate: 2.9279086589813232\n",
      "Epoch 1/10. Iteration 19000/47167 Losses: train: 2.7986867427825928, validate: 2.93245267868042\n",
      "Epoch 1/10. Iteration 19100/47167 Losses: train: 2.734832525253296, validate: 2.922748565673828\n",
      "Epoch 1/10. Iteration 19200/47167 Losses: train: 2.7869908809661865, validate: 2.9269797801971436\n",
      "Epoch 1/10. Iteration 19300/47167 Losses: train: 2.959524393081665, validate: 2.9322173595428467\n",
      "Epoch 1/10. Iteration 19400/47167 Losses: train: 2.932673931121826, validate: 2.9363789558410645\n",
      "Epoch 1/10. Iteration 19500/47167 Losses: train: 2.746060609817505, validate: 2.9421207904815674\n",
      "Epoch 1/10. Iteration 19600/47167 Losses: train: 2.874209403991699, validate: 2.933964252471924\n",
      "Epoch 1/10. Iteration 19700/47167 Losses: train: 2.805858850479126, validate: 2.9294915199279785\n",
      "Epoch 1/10. Iteration 19800/47167 Losses: train: 2.5221681594848633, validate: 2.9334049224853516\n",
      "Epoch 1/10. Iteration 19900/47167 Losses: train: 2.6883111000061035, validate: 2.9371235370635986\n",
      "Epoch 1/10. Iteration 20000/47167 Losses: train: 2.766404390335083, validate: 2.9354872703552246\n",
      "Epoch 1/10. Iteration 20100/47167 Losses: train: 2.6935946941375732, validate: 2.9273476600646973\n",
      "Epoch 1/10. Iteration 20200/47167 Losses: train: 2.828625202178955, validate: 2.9369547367095947\n",
      "Epoch 1/10. Iteration 20300/47167 Losses: train: 2.734828233718872, validate: 2.934232234954834\n",
      "Epoch 1/10. Iteration 20400/47167 Losses: train: 2.771535634994507, validate: 2.929941415786743\n",
      "Epoch 1/10. Iteration 20500/47167 Losses: train: 2.833402156829834, validate: 2.928164005279541\n",
      "Epoch 1/10. Iteration 20600/47167 Losses: train: 2.7016761302948, validate: 2.922769069671631\n",
      "Epoch 1/10. Iteration 20700/47167 Losses: train: 2.674339532852173, validate: 2.9213287830352783\n",
      "Epoch 1/10. Iteration 20800/47167 Losses: train: 2.591994285583496, validate: 2.928913116455078\n",
      "Epoch 1/10. Iteration 20900/47167 Losses: train: 2.7452383041381836, validate: 2.9392707347869873\n",
      "Epoch 1/10. Iteration 21000/47167 Losses: train: 2.894939422607422, validate: 2.926715612411499\n",
      "Epoch 1/10. Iteration 21100/47167 Losses: train: 2.7400357723236084, validate: 2.9200644493103027\n",
      "Epoch 1/10. Iteration 21200/47167 Losses: train: 2.802919626235962, validate: 2.9149422645568848\n",
      "Epoch 1/10. Iteration 21300/47167 Losses: train: 2.7432444095611572, validate: 2.919163465499878\n",
      "Epoch 1/10. Iteration 21400/47167 Losses: train: 2.637526273727417, validate: 2.911428213119507\n",
      "Epoch 1/10. Iteration 21500/47167 Losses: train: 2.8865723609924316, validate: 2.922520160675049\n",
      "Epoch 1/10. Iteration 21600/47167 Losses: train: 2.7535622119903564, validate: 2.9196271896362305\n",
      "Epoch 1/10. Iteration 21700/47167 Losses: train: 2.689849376678467, validate: 2.931724786758423\n",
      "Epoch 1/10. Iteration 21800/47167 Losses: train: 2.728647470474243, validate: 2.9220497608184814\n",
      "Epoch 1/10. Iteration 21900/47167 Losses: train: 2.668278455734253, validate: 2.9270710945129395\n",
      "Epoch 1/10. Iteration 22000/47167 Losses: train: 2.7555017471313477, validate: 2.925734043121338\n",
      "Epoch 1/10. Iteration 22100/47167 Losses: train: 2.708634853363037, validate: 2.9241347312927246\n",
      "Epoch 1/10. Iteration 22200/47167 Losses: train: 2.8689284324645996, validate: 2.9072673320770264\n",
      "Epoch 1/10. Iteration 22300/47167 Losses: train: 2.8687427043914795, validate: 2.905930757522583\n",
      "Epoch 1/10. Iteration 22400/47167 Losses: train: 2.703068494796753, validate: 2.9104909896850586\n",
      "Epoch 1/10. Iteration 22500/47167 Losses: train: 2.511028528213501, validate: 2.910701274871826\n",
      "Epoch 1/10. Iteration 22600/47167 Losses: train: 2.8799352645874023, validate: 2.9114952087402344\n",
      "Epoch 1/10. Iteration 22700/47167 Losses: train: 2.725846290588379, validate: 2.915497064590454\n",
      "Epoch 1/10. Iteration 22800/47167 Losses: train: 2.7545599937438965, validate: 2.904658794403076\n",
      "Epoch 1/10. Iteration 22900/47167 Losses: train: 2.921436309814453, validate: 2.902622699737549\n",
      "Epoch 1/10. Iteration 23000/47167 Losses: train: 2.906144618988037, validate: 2.899883985519409\n",
      "Epoch 1/10. Iteration 23100/47167 Losses: train: 2.7335939407348633, validate: 2.8925700187683105\n",
      "Epoch 1/10. Iteration 23200/47167 Losses: train: 2.7872185707092285, validate: 2.9040679931640625\n",
      "Epoch 1/10. Iteration 23300/47167 Losses: train: 3.0593485832214355, validate: 2.9079957008361816\n",
      "Epoch 1/10. Iteration 23400/47167 Losses: train: 2.8717916011810303, validate: 2.899756669998169\n",
      "Epoch 1/10. Iteration 23500/47167 Losses: train: 2.583566427230835, validate: 2.900639295578003\n",
      "Epoch 1/10. Iteration 23600/47167 Losses: train: 2.606563091278076, validate: 2.908450126647949\n",
      "Epoch 1/10. Iteration 23700/47167 Losses: train: 2.898444175720215, validate: 2.9094061851501465\n",
      "Epoch 1/10. Iteration 23800/47167 Losses: train: 2.8001813888549805, validate: 2.90326189994812\n",
      "Epoch 1/10. Iteration 23900/47167 Losses: train: 2.6870687007904053, validate: 2.9017271995544434\n",
      "Epoch 1/10. Iteration 24000/47167 Losses: train: 2.91072154045105, validate: 2.8956046104431152\n",
      "Epoch 1/10. Iteration 24100/47167 Losses: train: 2.948370933532715, validate: 2.8995471000671387\n",
      "Epoch 1/10. Iteration 24200/47167 Losses: train: 2.904435634613037, validate: 2.898388624191284\n",
      "Epoch 1/10. Iteration 24300/47167 Losses: train: 2.836024045944214, validate: 2.897977113723755\n",
      "Epoch 1/10. Iteration 24400/47167 Losses: train: 2.447676658630371, validate: 2.8981313705444336\n",
      "Epoch 1/10. Iteration 24500/47167 Losses: train: 2.7628204822540283, validate: 2.9053945541381836\n",
      "Epoch 1/10. Iteration 24600/47167 Losses: train: 2.975571870803833, validate: 2.9068148136138916\n",
      "Epoch 1/10. Iteration 24700/47167 Losses: train: 2.8944902420043945, validate: 2.9014534950256348\n",
      "Epoch 1/10. Iteration 24800/47167 Losses: train: 2.648252010345459, validate: 2.8991055488586426\n",
      "Epoch 1/10. Iteration 24900/47167 Losses: train: 2.8990702629089355, validate: 2.891474485397339\n",
      "Epoch 1/10. Iteration 25000/47167 Losses: train: 2.748461961746216, validate: 2.896091938018799\n",
      "Epoch 1/10. Iteration 25100/47167 Losses: train: 2.8218491077423096, validate: 2.897857427597046\n",
      "Epoch 1/10. Iteration 25200/47167 Losses: train: 2.7284679412841797, validate: 2.8968331813812256\n",
      "Epoch 1/10. Iteration 25300/47167 Losses: train: 2.998279571533203, validate: 2.8904407024383545\n",
      "Epoch 1/10. Iteration 25400/47167 Losses: train: 2.905535936355591, validate: 2.891890287399292\n",
      "Epoch 1/10. Iteration 25500/47167 Losses: train: 2.8908016681671143, validate: 2.8921356201171875\n",
      "Epoch 1/10. Iteration 25600/47167 Losses: train: 2.781109094619751, validate: 2.886871576309204\n",
      "Epoch 1/10. Iteration 25700/47167 Losses: train: 2.997095823287964, validate: 2.8941638469696045\n",
      "Epoch 1/10. Iteration 25800/47167 Losses: train: 2.6534488201141357, validate: 2.893200397491455\n",
      "Epoch 1/10. Iteration 25900/47167 Losses: train: 2.8684418201446533, validate: 2.8917269706726074\n",
      "Epoch 1/10. Iteration 26000/47167 Losses: train: 2.6030824184417725, validate: 2.895294189453125\n",
      "Epoch 1/10. Iteration 26100/47167 Losses: train: 2.6066174507141113, validate: 2.8958098888397217\n",
      "Epoch 1/10. Iteration 26200/47167 Losses: train: 2.7737464904785156, validate: 2.8835880756378174\n",
      "Epoch 1/10. Iteration 26300/47167 Losses: train: 2.5707736015319824, validate: 2.895442485809326\n",
      "Epoch 1/10. Iteration 26400/47167 Losses: train: 2.6260244846343994, validate: 2.8811581134796143\n",
      "Epoch 1/10. Iteration 26500/47167 Losses: train: 2.8969202041625977, validate: 2.8802244663238525\n",
      "Epoch 1/10. Iteration 26600/47167 Losses: train: 2.815183639526367, validate: 2.8901584148406982\n",
      "Epoch 1/10. Iteration 26700/47167 Losses: train: 2.7268524169921875, validate: 2.891369104385376\n",
      "Epoch 1/10. Iteration 26800/47167 Losses: train: 2.9885294437408447, validate: 2.8909685611724854\n",
      "Epoch 1/10. Iteration 26900/47167 Losses: train: 2.8008787631988525, validate: 2.8919291496276855\n",
      "Epoch 1/10. Iteration 27000/47167 Losses: train: 2.711303472518921, validate: 2.890019655227661\n",
      "Epoch 1/10. Iteration 27100/47167 Losses: train: 2.620018243789673, validate: 2.8883564472198486\n",
      "Epoch 1/10. Iteration 27200/47167 Losses: train: 3.0779175758361816, validate: 2.8907310962677\n",
      "Epoch 1/10. Iteration 27300/47167 Losses: train: 2.798126220703125, validate: 2.888465166091919\n",
      "Epoch 1/10. Iteration 27400/47167 Losses: train: 2.693988561630249, validate: 2.887359380722046\n",
      "Epoch 1/10. Iteration 27500/47167 Losses: train: 2.777273178100586, validate: 2.892631769180298\n",
      "Epoch 1/10. Iteration 27600/47167 Losses: train: 2.7181148529052734, validate: 2.8874402046203613\n",
      "Epoch 1/10. Iteration 27700/47167 Losses: train: 2.693483591079712, validate: 2.884694814682007\n",
      "Epoch 1/10. Iteration 27800/47167 Losses: train: 2.761140823364258, validate: 2.895048141479492\n",
      "Epoch 1/10. Iteration 27900/47167 Losses: train: 2.863917350769043, validate: 2.886540651321411\n",
      "Epoch 1/10. Iteration 28000/47167 Losses: train: 2.8257181644439697, validate: 2.8893203735351562\n",
      "Epoch 1/10. Iteration 28100/47167 Losses: train: 2.7418394088745117, validate: 2.8817203044891357\n",
      "Epoch 1/10. Iteration 28200/47167 Losses: train: 2.8087527751922607, validate: 2.881211519241333\n",
      "Epoch 1/10. Iteration 28300/47167 Losses: train: 2.834355115890503, validate: 2.878053665161133\n",
      "Epoch 1/10. Iteration 28400/47167 Losses: train: 2.679673433303833, validate: 2.8771555423736572\n",
      "Epoch 1/10. Iteration 28500/47167 Losses: train: 2.731020927429199, validate: 2.8868603706359863\n",
      "Epoch 1/10. Iteration 28600/47167 Losses: train: 2.797410726547241, validate: 2.8760769367218018\n",
      "Epoch 1/10. Iteration 28700/47167 Losses: train: 2.628974199295044, validate: 2.876972198486328\n",
      "Epoch 1/10. Iteration 28800/47167 Losses: train: 2.7873899936676025, validate: 2.8809010982513428\n",
      "Epoch 1/10. Iteration 28900/47167 Losses: train: 2.7024126052856445, validate: 2.8830316066741943\n",
      "Epoch 1/10. Iteration 29000/47167 Losses: train: 2.8824424743652344, validate: 2.8802692890167236\n",
      "Epoch 1/10. Iteration 29100/47167 Losses: train: 2.7459421157836914, validate: 2.88283109664917\n",
      "Epoch 1/10. Iteration 29200/47167 Losses: train: 2.7210638523101807, validate: 2.874138832092285\n",
      "Epoch 1/10. Iteration 29300/47167 Losses: train: 2.704596996307373, validate: 2.875448226928711\n",
      "Epoch 1/10. Iteration 29400/47167 Losses: train: 2.7816343307495117, validate: 2.8717830181121826\n",
      "Epoch 1/10. Iteration 29500/47167 Losses: train: 2.690932512283325, validate: 2.867408037185669\n",
      "Epoch 1/10. Iteration 29600/47167 Losses: train: 2.7437779903411865, validate: 2.8700456619262695\n",
      "Epoch 1/10. Iteration 29700/47167 Losses: train: 2.730581045150757, validate: 2.871762752532959\n",
      "Epoch 1/10. Iteration 29800/47167 Losses: train: 2.6972150802612305, validate: 2.873300552368164\n",
      "Epoch 1/10. Iteration 29900/47167 Losses: train: 2.6207633018493652, validate: 2.8689124584198\n",
      "Epoch 1/10. Iteration 30000/47167 Losses: train: 2.7187814712524414, validate: 2.871586322784424\n",
      "Epoch 1/10. Iteration 30100/47167 Losses: train: 2.7697083950042725, validate: 2.8710312843322754\n",
      "Epoch 1/10. Iteration 30200/47167 Losses: train: 2.8902742862701416, validate: 2.8671836853027344\n",
      "Epoch 1/10. Iteration 30300/47167 Losses: train: 2.6791539192199707, validate: 2.864037036895752\n",
      "Epoch 1/10. Iteration 30400/47167 Losses: train: 2.844008207321167, validate: 2.876765012741089\n",
      "Epoch 1/10. Iteration 30500/47167 Losses: train: 2.8637993335723877, validate: 2.875866651535034\n",
      "Epoch 1/10. Iteration 30600/47167 Losses: train: 2.5901663303375244, validate: 2.8660941123962402\n",
      "Epoch 1/10. Iteration 30700/47167 Losses: train: 2.7033908367156982, validate: 2.86956787109375\n",
      "Epoch 1/10. Iteration 30800/47167 Losses: train: 2.425931215286255, validate: 2.865224599838257\n",
      "Epoch 1/10. Iteration 30900/47167 Losses: train: 2.8204007148742676, validate: 2.867828607559204\n",
      "Epoch 1/10. Iteration 31000/47167 Losses: train: 2.6347873210906982, validate: 2.8630623817443848\n",
      "Epoch 1/10. Iteration 31100/47167 Losses: train: 2.781217098236084, validate: 2.867100954055786\n",
      "Epoch 1/10. Iteration 31200/47167 Losses: train: 2.5525786876678467, validate: 2.8477065563201904\n",
      "Epoch 1/10. Iteration 31300/47167 Losses: train: 2.6554925441741943, validate: 2.863400459289551\n",
      "Epoch 1/10. Iteration 31400/47167 Losses: train: 3.0672812461853027, validate: 2.8617429733276367\n",
      "Epoch 1/10. Iteration 31500/47167 Losses: train: 2.735553741455078, validate: 2.861567974090576\n",
      "Epoch 1/10. Iteration 31600/47167 Losses: train: 2.625134229660034, validate: 2.862555742263794\n",
      "Epoch 1/10. Iteration 31700/47167 Losses: train: 2.8201396465301514, validate: 2.860475778579712\n",
      "Epoch 1/10. Iteration 31800/47167 Losses: train: 2.5869743824005127, validate: 2.863922119140625\n",
      "Epoch 1/10. Iteration 31900/47167 Losses: train: 2.6777760982513428, validate: 2.8633675575256348\n",
      "Epoch 1/10. Iteration 32000/47167 Losses: train: 2.593127489089966, validate: 2.8694121837615967\n",
      "Epoch 1/10. Iteration 32100/47167 Losses: train: 2.682553291320801, validate: 2.860170364379883\n",
      "Epoch 1/10. Iteration 32200/47167 Losses: train: 2.597275733947754, validate: 2.865356922149658\n",
      "Epoch 1/10. Iteration 32300/47167 Losses: train: 2.836718797683716, validate: 2.861884117126465\n",
      "Epoch 1/10. Iteration 32400/47167 Losses: train: 2.5467162132263184, validate: 2.8543660640716553\n",
      "Epoch 1/10. Iteration 32500/47167 Losses: train: 2.6533472537994385, validate: 2.855149745941162\n",
      "Epoch 1/10. Iteration 32600/47167 Losses: train: 2.7896270751953125, validate: 2.856984853744507\n",
      "Epoch 1/10. Iteration 32700/47167 Losses: train: 2.73669171333313, validate: 2.851278781890869\n",
      "Epoch 1/10. Iteration 32800/47167 Losses: train: 2.767742156982422, validate: 2.845445156097412\n",
      "Epoch 1/10. Iteration 32900/47167 Losses: train: 2.6202783584594727, validate: 2.8608756065368652\n",
      "Epoch 1/10. Iteration 33000/47167 Losses: train: 2.7420060634613037, validate: 2.8579905033111572\n",
      "Epoch 1/10. Iteration 33100/47167 Losses: train: 2.9567625522613525, validate: 2.8581767082214355\n",
      "Epoch 1/10. Iteration 33200/47167 Losses: train: 2.808666944503784, validate: 2.8542397022247314\n",
      "Epoch 1/10. Iteration 33300/47167 Losses: train: 2.9543519020080566, validate: 2.8544390201568604\n",
      "Epoch 1/10. Iteration 33400/47167 Losses: train: 2.589409828186035, validate: 2.857301950454712\n",
      "Epoch 1/10. Iteration 33500/47167 Losses: train: 2.83027720451355, validate: 2.8492348194122314\n",
      "Epoch 1/10. Iteration 33600/47167 Losses: train: 2.7273197174072266, validate: 2.8526999950408936\n",
      "Epoch 1/10. Iteration 33700/47167 Losses: train: 2.641711950302124, validate: 2.8546059131622314\n",
      "Epoch 1/10. Iteration 33800/47167 Losses: train: 2.6756303310394287, validate: 2.851581335067749\n",
      "Epoch 1/10. Iteration 33900/47167 Losses: train: 2.6327269077301025, validate: 2.8523154258728027\n",
      "Epoch 1/10. Iteration 34000/47167 Losses: train: 2.666208267211914, validate: 2.8576290607452393\n",
      "Epoch 1/10. Iteration 34100/47167 Losses: train: 2.8169379234313965, validate: 2.8579678535461426\n",
      "Epoch 1/10. Iteration 34200/47167 Losses: train: 2.6884572505950928, validate: 2.8533074855804443\n",
      "Epoch 1/10. Iteration 34300/47167 Losses: train: 2.887547492980957, validate: 2.851421356201172\n",
      "Epoch 1/10. Iteration 34400/47167 Losses: train: 2.6881392002105713, validate: 2.84483003616333\n",
      "Epoch 1/10. Iteration 34500/47167 Losses: train: 2.8128576278686523, validate: 2.839935541152954\n",
      "Epoch 1/10. Iteration 34600/47167 Losses: train: 2.856712579727173, validate: 2.856177806854248\n",
      "Epoch 1/10. Iteration 34700/47167 Losses: train: 2.844956159591675, validate: 2.853912830352783\n",
      "Epoch 1/10. Iteration 34800/47167 Losses: train: 2.793830156326294, validate: 2.8559842109680176\n",
      "Epoch 1/10. Iteration 34900/47167 Losses: train: 2.7205159664154053, validate: 2.8541619777679443\n",
      "Epoch 1/10. Iteration 35000/47167 Losses: train: 2.833991527557373, validate: 2.8499491214752197\n",
      "Epoch 1/10. Iteration 35100/47167 Losses: train: 2.839872360229492, validate: 2.8397748470306396\n",
      "Epoch 1/10. Iteration 35200/47167 Losses: train: 2.5326600074768066, validate: 2.8493266105651855\n",
      "Epoch 1/10. Iteration 35300/47167 Losses: train: 2.8550612926483154, validate: 2.8543787002563477\n",
      "Epoch 1/10. Iteration 35400/47167 Losses: train: 2.7196104526519775, validate: 2.8559250831604004\n",
      "Epoch 1/10. Iteration 35500/47167 Losses: train: 2.7019612789154053, validate: 2.838709592819214\n",
      "Epoch 1/10. Iteration 35600/47167 Losses: train: 2.745313882827759, validate: 2.8397231101989746\n",
      "Epoch 1/10. Iteration 35700/47167 Losses: train: 2.676888942718506, validate: 2.8459086418151855\n",
      "Epoch 1/10. Iteration 35800/47167 Losses: train: 2.6246535778045654, validate: 2.838017225265503\n",
      "Epoch 1/10. Iteration 35900/47167 Losses: train: 2.4946272373199463, validate: 2.8462836742401123\n",
      "Epoch 1/10. Iteration 36000/47167 Losses: train: 2.805190086364746, validate: 2.8456833362579346\n",
      "Epoch 1/10. Iteration 36100/47167 Losses: train: 2.5454001426696777, validate: 2.8481857776641846\n",
      "Epoch 1/10. Iteration 36200/47167 Losses: train: 2.425150156021118, validate: 2.842805862426758\n",
      "Epoch 1/10. Iteration 36300/47167 Losses: train: 2.8504719734191895, validate: 2.843693971633911\n",
      "Epoch 1/10. Iteration 36400/47167 Losses: train: 2.812953233718872, validate: 2.8515846729278564\n",
      "Epoch 1/10. Iteration 36500/47167 Losses: train: 2.6479053497314453, validate: 2.8406453132629395\n",
      "Epoch 1/10. Iteration 36600/47167 Losses: train: 2.606065511703491, validate: 2.8369956016540527\n",
      "Epoch 1/10. Iteration 36700/47167 Losses: train: 2.927748918533325, validate: 2.842015027999878\n",
      "Epoch 1/10. Iteration 36800/47167 Losses: train: 2.6184794902801514, validate: 2.8390896320343018\n",
      "Epoch 1/10. Iteration 36900/47167 Losses: train: 2.703458547592163, validate: 2.8379199504852295\n",
      "Epoch 1/10. Iteration 37000/47167 Losses: train: 2.514594554901123, validate: 2.838498592376709\n",
      "Epoch 1/10. Iteration 37100/47167 Losses: train: 2.7148425579071045, validate: 2.8384106159210205\n",
      "Epoch 1/10. Iteration 37200/47167 Losses: train: 2.689357042312622, validate: 2.847891092300415\n",
      "Epoch 1/10. Iteration 37300/47167 Losses: train: 2.709139108657837, validate: 2.8384134769439697\n",
      "Epoch 1/10. Iteration 37400/47167 Losses: train: 2.570948839187622, validate: 2.8351364135742188\n",
      "Epoch 1/10. Iteration 37500/47167 Losses: train: 2.539074182510376, validate: 2.838998556137085\n",
      "Epoch 1/10. Iteration 37600/47167 Losses: train: 2.7167444229125977, validate: 2.835545539855957\n",
      "Epoch 1/10. Iteration 37700/47167 Losses: train: 2.525064468383789, validate: 2.831784963607788\n",
      "Epoch 1/10. Iteration 37800/47167 Losses: train: 2.792341947555542, validate: 2.837888240814209\n",
      "Epoch 1/10. Iteration 37900/47167 Losses: train: 2.77687931060791, validate: 2.8366754055023193\n",
      "Epoch 1/10. Iteration 38000/47167 Losses: train: 2.7459781169891357, validate: 2.8402786254882812\n",
      "Epoch 1/10. Iteration 38100/47167 Losses: train: 2.6886181831359863, validate: 2.8422698974609375\n",
      "Epoch 1/10. Iteration 38200/47167 Losses: train: 2.7150626182556152, validate: 2.848111629486084\n",
      "Epoch 1/10. Iteration 38300/47167 Losses: train: 2.755347728729248, validate: 2.8454830646514893\n",
      "Epoch 1/10. Iteration 38400/47167 Losses: train: 2.669445276260376, validate: 2.841925859451294\n",
      "Epoch 1/10. Iteration 38500/47167 Losses: train: 2.571451187133789, validate: 2.8366987705230713\n",
      "Epoch 1/10. Iteration 38600/47167 Losses: train: 2.5457539558410645, validate: 2.837583541870117\n",
      "Epoch 1/10. Iteration 38700/47167 Losses: train: 2.635648250579834, validate: 2.8325045108795166\n",
      "Epoch 1/10. Iteration 38800/47167 Losses: train: 2.8091180324554443, validate: 2.833512306213379\n",
      "Epoch 1/10. Iteration 38900/47167 Losses: train: 2.6625547409057617, validate: 2.841010808944702\n",
      "Epoch 1/10. Iteration 39000/47167 Losses: train: 2.6855218410491943, validate: 2.839540719985962\n",
      "Epoch 1/10. Iteration 39100/47167 Losses: train: 2.8124001026153564, validate: 2.8348381519317627\n",
      "Epoch 1/10. Iteration 39200/47167 Losses: train: 2.4519565105438232, validate: 2.834550380706787\n",
      "Epoch 1/10. Iteration 39300/47167 Losses: train: 2.7038462162017822, validate: 2.8225457668304443\n",
      "Epoch 1/10. Iteration 39400/47167 Losses: train: 2.925692081451416, validate: 2.8206100463867188\n",
      "Epoch 1/10. Iteration 39500/47167 Losses: train: 2.824820041656494, validate: 2.8279972076416016\n",
      "Epoch 1/10. Iteration 39600/47167 Losses: train: 2.6680617332458496, validate: 2.828778028488159\n",
      "Epoch 1/10. Iteration 39700/47167 Losses: train: 2.9274649620056152, validate: 2.832104444503784\n",
      "Epoch 1/10. Iteration 39800/47167 Losses: train: 2.669600248336792, validate: 2.8278913497924805\n",
      "Epoch 1/10. Iteration 39900/47167 Losses: train: 2.631953477859497, validate: 2.8318164348602295\n",
      "Epoch 1/10. Iteration 40000/47167 Losses: train: 2.736212968826294, validate: 2.829089879989624\n",
      "Epoch 1/10. Iteration 40100/47167 Losses: train: 2.6663713455200195, validate: 2.826158046722412\n",
      "Epoch 1/10. Iteration 40200/47167 Losses: train: 2.7296454906463623, validate: 2.8320159912109375\n",
      "Epoch 1/10. Iteration 40300/47167 Losses: train: 2.6215415000915527, validate: 2.8261287212371826\n",
      "Epoch 1/10. Iteration 40400/47167 Losses: train: 2.658966541290283, validate: 2.8222782611846924\n",
      "Epoch 1/10. Iteration 40500/47167 Losses: train: 2.608184337615967, validate: 2.8287055492401123\n",
      "Epoch 1/10. Iteration 40600/47167 Losses: train: 2.6818888187408447, validate: 2.833960771560669\n",
      "Epoch 1/10. Iteration 40700/47167 Losses: train: 2.7722620964050293, validate: 2.836232900619507\n",
      "Epoch 1/10. Iteration 40800/47167 Losses: train: 2.514681816101074, validate: 2.8266921043395996\n",
      "Epoch 1/10. Iteration 40900/47167 Losses: train: 2.644740581512451, validate: 2.8274621963500977\n",
      "Epoch 1/10. Iteration 41000/47167 Losses: train: 2.7182068824768066, validate: 2.819777250289917\n",
      "Epoch 1/10. Iteration 41100/47167 Losses: train: 2.8298394680023193, validate: 2.815441608428955\n",
      "Epoch 1/10. Iteration 41200/47167 Losses: train: 2.476489543914795, validate: 2.8227434158325195\n",
      "Epoch 1/10. Iteration 41300/47167 Losses: train: 2.5385749340057373, validate: 2.827312707901001\n",
      "Epoch 1/10. Iteration 41400/47167 Losses: train: 2.7276771068573, validate: 2.8249752521514893\n",
      "Epoch 1/10. Iteration 41500/47167 Losses: train: 2.7193853855133057, validate: 2.83128023147583\n",
      "Epoch 1/10. Iteration 41600/47167 Losses: train: 2.837991952896118, validate: 2.8209099769592285\n",
      "Epoch 1/10. Iteration 41700/47167 Losses: train: 2.680202007293701, validate: 2.8330764770507812\n",
      "Epoch 1/10. Iteration 41800/47167 Losses: train: 2.5981483459472656, validate: 2.8214914798736572\n",
      "Epoch 1/10. Iteration 41900/47167 Losses: train: 2.802124500274658, validate: 2.8302419185638428\n",
      "Epoch 1/10. Iteration 42000/47167 Losses: train: 2.6234188079833984, validate: 2.8267412185668945\n",
      "Epoch 1/10. Iteration 42100/47167 Losses: train: 2.8070147037506104, validate: 2.835012674331665\n",
      "Epoch 1/10. Iteration 42200/47167 Losses: train: 2.5887832641601562, validate: 2.830705165863037\n",
      "Epoch 1/10. Iteration 42300/47167 Losses: train: 2.7380714416503906, validate: 2.8284265995025635\n",
      "Epoch 1/10. Iteration 42400/47167 Losses: train: 2.5885279178619385, validate: 2.832216501235962\n",
      "Epoch 1/10. Iteration 42500/47167 Losses: train: 2.686811685562134, validate: 2.8258395195007324\n",
      "Epoch 1/10. Iteration 42600/47167 Losses: train: 2.8546738624572754, validate: 2.8221969604492188\n",
      "Epoch 1/10. Iteration 42700/47167 Losses: train: 2.7626495361328125, validate: 2.8287582397460938\n",
      "Epoch 1/10. Iteration 42800/47167 Losses: train: 2.579710006713867, validate: 2.819056987762451\n",
      "Epoch 1/10. Iteration 42900/47167 Losses: train: 2.803583860397339, validate: 2.8204636573791504\n",
      "Epoch 1/10. Iteration 43000/47167 Losses: train: 2.60654354095459, validate: 2.823448657989502\n",
      "Epoch 1/10. Iteration 43100/47167 Losses: train: 2.618377923965454, validate: 2.8292043209075928\n",
      "Epoch 1/10. Iteration 43200/47167 Losses: train: 2.844149589538574, validate: 2.826320171356201\n",
      "Epoch 1/10. Iteration 43300/47167 Losses: train: 2.5735225677490234, validate: 2.828113079071045\n",
      "Epoch 1/10. Iteration 43400/47167 Losses: train: 2.7939374446868896, validate: 2.8141777515411377\n",
      "Epoch 1/10. Iteration 43500/47167 Losses: train: 2.4722113609313965, validate: 2.8267412185668945\n",
      "Epoch 1/10. Iteration 43600/47167 Losses: train: 2.6919617652893066, validate: 2.8176214694976807\n",
      "Epoch 1/10. Iteration 43700/47167 Losses: train: 2.654306411743164, validate: 2.8208987712860107\n",
      "Epoch 1/10. Iteration 43800/47167 Losses: train: 2.6718907356262207, validate: 2.8197009563446045\n",
      "Epoch 1/10. Iteration 43900/47167 Losses: train: 2.776259183883667, validate: 2.8120861053466797\n",
      "Epoch 1/10. Iteration 44000/47167 Losses: train: 2.778043508529663, validate: 2.8096470832824707\n",
      "Epoch 1/10. Iteration 44100/47167 Losses: train: 2.7695322036743164, validate: 2.8104665279388428\n",
      "Epoch 1/10. Iteration 44200/47167 Losses: train: 2.5497753620147705, validate: 2.8039708137512207\n",
      "Epoch 1/10. Iteration 44300/47167 Losses: train: 2.643549680709839, validate: 2.810575485229492\n",
      "Epoch 1/10. Iteration 44400/47167 Losses: train: 2.7308783531188965, validate: 2.8072121143341064\n",
      "Epoch 1/10. Iteration 44500/47167 Losses: train: 2.7610645294189453, validate: 2.815723180770874\n",
      "Epoch 1/10. Iteration 44600/47167 Losses: train: 2.591196298599243, validate: 2.8137826919555664\n",
      "Epoch 1/10. Iteration 44700/47167 Losses: train: 2.772554874420166, validate: 2.8116118907928467\n",
      "Epoch 1/10. Iteration 44800/47167 Losses: train: 2.8150856494903564, validate: 2.809201955795288\n",
      "Epoch 1/10. Iteration 44900/47167 Losses: train: 2.7654314041137695, validate: 2.805281162261963\n",
      "Epoch 1/10. Iteration 45000/47167 Losses: train: 2.6356213092803955, validate: 2.8066654205322266\n",
      "Epoch 1/10. Iteration 45100/47167 Losses: train: 2.8446154594421387, validate: 2.8047642707824707\n",
      "Epoch 1/10. Iteration 45200/47167 Losses: train: 2.754683256149292, validate: 2.813613176345825\n",
      "Epoch 1/10. Iteration 45300/47167 Losses: train: 2.6572265625, validate: 2.80633544921875\n",
      "Epoch 1/10. Iteration 45400/47167 Losses: train: 2.5424911975860596, validate: 2.803169012069702\n",
      "Epoch 1/10. Iteration 45500/47167 Losses: train: 2.542766809463501, validate: 2.802232265472412\n",
      "Epoch 1/10. Iteration 45600/47167 Losses: train: 2.701228380203247, validate: 2.804140329360962\n",
      "Epoch 1/10. Iteration 45700/47167 Losses: train: 2.506945848464966, validate: 2.8071951866149902\n",
      "Epoch 1/10. Iteration 45800/47167 Losses: train: 2.740872859954834, validate: 2.8041908740997314\n",
      "Epoch 1/10. Iteration 45900/47167 Losses: train: 2.7671101093292236, validate: 2.806546688079834\n",
      "Epoch 1/10. Iteration 46000/47167 Losses: train: 2.59440279006958, validate: 2.810404062271118\n",
      "Epoch 1/10. Iteration 46100/47167 Losses: train: 2.809316396713257, validate: 2.8038883209228516\n",
      "Epoch 1/10. Iteration 46200/47167 Losses: train: 2.7265267372131348, validate: 2.80706787109375\n",
      "Epoch 1/10. Iteration 46300/47167 Losses: train: 2.6910414695739746, validate: 2.7998886108398438\n",
      "Epoch 1/10. Iteration 46400/47167 Losses: train: 2.672715425491333, validate: 2.80300235748291\n",
      "Epoch 1/10. Iteration 46500/47167 Losses: train: 2.75022554397583, validate: 2.7980291843414307\n",
      "Epoch 1/10. Iteration 46600/47167 Losses: train: 2.6269333362579346, validate: 2.806705951690674\n",
      "Epoch 1/10. Iteration 46700/47167 Losses: train: 2.9090354442596436, validate: 2.8061113357543945\n",
      "Epoch 1/10. Iteration 46800/47167 Losses: train: 2.6032190322875977, validate: 2.803760528564453\n",
      "Epoch 1/10. Iteration 46900/47167 Losses: train: 2.5573055744171143, validate: 2.8138904571533203\n",
      "Epoch 1/10. Iteration 47000/47167 Losses: train: 2.7517879009246826, validate: 2.80267596244812\n",
      "Epoch 1/10. Iteration 47100/47167 Losses: train: 2.742255687713623, validate: 2.7972397804260254\n",
      "Epoch 2/10. Iteration 100/47167 Losses: train: 2.556323766708374, validate: 2.802677631378174\n",
      "Epoch 2/10. Iteration 200/47167 Losses: train: 2.7501964569091797, validate: 2.8028130531311035\n",
      "Epoch 2/10. Iteration 300/47167 Losses: train: 2.6379833221435547, validate: 2.7941927909851074\n",
      "Epoch 2/10. Iteration 400/47167 Losses: train: 2.6497435569763184, validate: 2.807551622390747\n",
      "Epoch 2/10. Iteration 500/47167 Losses: train: 2.664591073989868, validate: 2.799384593963623\n",
      "Epoch 2/10. Iteration 600/47167 Losses: train: 2.5458996295928955, validate: 2.7956855297088623\n",
      "Epoch 2/10. Iteration 700/47167 Losses: train: 2.553022623062134, validate: 2.795318841934204\n",
      "Epoch 2/10. Iteration 800/47167 Losses: train: 2.6803836822509766, validate: 2.8014190196990967\n",
      "Epoch 2/10. Iteration 900/47167 Losses: train: 2.653003215789795, validate: 2.797808885574341\n",
      "Epoch 2/10. Iteration 1000/47167 Losses: train: 2.783900022506714, validate: 2.8023645877838135\n",
      "Epoch 2/10. Iteration 1100/47167 Losses: train: 2.4440972805023193, validate: 2.8015496730804443\n",
      "Epoch 2/10. Iteration 1200/47167 Losses: train: 2.7235188484191895, validate: 2.792015790939331\n",
      "Epoch 2/10. Iteration 1300/47167 Losses: train: 2.619868040084839, validate: 2.795546770095825\n",
      "Epoch 2/10. Iteration 1400/47167 Losses: train: 2.6989641189575195, validate: 2.7915658950805664\n",
      "Epoch 2/10. Iteration 1500/47167 Losses: train: 2.9864394664764404, validate: 2.788869857788086\n",
      "Epoch 2/10. Iteration 1600/47167 Losses: train: 2.413703680038452, validate: 2.7952780723571777\n",
      "Epoch 2/10. Iteration 1700/47167 Losses: train: 2.638381004333496, validate: 2.791891574859619\n",
      "Epoch 2/10. Iteration 1800/47167 Losses: train: 2.400355577468872, validate: 2.7958984375\n",
      "Epoch 2/10. Iteration 1900/47167 Losses: train: 2.6535873413085938, validate: 2.7888925075531006\n",
      "Epoch 2/10. Iteration 2000/47167 Losses: train: 2.4867968559265137, validate: 2.7950339317321777\n",
      "Epoch 2/10. Iteration 2100/47167 Losses: train: 2.532008647918701, validate: 2.7907698154449463\n",
      "Epoch 2/10. Iteration 2200/47167 Losses: train: 2.670243978500366, validate: 2.8000736236572266\n",
      "Epoch 2/10. Iteration 2300/47167 Losses: train: 2.7590396404266357, validate: 2.789630651473999\n",
      "Epoch 2/10. Iteration 2400/47167 Losses: train: 2.561408281326294, validate: 2.799424409866333\n",
      "Epoch 2/10. Iteration 2500/47167 Losses: train: 2.589834451675415, validate: 2.8012759685516357\n",
      "Epoch 2/10. Iteration 2600/47167 Losses: train: 2.7673864364624023, validate: 2.8052661418914795\n",
      "Epoch 2/10. Iteration 2700/47167 Losses: train: 2.5782313346862793, validate: 2.7988028526306152\n",
      "Epoch 2/10. Iteration 2800/47167 Losses: train: 2.5706405639648438, validate: 2.78956937789917\n",
      "Epoch 2/10. Iteration 2900/47167 Losses: train: 2.6081812381744385, validate: 2.7917816638946533\n",
      "Epoch 2/10. Iteration 3000/47167 Losses: train: 2.4478955268859863, validate: 2.7900636196136475\n",
      "Epoch 2/10. Iteration 3100/47167 Losses: train: 2.7039079666137695, validate: 2.7870357036590576\n",
      "Epoch 2/10. Iteration 3200/47167 Losses: train: 2.7987749576568604, validate: 2.7950751781463623\n",
      "Epoch 2/10. Iteration 3300/47167 Losses: train: 2.84629487991333, validate: 2.7950658798217773\n",
      "Epoch 2/10. Iteration 3400/47167 Losses: train: 2.8480589389801025, validate: 2.7875239849090576\n",
      "Epoch 2/10. Iteration 3500/47167 Losses: train: 2.852095603942871, validate: 2.7894108295440674\n",
      "Epoch 2/10. Iteration 3600/47167 Losses: train: 2.649127960205078, validate: 2.7921741008758545\n",
      "Epoch 2/10. Iteration 3700/47167 Losses: train: 2.7217938899993896, validate: 2.7909107208251953\n",
      "Epoch 2/10. Iteration 3800/47167 Losses: train: 2.5612471103668213, validate: 2.7874255180358887\n",
      "Epoch 2/10. Iteration 3900/47167 Losses: train: 2.5763800144195557, validate: 2.789259672164917\n",
      "Epoch 2/10. Iteration 4000/47167 Losses: train: 2.5267746448516846, validate: 2.787339687347412\n",
      "Epoch 2/10. Iteration 4100/47167 Losses: train: 2.781248092651367, validate: 2.786482334136963\n",
      "Epoch 2/10. Iteration 4200/47167 Losses: train: 2.7203586101531982, validate: 2.780471086502075\n",
      "Epoch 2/10. Iteration 4300/47167 Losses: train: 2.436218738555908, validate: 2.7943296432495117\n",
      "Epoch 2/10. Iteration 4400/47167 Losses: train: 2.6247031688690186, validate: 2.7851614952087402\n",
      "Epoch 2/10. Iteration 4500/47167 Losses: train: 2.6256580352783203, validate: 2.795020580291748\n",
      "Epoch 2/10. Iteration 4600/47167 Losses: train: 2.5544817447662354, validate: 2.7883787155151367\n",
      "Epoch 2/10. Iteration 4700/47167 Losses: train: 2.4797563552856445, validate: 2.783918619155884\n",
      "Epoch 2/10. Iteration 4800/47167 Losses: train: 2.668999671936035, validate: 2.7946221828460693\n",
      "Epoch 2/10. Iteration 4900/47167 Losses: train: 2.6713101863861084, validate: 2.779942274093628\n",
      "Epoch 2/10. Iteration 5000/47167 Losses: train: 2.6277709007263184, validate: 2.7800607681274414\n",
      "Epoch 2/10. Iteration 5100/47167 Losses: train: 2.6407532691955566, validate: 2.7925865650177\n",
      "Epoch 2/10. Iteration 5200/47167 Losses: train: 2.764817237854004, validate: 2.793309211730957\n",
      "Epoch 2/10. Iteration 5300/47167 Losses: train: 2.7186007499694824, validate: 2.784468173980713\n",
      "Epoch 2/10. Iteration 5400/47167 Losses: train: 2.662003517150879, validate: 2.7913942337036133\n",
      "Epoch 2/10. Iteration 5500/47167 Losses: train: 2.546107769012451, validate: 2.781831741333008\n",
      "Epoch 2/10. Iteration 5600/47167 Losses: train: 2.6248247623443604, validate: 2.77614164352417\n",
      "Epoch 2/10. Iteration 5700/47167 Losses: train: 2.67484450340271, validate: 2.7908246517181396\n",
      "Epoch 2/10. Iteration 5800/47167 Losses: train: 2.524883508682251, validate: 2.77856707572937\n",
      "Epoch 2/10. Iteration 5900/47167 Losses: train: 2.617323398590088, validate: 2.7814104557037354\n",
      "Epoch 2/10. Iteration 6000/47167 Losses: train: 2.4501266479492188, validate: 2.785426139831543\n",
      "Epoch 2/10. Iteration 6100/47167 Losses: train: 2.4619932174682617, validate: 2.784108877182007\n",
      "Epoch 2/10. Iteration 6200/47167 Losses: train: 2.6173853874206543, validate: 2.7814481258392334\n",
      "Epoch 2/10. Iteration 6300/47167 Losses: train: 2.4963254928588867, validate: 2.7825088500976562\n",
      "Epoch 2/10. Iteration 6400/47167 Losses: train: 2.6544625759124756, validate: 2.7832138538360596\n",
      "Epoch 2/10. Iteration 6500/47167 Losses: train: 2.4317026138305664, validate: 2.7777066230773926\n",
      "Epoch 2/10. Iteration 6600/47167 Losses: train: 2.7048373222351074, validate: 2.7818026542663574\n",
      "Epoch 2/10. Iteration 6700/47167 Losses: train: 2.392632007598877, validate: 2.7867181301116943\n",
      "Epoch 2/10. Iteration 6800/47167 Losses: train: 2.575712203979492, validate: 2.786802053451538\n",
      "Epoch 2/10. Iteration 6900/47167 Losses: train: 2.659452199935913, validate: 2.78794527053833\n",
      "Epoch 2/10. Iteration 7000/47167 Losses: train: 2.574939012527466, validate: 2.7858645915985107\n",
      "Epoch 2/10. Iteration 7100/47167 Losses: train: 2.7622852325439453, validate: 2.7780492305755615\n",
      "Epoch 2/10. Iteration 7200/47167 Losses: train: 2.7047290802001953, validate: 2.7777044773101807\n",
      "Epoch 2/10. Iteration 7300/47167 Losses: train: 2.399611473083496, validate: 2.780731678009033\n",
      "Epoch 2/10. Iteration 7400/47167 Losses: train: 2.4374542236328125, validate: 2.7886722087860107\n",
      "Epoch 2/10. Iteration 7500/47167 Losses: train: 2.503533124923706, validate: 2.779601573944092\n",
      "Epoch 2/10. Iteration 7600/47167 Losses: train: 2.5081262588500977, validate: 2.778031826019287\n",
      "Epoch 2/10. Iteration 7700/47167 Losses: train: 2.6104495525360107, validate: 2.77994966506958\n",
      "Epoch 2/10. Iteration 7800/47167 Losses: train: 2.64346981048584, validate: 2.775867223739624\n",
      "Epoch 2/10. Iteration 7900/47167 Losses: train: 2.79469633102417, validate: 2.773012161254883\n",
      "Epoch 2/10. Iteration 8000/47167 Losses: train: 2.7018990516662598, validate: 2.777223587036133\n",
      "Epoch 2/10. Iteration 8100/47167 Losses: train: 2.6043317317962646, validate: 2.7756400108337402\n",
      "Epoch 2/10. Iteration 8200/47167 Losses: train: 2.61527156829834, validate: 2.7794790267944336\n",
      "Epoch 2/10. Iteration 8300/47167 Losses: train: 2.7621302604675293, validate: 2.781766653060913\n",
      "Epoch 2/10. Iteration 8400/47167 Losses: train: 2.7335729598999023, validate: 2.7725348472595215\n",
      "Epoch 2/10. Iteration 8500/47167 Losses: train: 2.7217891216278076, validate: 2.7750790119171143\n",
      "Epoch 2/10. Iteration 8600/47167 Losses: train: 2.5250911712646484, validate: 2.776477098464966\n",
      "Epoch 2/10. Iteration 8700/47167 Losses: train: 2.6321609020233154, validate: 2.7664072513580322\n",
      "Epoch 2/10. Iteration 8800/47167 Losses: train: 2.523221731185913, validate: 2.771684408187866\n",
      "Epoch 2/10. Iteration 8900/47167 Losses: train: 2.6304872035980225, validate: 2.7841577529907227\n",
      "Epoch 2/10. Iteration 9000/47167 Losses: train: 2.622699499130249, validate: 2.775054931640625\n",
      "Epoch 2/10. Iteration 9100/47167 Losses: train: 2.645131826400757, validate: 2.777024269104004\n",
      "Epoch 2/10. Iteration 9200/47167 Losses: train: 2.511638879776001, validate: 2.771921157836914\n",
      "Epoch 2/10. Iteration 9300/47167 Losses: train: 2.6596877574920654, validate: 2.7657015323638916\n",
      "Epoch 2/10. Iteration 9400/47167 Losses: train: 2.654817581176758, validate: 2.770010471343994\n",
      "Epoch 2/10. Iteration 9500/47167 Losses: train: 2.6177468299865723, validate: 2.768259286880493\n",
      "Epoch 2/10. Iteration 9600/47167 Losses: train: 2.7030911445617676, validate: 2.7664337158203125\n",
      "Epoch 2/10. Iteration 9700/47167 Losses: train: 2.721888780593872, validate: 2.768552303314209\n",
      "Epoch 2/10. Iteration 9800/47167 Losses: train: 2.759657144546509, validate: 2.767587661743164\n",
      "Epoch 2/10. Iteration 9900/47167 Losses: train: 2.657428026199341, validate: 2.768187999725342\n",
      "Epoch 2/10. Iteration 10000/47167 Losses: train: 2.7412233352661133, validate: 2.765822410583496\n",
      "Epoch 2/10. Iteration 10100/47167 Losses: train: 2.528468370437622, validate: 2.7698090076446533\n",
      "Epoch 2/10. Iteration 10200/47167 Losses: train: 2.757133960723877, validate: 2.762298583984375\n",
      "Epoch 2/10. Iteration 10300/47167 Losses: train: 2.4939191341400146, validate: 2.7613930702209473\n",
      "Epoch 2/10. Iteration 10400/47167 Losses: train: 2.6028146743774414, validate: 2.767265558242798\n",
      "Epoch 2/10. Iteration 10500/47167 Losses: train: 2.5409624576568604, validate: 2.766477108001709\n",
      "Epoch 2/10. Iteration 10600/47167 Losses: train: 2.5683341026306152, validate: 2.77213978767395\n",
      "Epoch 2/10. Iteration 10700/47167 Losses: train: 2.530886173248291, validate: 2.7587828636169434\n",
      "Epoch 2/10. Iteration 10800/47167 Losses: train: 2.79172682762146, validate: 2.765585422515869\n",
      "Epoch 2/10. Iteration 10900/47167 Losses: train: 2.669346570968628, validate: 2.7712979316711426\n",
      "Epoch 2/10. Iteration 11000/47167 Losses: train: 2.695195198059082, validate: 2.7699966430664062\n",
      "Epoch 2/10. Iteration 11100/47167 Losses: train: 2.595634698867798, validate: 2.771488666534424\n",
      "Epoch 2/10. Iteration 11200/47167 Losses: train: 2.5803983211517334, validate: 2.7633414268493652\n",
      "Epoch 2/10. Iteration 11300/47167 Losses: train: 2.815988540649414, validate: 2.757319211959839\n",
      "Epoch 2/10. Iteration 11400/47167 Losses: train: 2.464433193206787, validate: 2.7624738216400146\n",
      "Epoch 2/10. Iteration 11500/47167 Losses: train: 2.624156951904297, validate: 2.754206418991089\n",
      "Epoch 2/10. Iteration 11600/47167 Losses: train: 2.5180628299713135, validate: 2.7585792541503906\n",
      "Epoch 2/10. Iteration 11700/47167 Losses: train: 2.52875018119812, validate: 2.760572671890259\n",
      "Epoch 2/10. Iteration 11800/47167 Losses: train: 2.71968150138855, validate: 2.76900577545166\n",
      "Epoch 2/10. Iteration 11900/47167 Losses: train: 2.617828845977783, validate: 2.7677700519561768\n",
      "Epoch 2/10. Iteration 12000/47167 Losses: train: 2.5615367889404297, validate: 2.7561511993408203\n",
      "Epoch 2/10. Iteration 12100/47167 Losses: train: 2.5421082973480225, validate: 2.7510712146759033\n",
      "Epoch 2/10. Iteration 12200/47167 Losses: train: 2.8441336154937744, validate: 2.755194902420044\n",
      "Epoch 2/10. Iteration 12300/47167 Losses: train: 2.3888494968414307, validate: 2.755854845046997\n",
      "Epoch 2/10. Iteration 12400/47167 Losses: train: 2.4956064224243164, validate: 2.7581980228424072\n",
      "Epoch 2/10. Iteration 12500/47167 Losses: train: 2.696859836578369, validate: 2.764892578125\n",
      "Epoch 2/10. Iteration 12600/47167 Losses: train: 2.6686129570007324, validate: 2.7592177391052246\n",
      "Epoch 2/10. Iteration 12700/47167 Losses: train: 2.408379316329956, validate: 2.755455732345581\n",
      "Epoch 2/10. Iteration 12800/47167 Losses: train: 2.6781399250030518, validate: 2.7588250637054443\n",
      "Epoch 2/10. Iteration 12900/47167 Losses: train: 2.5415146350860596, validate: 2.7605576515197754\n",
      "Epoch 2/10. Iteration 13000/47167 Losses: train: 2.6564106941223145, validate: 2.758540153503418\n",
      "Epoch 2/10. Iteration 13100/47167 Losses: train: 2.7527804374694824, validate: 2.761246919631958\n",
      "Epoch 2/10. Iteration 13200/47167 Losses: train: 2.515377998352051, validate: 2.7548186779022217\n",
      "Epoch 2/10. Iteration 13300/47167 Losses: train: 2.604809045791626, validate: 2.7653238773345947\n",
      "Epoch 2/10. Iteration 13400/47167 Losses: train: 2.7040131092071533, validate: 2.7574517726898193\n",
      "Epoch 2/10. Iteration 13500/47167 Losses: train: 2.721231460571289, validate: 2.758274793624878\n",
      "Epoch 2/10. Iteration 13600/47167 Losses: train: 2.6201047897338867, validate: 2.7586450576782227\n",
      "Epoch 2/10. Iteration 13700/47167 Losses: train: 2.775221824645996, validate: 2.7541329860687256\n",
      "Epoch 2/10. Iteration 13800/47167 Losses: train: 2.6505324840545654, validate: 2.751925468444824\n",
      "Epoch 2/10. Iteration 13900/47167 Losses: train: 2.5517072677612305, validate: 2.7576279640197754\n",
      "Epoch 2/10. Iteration 14000/47167 Losses: train: 2.568803548812866, validate: 2.7600204944610596\n",
      "Epoch 2/10. Iteration 14100/47167 Losses: train: 2.446321487426758, validate: 2.7523839473724365\n",
      "Epoch 2/10. Iteration 14200/47167 Losses: train: 2.5275778770446777, validate: 2.748236894607544\n",
      "Epoch 2/10. Iteration 14300/47167 Losses: train: 2.446753740310669, validate: 2.7526981830596924\n",
      "Epoch 2/10. Iteration 14400/47167 Losses: train: 2.673428535461426, validate: 2.7487220764160156\n",
      "Epoch 2/10. Iteration 14500/47167 Losses: train: 2.58170485496521, validate: 2.748319149017334\n",
      "Epoch 2/10. Iteration 14600/47167 Losses: train: 2.495528221130371, validate: 2.7429163455963135\n",
      "Epoch 2/10. Iteration 14700/47167 Losses: train: 2.476747512817383, validate: 2.7495126724243164\n",
      "Epoch 2/10. Iteration 14800/47167 Losses: train: 2.645479917526245, validate: 2.7463340759277344\n",
      "Epoch 2/10. Iteration 14900/47167 Losses: train: 2.4484148025512695, validate: 2.7560417652130127\n",
      "Epoch 2/10. Iteration 15000/47167 Losses: train: 2.6546640396118164, validate: 2.7499308586120605\n",
      "Epoch 2/10. Iteration 15100/47167 Losses: train: 2.8581717014312744, validate: 2.7471864223480225\n",
      "Epoch 2/10. Iteration 15200/47167 Losses: train: 2.655789852142334, validate: 2.753436326980591\n",
      "Epoch 2/10. Iteration 15300/47167 Losses: train: 2.7910404205322266, validate: 2.755584478378296\n",
      "Epoch 2/10. Iteration 15400/47167 Losses: train: 2.672708749771118, validate: 2.741464376449585\n",
      "Epoch 2/10. Iteration 15500/47167 Losses: train: 2.34088134765625, validate: 2.7454752922058105\n",
      "Epoch 2/10. Iteration 15600/47167 Losses: train: 2.6984126567840576, validate: 2.752772569656372\n",
      "Epoch 2/10. Iteration 15700/47167 Losses: train: 2.772427558898926, validate: 2.7513716220855713\n",
      "Epoch 2/10. Iteration 15800/47167 Losses: train: 2.687835216522217, validate: 2.7537546157836914\n",
      "Epoch 2/10. Iteration 15900/47167 Losses: train: 2.8078744411468506, validate: 2.741694211959839\n",
      "Epoch 2/10. Iteration 16000/47167 Losses: train: 2.651831865310669, validate: 2.7429752349853516\n",
      "Epoch 2/10. Iteration 16100/47167 Losses: train: 2.53940486907959, validate: 2.7461423873901367\n",
      "Epoch 2/10. Iteration 16200/47167 Losses: train: 2.6578657627105713, validate: 2.7548272609710693\n",
      "Epoch 2/10. Iteration 16300/47167 Losses: train: 2.745069980621338, validate: 2.7426390647888184\n",
      "Epoch 2/10. Iteration 16400/47167 Losses: train: 2.8032147884368896, validate: 2.748368740081787\n",
      "Epoch 2/10. Iteration 16500/47167 Losses: train: 2.6817166805267334, validate: 2.7402429580688477\n",
      "Epoch 2/10. Iteration 16600/47167 Losses: train: 2.565160036087036, validate: 2.748389959335327\n",
      "Epoch 2/10. Iteration 16700/47167 Losses: train: 2.5118815898895264, validate: 2.7452447414398193\n",
      "Epoch 2/10. Iteration 16800/47167 Losses: train: 2.6026155948638916, validate: 2.7399418354034424\n",
      "Epoch 2/10. Iteration 16900/47167 Losses: train: 2.698707342147827, validate: 2.7480196952819824\n",
      "Epoch 2/10. Iteration 17000/47167 Losses: train: 2.5684566497802734, validate: 2.7414636611938477\n",
      "Epoch 2/10. Iteration 17100/47167 Losses: train: 2.670518159866333, validate: 2.7448511123657227\n",
      "Epoch 2/10. Iteration 17200/47167 Losses: train: 2.653838634490967, validate: 2.7402913570404053\n",
      "Epoch 2/10. Iteration 17300/47167 Losses: train: 2.5080084800720215, validate: 2.7319743633270264\n",
      "Epoch 2/10. Iteration 17400/47167 Losses: train: 2.567964553833008, validate: 2.740771770477295\n",
      "Epoch 2/10. Iteration 17500/47167 Losses: train: 2.6427671909332275, validate: 2.748283624649048\n",
      "Epoch 2/10. Iteration 17600/47167 Losses: train: 2.568718671798706, validate: 2.749415159225464\n",
      "Epoch 2/10. Iteration 17700/47167 Losses: train: 2.424021005630493, validate: 2.7452564239501953\n",
      "Epoch 2/10. Iteration 17800/47167 Losses: train: 2.3883094787597656, validate: 2.7454135417938232\n",
      "Epoch 2/10. Iteration 17900/47167 Losses: train: 2.5991177558898926, validate: 2.743846893310547\n",
      "Epoch 2/10. Iteration 18000/47167 Losses: train: 2.479065418243408, validate: 2.745286226272583\n",
      "Epoch 2/10. Iteration 18100/47167 Losses: train: 2.520768642425537, validate: 2.7470316886901855\n",
      "Epoch 2/10. Iteration 18200/47167 Losses: train: 2.7981960773468018, validate: 2.7401816844940186\n",
      "Epoch 2/10. Iteration 18300/47167 Losses: train: 2.6633787155151367, validate: 2.745995044708252\n",
      "Epoch 2/10. Iteration 18400/47167 Losses: train: 2.6391165256500244, validate: 2.7459356784820557\n",
      "Epoch 2/10. Iteration 18500/47167 Losses: train: 2.770622968673706, validate: 2.7445812225341797\n",
      "Epoch 2/10. Iteration 18600/47167 Losses: train: 2.5958192348480225, validate: 2.746093273162842\n",
      "Epoch 2/10. Iteration 18700/47167 Losses: train: 2.631711959838867, validate: 2.735687494277954\n",
      "Epoch 2/10. Iteration 18800/47167 Losses: train: 2.8455545902252197, validate: 2.7275729179382324\n",
      "Epoch 2/10. Iteration 18900/47167 Losses: train: 2.7550034523010254, validate: 2.7302544116973877\n",
      "Epoch 2/10. Iteration 19000/47167 Losses: train: 2.4446372985839844, validate: 2.7320797443389893\n",
      "Epoch 2/10. Iteration 19100/47167 Losses: train: 2.4103035926818848, validate: 2.7312023639678955\n",
      "Epoch 2/10. Iteration 19200/47167 Losses: train: 2.4484546184539795, validate: 2.7382254600524902\n",
      "Epoch 2/10. Iteration 19300/47167 Losses: train: 2.6435248851776123, validate: 2.7394299507141113\n",
      "Epoch 2/10. Iteration 19400/47167 Losses: train: 2.6511852741241455, validate: 2.7451531887054443\n",
      "Epoch 2/10. Iteration 19500/47167 Losses: train: 2.5471580028533936, validate: 2.736985206604004\n",
      "Epoch 2/10. Iteration 19600/47167 Losses: train: 2.482379674911499, validate: 2.7338712215423584\n",
      "Epoch 2/10. Iteration 19700/47167 Losses: train: 2.599431276321411, validate: 2.7313601970672607\n",
      "Epoch 2/10. Iteration 19800/47167 Losses: train: 2.646695137023926, validate: 2.7307956218719482\n",
      "Epoch 2/10. Iteration 19900/47167 Losses: train: 2.5347704887390137, validate: 2.740830421447754\n",
      "Epoch 2/10. Iteration 20000/47167 Losses: train: 2.5219545364379883, validate: 2.736441135406494\n",
      "Epoch 2/10. Iteration 20100/47167 Losses: train: 2.7706258296966553, validate: 2.737520456314087\n",
      "Epoch 2/10. Iteration 20200/47167 Losses: train: 2.449435234069824, validate: 2.738372802734375\n",
      "Epoch 2/10. Iteration 20300/47167 Losses: train: 2.5916802883148193, validate: 2.727010488510132\n",
      "Epoch 2/10. Iteration 20400/47167 Losses: train: 2.632251262664795, validate: 2.731989622116089\n",
      "Epoch 2/10. Iteration 20500/47167 Losses: train: 2.6831469535827637, validate: 2.7350027561187744\n",
      "Epoch 2/10. Iteration 20600/47167 Losses: train: 2.4171853065490723, validate: 2.7460556030273438\n",
      "Epoch 2/10. Iteration 20700/47167 Losses: train: 2.57771372795105, validate: 2.7450599670410156\n",
      "Epoch 2/10. Iteration 20800/47167 Losses: train: 2.5090463161468506, validate: 2.746325969696045\n",
      "Epoch 2/10. Iteration 20900/47167 Losses: train: 2.539210796356201, validate: 2.737957239151001\n",
      "Epoch 2/10. Iteration 21000/47167 Losses: train: 2.6673569679260254, validate: 2.7327818870544434\n",
      "Epoch 2/10. Iteration 21100/47167 Losses: train: 2.4843742847442627, validate: 2.7246153354644775\n",
      "Epoch 2/10. Iteration 21200/47167 Losses: train: 2.5112879276275635, validate: 2.7343246936798096\n",
      "Epoch 2/10. Iteration 21300/47167 Losses: train: 2.6389946937561035, validate: 2.737443685531616\n",
      "Epoch 2/10. Iteration 21400/47167 Losses: train: 2.6268317699432373, validate: 2.7320992946624756\n",
      "Epoch 2/10. Iteration 21500/47167 Losses: train: 2.437335968017578, validate: 2.7269575595855713\n",
      "Epoch 2/10. Iteration 21600/47167 Losses: train: 2.596890687942505, validate: 2.734198808670044\n",
      "Epoch 2/10. Iteration 21700/47167 Losses: train: 2.5278971195220947, validate: 2.729846715927124\n",
      "Epoch 2/10. Iteration 21800/47167 Losses: train: 2.6558799743652344, validate: 2.724318265914917\n",
      "Epoch 2/10. Iteration 21900/47167 Losses: train: 2.759798526763916, validate: 2.735189199447632\n",
      "Epoch 2/10. Iteration 22000/47167 Losses: train: 2.582045555114746, validate: 2.725243330001831\n",
      "Epoch 2/10. Iteration 22100/47167 Losses: train: 2.607743740081787, validate: 2.731794834136963\n",
      "Epoch 2/10. Iteration 22200/47167 Losses: train: 2.4825496673583984, validate: 2.720161199569702\n",
      "Epoch 2/10. Iteration 22300/47167 Losses: train: 2.562026262283325, validate: 2.7345266342163086\n",
      "Epoch 2/10. Iteration 22400/47167 Losses: train: 2.6771435737609863, validate: 2.7290618419647217\n",
      "Epoch 2/10. Iteration 22500/47167 Losses: train: 2.869201898574829, validate: 2.7272393703460693\n",
      "Epoch 2/10. Iteration 22600/47167 Losses: train: 2.5665042400360107, validate: 2.7317285537719727\n",
      "Epoch 2/10. Iteration 22700/47167 Losses: train: 2.5849497318267822, validate: 2.729374408721924\n",
      "Epoch 2/10. Iteration 22800/47167 Losses: train: 2.5016286373138428, validate: 2.7276408672332764\n",
      "Epoch 2/10. Iteration 22900/47167 Losses: train: 2.7523715496063232, validate: 2.7293362617492676\n",
      "Epoch 2/10. Iteration 23000/47167 Losses: train: 2.6608924865722656, validate: 2.7315828800201416\n",
      "Epoch 2/10. Iteration 23100/47167 Losses: train: 2.789649724960327, validate: 2.732715606689453\n",
      "Epoch 2/10. Iteration 23200/47167 Losses: train: 2.4992663860321045, validate: 2.7273964881896973\n",
      "Epoch 2/10. Iteration 23300/47167 Losses: train: 2.578810214996338, validate: 2.7252213954925537\n",
      "Epoch 2/10. Iteration 23400/47167 Losses: train: 2.7533185482025146, validate: 2.7271766662597656\n",
      "Epoch 2/10. Iteration 23500/47167 Losses: train: 2.652775526046753, validate: 2.727489709854126\n",
      "Epoch 2/10. Iteration 23600/47167 Losses: train: 2.553889036178589, validate: 2.7157044410705566\n",
      "Epoch 2/10. Iteration 23700/47167 Losses: train: 2.6919057369232178, validate: 2.7177813053131104\n",
      "Epoch 2/10. Iteration 23800/47167 Losses: train: 2.569847583770752, validate: 2.721695899963379\n",
      "Epoch 2/10. Iteration 23900/47167 Losses: train: 2.5744144916534424, validate: 2.7206077575683594\n",
      "Epoch 2/10. Iteration 24000/47167 Losses: train: 2.4478647708892822, validate: 2.7259202003479004\n",
      "Epoch 2/10. Iteration 24100/47167 Losses: train: 2.6040568351745605, validate: 2.7128047943115234\n",
      "Epoch 2/10. Iteration 24200/47167 Losses: train: 2.5695557594299316, validate: 2.7279205322265625\n",
      "Epoch 2/10. Iteration 24300/47167 Losses: train: 2.4668922424316406, validate: 2.716383218765259\n",
      "Epoch 2/10. Iteration 24400/47167 Losses: train: 2.696465015411377, validate: 2.722583532333374\n",
      "Epoch 2/10. Iteration 24500/47167 Losses: train: 2.76766037940979, validate: 2.7183148860931396\n",
      "Epoch 2/10. Iteration 24600/47167 Losses: train: 2.556337356567383, validate: 2.7254648208618164\n",
      "Epoch 2/10. Iteration 24700/47167 Losses: train: 2.49735426902771, validate: 2.723815679550171\n",
      "Epoch 2/10. Iteration 24800/47167 Losses: train: 2.6653356552124023, validate: 2.7235827445983887\n",
      "Epoch 2/10. Iteration 24900/47167 Losses: train: 2.6831741333007812, validate: 2.722224235534668\n",
      "Epoch 2/10. Iteration 25000/47167 Losses: train: 2.4843883514404297, validate: 2.73002290725708\n",
      "Epoch 2/10. Iteration 25100/47167 Losses: train: 2.6329658031463623, validate: 2.7218332290649414\n",
      "Epoch 2/10. Iteration 25200/47167 Losses: train: 2.5647647380828857, validate: 2.7145814895629883\n",
      "Epoch 2/10. Iteration 25300/47167 Losses: train: 2.587785243988037, validate: 2.7231638431549072\n",
      "Epoch 2/10. Iteration 25400/47167 Losses: train: 2.641706943511963, validate: 2.7195310592651367\n",
      "Epoch 2/10. Iteration 25500/47167 Losses: train: 2.5316500663757324, validate: 2.7275521755218506\n",
      "Epoch 2/10. Iteration 25600/47167 Losses: train: 2.548243761062622, validate: 2.7199103832244873\n",
      "Epoch 2/10. Iteration 25700/47167 Losses: train: 2.536353588104248, validate: 2.7079992294311523\n",
      "Epoch 2/10. Iteration 25800/47167 Losses: train: 2.378399610519409, validate: 2.710153818130493\n",
      "Epoch 2/10. Iteration 25900/47167 Losses: train: 2.4428906440734863, validate: 2.708512544631958\n",
      "Epoch 2/10. Iteration 26000/47167 Losses: train: 2.5365254878997803, validate: 2.7173116207122803\n",
      "Epoch 2/10. Iteration 26100/47167 Losses: train: 2.643385171890259, validate: 2.720432996749878\n",
      "Epoch 2/10. Iteration 26200/47167 Losses: train: 2.5895018577575684, validate: 2.7208847999572754\n",
      "Epoch 2/10. Iteration 26300/47167 Losses: train: 2.9133460521698, validate: 2.7206473350524902\n",
      "Epoch 2/10. Iteration 26400/47167 Losses: train: 2.832821846008301, validate: 2.709064483642578\n",
      "Epoch 2/10. Iteration 26500/47167 Losses: train: 2.473971366882324, validate: 2.716430902481079\n",
      "Epoch 2/10. Iteration 26600/47167 Losses: train: 2.5929486751556396, validate: 2.717982530593872\n",
      "Epoch 2/10. Iteration 26700/47167 Losses: train: 2.7271125316619873, validate: 2.71110463142395\n",
      "Epoch 2/10. Iteration 26800/47167 Losses: train: 2.6989386081695557, validate: 2.718458414077759\n",
      "Epoch 2/10. Iteration 26900/47167 Losses: train: 2.473233222961426, validate: 2.725278854370117\n",
      "Epoch 2/10. Iteration 27000/47167 Losses: train: 2.3654773235321045, validate: 2.709434986114502\n",
      "Epoch 2/10. Iteration 27100/47167 Losses: train: 2.3769822120666504, validate: 2.7167155742645264\n",
      "Epoch 2/10. Iteration 27200/47167 Losses: train: 2.7421679496765137, validate: 2.726494789123535\n",
      "Epoch 2/10. Iteration 27300/47167 Losses: train: 2.618640899658203, validate: 2.709343671798706\n",
      "Epoch 2/10. Iteration 27400/47167 Losses: train: 2.6205079555511475, validate: 2.723562479019165\n",
      "Epoch 2/10. Iteration 27500/47167 Losses: train: 2.8424651622772217, validate: 2.7185890674591064\n",
      "Epoch 2/10. Iteration 27600/47167 Losses: train: 2.595294237136841, validate: 2.7214064598083496\n",
      "Epoch 2/10. Iteration 27700/47167 Losses: train: 2.4217991828918457, validate: 2.7177884578704834\n",
      "Epoch 2/10. Iteration 27800/47167 Losses: train: 2.6690163612365723, validate: 2.711470127105713\n",
      "Epoch 2/10. Iteration 27900/47167 Losses: train: 2.671508312225342, validate: 2.7215542793273926\n",
      "Epoch 2/10. Iteration 28000/47167 Losses: train: 2.584521770477295, validate: 2.7165563106536865\n",
      "Epoch 2/10. Iteration 28100/47167 Losses: train: 2.385199785232544, validate: 2.7157578468322754\n",
      "Epoch 2/10. Iteration 28200/47167 Losses: train: 2.5205078125, validate: 2.7159159183502197\n",
      "Epoch 2/10. Iteration 28300/47167 Losses: train: 2.688166618347168, validate: 2.723806858062744\n",
      "Epoch 2/10. Iteration 28400/47167 Losses: train: 2.5680086612701416, validate: 2.7153494358062744\n",
      "Epoch 2/10. Iteration 28500/47167 Losses: train: 2.6991491317749023, validate: 2.710125207901001\n",
      "Epoch 2/10. Iteration 28600/47167 Losses: train: 2.5258922576904297, validate: 2.7166049480438232\n",
      "Epoch 2/10. Iteration 28700/47167 Losses: train: 2.496217727661133, validate: 2.7022547721862793\n",
      "Epoch 2/10. Iteration 28800/47167 Losses: train: 2.626332998275757, validate: 2.7007827758789062\n",
      "Epoch 2/10. Iteration 28900/47167 Losses: train: 2.6099350452423096, validate: 2.704939126968384\n",
      "Epoch 2/10. Iteration 29000/47167 Losses: train: 2.5172066688537598, validate: 2.7150425910949707\n",
      "Epoch 2/10. Iteration 29100/47167 Losses: train: 2.5974886417388916, validate: 2.7214767932891846\n",
      "Epoch 2/10. Iteration 29200/47167 Losses: train: 2.668583869934082, validate: 2.7177228927612305\n",
      "Epoch 2/10. Iteration 29300/47167 Losses: train: 2.8319199085235596, validate: 2.7124009132385254\n",
      "Epoch 2/10. Iteration 29400/47167 Losses: train: 2.5304465293884277, validate: 2.7081198692321777\n",
      "Epoch 2/10. Iteration 29500/47167 Losses: train: 2.4509551525115967, validate: 2.714463472366333\n",
      "Epoch 2/10. Iteration 29600/47167 Losses: train: 2.5616512298583984, validate: 2.710468053817749\n",
      "Epoch 2/10. Iteration 29700/47167 Losses: train: 2.6042349338531494, validate: 2.70341420173645\n",
      "Epoch 2/10. Iteration 29800/47167 Losses: train: 2.62641978263855, validate: 2.7061216831207275\n",
      "Epoch 2/10. Iteration 29900/47167 Losses: train: 2.6134417057037354, validate: 2.7167932987213135\n",
      "Epoch 2/10. Iteration 30000/47167 Losses: train: 2.4964780807495117, validate: 2.70906925201416\n",
      "Epoch 2/10. Iteration 30100/47167 Losses: train: 2.5394392013549805, validate: 2.7192273139953613\n",
      "Epoch 2/10. Iteration 30200/47167 Losses: train: 2.4586539268493652, validate: 2.7154102325439453\n",
      "Epoch 2/10. Iteration 30300/47167 Losses: train: 2.5555131435394287, validate: 2.7077012062072754\n",
      "Epoch 2/10. Iteration 30400/47167 Losses: train: 2.6481387615203857, validate: 2.7080841064453125\n",
      "Epoch 2/10. Iteration 30500/47167 Losses: train: 2.734610080718994, validate: 2.7037110328674316\n",
      "Epoch 2/10. Iteration 30600/47167 Losses: train: 2.556048631668091, validate: 2.7033190727233887\n",
      "Epoch 2/10. Iteration 30700/47167 Losses: train: 2.696345567703247, validate: 2.715601921081543\n",
      "Epoch 2/10. Iteration 30800/47167 Losses: train: 2.6392130851745605, validate: 2.7037644386291504\n",
      "Epoch 2/10. Iteration 30900/47167 Losses: train: 2.6090221405029297, validate: 2.712799072265625\n",
      "Epoch 2/10. Iteration 31000/47167 Losses: train: 2.761674642562866, validate: 2.717485189437866\n",
      "Epoch 2/10. Iteration 31100/47167 Losses: train: 2.4791016578674316, validate: 2.711296558380127\n",
      "Epoch 2/10. Iteration 31200/47167 Losses: train: 2.5703301429748535, validate: 2.7108805179595947\n",
      "Epoch 2/10. Iteration 31300/47167 Losses: train: 2.6742775440216064, validate: 2.7130038738250732\n",
      "Epoch 2/10. Iteration 31400/47167 Losses: train: 2.65742564201355, validate: 2.717306137084961\n",
      "Epoch 2/10. Iteration 31500/47167 Losses: train: 2.5865354537963867, validate: 2.71260666847229\n",
      "Epoch 2/10. Iteration 31600/47167 Losses: train: 2.430934190750122, validate: 2.7120003700256348\n",
      "Epoch 2/10. Iteration 31700/47167 Losses: train: 2.4335227012634277, validate: 2.7062549591064453\n",
      "Epoch 2/10. Iteration 31800/47167 Losses: train: 2.3662869930267334, validate: 2.718905448913574\n",
      "Epoch 2/10. Iteration 31900/47167 Losses: train: 2.6348154544830322, validate: 2.707287073135376\n",
      "Epoch 2/10. Iteration 32000/47167 Losses: train: 2.8279166221618652, validate: 2.7165138721466064\n",
      "Epoch 2/10. Iteration 32100/47167 Losses: train: 2.6454989910125732, validate: 2.7130823135375977\n",
      "Epoch 2/10. Iteration 32200/47167 Losses: train: 2.402433156967163, validate: 2.707474708557129\n",
      "Epoch 2/10. Iteration 32300/47167 Losses: train: 2.4446218013763428, validate: 2.7078194618225098\n",
      "Epoch 2/10. Iteration 32400/47167 Losses: train: 2.6418004035949707, validate: 2.705394983291626\n",
      "Epoch 2/10. Iteration 32500/47167 Losses: train: 2.55710506439209, validate: 2.7052340507507324\n",
      "Epoch 2/10. Iteration 32600/47167 Losses: train: 2.564525842666626, validate: 2.710399866104126\n",
      "Epoch 2/10. Iteration 32700/47167 Losses: train: 2.5672478675842285, validate: 2.7049102783203125\n",
      "Epoch 2/10. Iteration 32800/47167 Losses: train: 2.4674112796783447, validate: 2.7073841094970703\n",
      "Epoch 2/10. Iteration 32900/47167 Losses: train: 2.467435359954834, validate: 2.7030844688415527\n",
      "Epoch 2/10. Iteration 33000/47167 Losses: train: 2.548067808151245, validate: 2.698141574859619\n",
      "Epoch 2/10. Iteration 33100/47167 Losses: train: 2.5679426193237305, validate: 2.700432777404785\n",
      "Epoch 2/10. Iteration 33200/47167 Losses: train: 2.4971139430999756, validate: 2.6953415870666504\n",
      "Epoch 2/10. Iteration 33300/47167 Losses: train: 2.5997025966644287, validate: 2.7162933349609375\n",
      "Epoch 2/10. Iteration 33400/47167 Losses: train: 2.5113847255706787, validate: 2.698930263519287\n",
      "Epoch 2/10. Iteration 33500/47167 Losses: train: 2.4442129135131836, validate: 2.692551851272583\n",
      "Epoch 2/10. Iteration 33600/47167 Losses: train: 2.5848500728607178, validate: 2.6998844146728516\n",
      "Epoch 2/10. Iteration 33700/47167 Losses: train: 2.7660109996795654, validate: 2.6996564865112305\n",
      "Epoch 2/10. Iteration 33800/47167 Losses: train: 2.7510201930999756, validate: 2.6958673000335693\n",
      "Epoch 2/10. Iteration 33900/47167 Losses: train: 2.67811918258667, validate: 2.6996963024139404\n",
      "Epoch 2/10. Iteration 34000/47167 Losses: train: 2.723174810409546, validate: 2.697233200073242\n",
      "Epoch 2/10. Iteration 34100/47167 Losses: train: 2.74088454246521, validate: 2.6938364505767822\n",
      "Epoch 2/10. Iteration 34200/47167 Losses: train: 2.534482479095459, validate: 2.7007596492767334\n",
      "Epoch 2/10. Iteration 34300/47167 Losses: train: 2.5612664222717285, validate: 2.7032201290130615\n",
      "Epoch 2/10. Iteration 34400/47167 Losses: train: 2.5252737998962402, validate: 2.6947715282440186\n",
      "Epoch 2/10. Iteration 34500/47167 Losses: train: 2.463231086730957, validate: 2.7063791751861572\n",
      "Epoch 2/10. Iteration 34600/47167 Losses: train: 2.7157270908355713, validate: 2.699849843978882\n",
      "Epoch 2/10. Iteration 34700/47167 Losses: train: 2.5519793033599854, validate: 2.7049975395202637\n",
      "Epoch 2/10. Iteration 34800/47167 Losses: train: 2.4828290939331055, validate: 2.704691171646118\n",
      "Epoch 2/10. Iteration 34900/47167 Losses: train: 2.6575474739074707, validate: 2.704855442047119\n",
      "Epoch 2/10. Iteration 35000/47167 Losses: train: 2.3858187198638916, validate: 2.6950080394744873\n",
      "Epoch 2/10. Iteration 35100/47167 Losses: train: 2.6090967655181885, validate: 2.6997568607330322\n",
      "Epoch 2/10. Iteration 35200/47167 Losses: train: 2.5593369007110596, validate: 2.705413579940796\n",
      "Epoch 2/10. Iteration 35300/47167 Losses: train: 2.478254795074463, validate: 2.695819616317749\n",
      "Epoch 2/10. Iteration 35400/47167 Losses: train: 2.5282039642333984, validate: 2.7003514766693115\n",
      "Epoch 2/10. Iteration 35500/47167 Losses: train: 2.4601166248321533, validate: 2.6936228275299072\n",
      "Epoch 2/10. Iteration 35600/47167 Losses: train: 2.3906021118164062, validate: 2.6965701580047607\n",
      "Epoch 2/10. Iteration 35700/47167 Losses: train: 2.491006374359131, validate: 2.7060086727142334\n",
      "Epoch 2/10. Iteration 35800/47167 Losses: train: 2.668959617614746, validate: 2.7094838619232178\n",
      "Epoch 2/10. Iteration 35900/47167 Losses: train: 2.723371744155884, validate: 2.7040138244628906\n",
      "Epoch 2/10. Iteration 36000/47167 Losses: train: 2.7234644889831543, validate: 2.6951568126678467\n",
      "Epoch 2/10. Iteration 36100/47167 Losses: train: 2.6105782985687256, validate: 2.6941354274749756\n",
      "Epoch 2/10. Iteration 36200/47167 Losses: train: 2.7086784839630127, validate: 2.6910905838012695\n",
      "Epoch 2/10. Iteration 36300/47167 Losses: train: 2.586038112640381, validate: 2.6925902366638184\n",
      "Epoch 2/10. Iteration 36400/47167 Losses: train: 2.468950033187866, validate: 2.690074920654297\n",
      "Epoch 2/10. Iteration 36500/47167 Losses: train: 2.7124485969543457, validate: 2.6963350772857666\n",
      "Epoch 2/10. Iteration 36600/47167 Losses: train: 2.829627275466919, validate: 2.696441173553467\n",
      "Epoch 2/10. Iteration 36700/47167 Losses: train: 2.5369842052459717, validate: 2.6989567279815674\n",
      "Epoch 2/10. Iteration 36800/47167 Losses: train: 2.626223087310791, validate: 2.699995517730713\n",
      "Epoch 2/10. Iteration 36900/47167 Losses: train: 2.542736291885376, validate: 2.699890375137329\n",
      "Epoch 2/10. Iteration 37000/47167 Losses: train: 2.7729392051696777, validate: 2.6992478370666504\n",
      "Epoch 2/10. Iteration 37100/47167 Losses: train: 2.5597739219665527, validate: 2.7004401683807373\n",
      "Epoch 2/10. Iteration 37200/47167 Losses: train: 2.5113625526428223, validate: 2.6988446712493896\n",
      "Epoch 2/10. Iteration 37300/47167 Losses: train: 2.4945671558380127, validate: 2.6912739276885986\n",
      "Epoch 2/10. Iteration 37400/47167 Losses: train: 2.579885244369507, validate: 2.701117753982544\n",
      "Epoch 2/10. Iteration 37500/47167 Losses: train: 2.35357928276062, validate: 2.699504852294922\n",
      "Epoch 2/10. Iteration 37600/47167 Losses: train: 2.5114755630493164, validate: 2.6966676712036133\n",
      "Epoch 2/10. Iteration 37700/47167 Losses: train: 2.6267929077148438, validate: 2.699383020401001\n",
      "Epoch 2/10. Iteration 37800/47167 Losses: train: 2.6847825050354004, validate: 2.696396827697754\n",
      "Epoch 2/10. Iteration 37900/47167 Losses: train: 2.443082571029663, validate: 2.685702085494995\n",
      "Epoch 2/10. Iteration 38000/47167 Losses: train: 2.662505626678467, validate: 2.6936628818511963\n",
      "Epoch 2/10. Iteration 38100/47167 Losses: train: 2.8337759971618652, validate: 2.685476779937744\n",
      "Epoch 2/10. Iteration 38200/47167 Losses: train: 2.5654919147491455, validate: 2.697301149368286\n",
      "Epoch 2/10. Iteration 38300/47167 Losses: train: 2.4830148220062256, validate: 2.686042308807373\n",
      "Epoch 2/10. Iteration 38400/47167 Losses: train: 2.4595694541931152, validate: 2.7007434368133545\n",
      "Epoch 2/10. Iteration 38500/47167 Losses: train: 2.6980597972869873, validate: 2.697887659072876\n",
      "Epoch 2/10. Iteration 38600/47167 Losses: train: 2.429137706756592, validate: 2.6896262168884277\n",
      "Epoch 2/10. Iteration 38700/47167 Losses: train: 2.658885955810547, validate: 2.684112310409546\n",
      "Epoch 2/10. Iteration 38800/47167 Losses: train: 2.5629754066467285, validate: 2.6871237754821777\n",
      "Epoch 2/10. Iteration 38900/47167 Losses: train: 2.4571235179901123, validate: 2.6952998638153076\n",
      "Epoch 2/10. Iteration 39000/47167 Losses: train: 2.5640995502471924, validate: 2.6870105266571045\n",
      "Epoch 2/10. Iteration 39100/47167 Losses: train: 2.766505479812622, validate: 2.6872475147247314\n",
      "Epoch 2/10. Iteration 39200/47167 Losses: train: 2.5298187732696533, validate: 2.684087038040161\n",
      "Epoch 2/10. Iteration 39300/47167 Losses: train: 2.528505802154541, validate: 2.692451238632202\n",
      "Epoch 2/10. Iteration 39400/47167 Losses: train: 2.7243783473968506, validate: 2.688511610031128\n",
      "Epoch 2/10. Iteration 39500/47167 Losses: train: 2.6809310913085938, validate: 2.6913442611694336\n",
      "Epoch 2/10. Iteration 39600/47167 Losses: train: 2.6131958961486816, validate: 2.6871302127838135\n",
      "Epoch 2/10. Iteration 39700/47167 Losses: train: 2.533433437347412, validate: 2.690051317214966\n",
      "Epoch 2/10. Iteration 39800/47167 Losses: train: 2.4907565116882324, validate: 2.694462776184082\n",
      "Epoch 2/10. Iteration 39900/47167 Losses: train: 2.331145763397217, validate: 2.68593692779541\n",
      "Epoch 2/10. Iteration 40000/47167 Losses: train: 2.5893070697784424, validate: 2.6819355487823486\n",
      "Epoch 2/10. Iteration 40100/47167 Losses: train: 2.5780701637268066, validate: 2.6891610622406006\n",
      "Epoch 2/10. Iteration 40200/47167 Losses: train: 2.4356698989868164, validate: 2.6870758533477783\n",
      "Epoch 2/10. Iteration 40300/47167 Losses: train: 2.6609835624694824, validate: 2.6881067752838135\n",
      "Epoch 2/10. Iteration 40400/47167 Losses: train: 2.5371105670928955, validate: 2.690197229385376\n",
      "Epoch 2/10. Iteration 40500/47167 Losses: train: 2.6669580936431885, validate: 2.6837527751922607\n",
      "Epoch 2/10. Iteration 40600/47167 Losses: train: 2.5254054069519043, validate: 2.693084716796875\n",
      "Epoch 2/10. Iteration 40700/47167 Losses: train: 2.415469169616699, validate: 2.6833176612854004\n",
      "Epoch 2/10. Iteration 40800/47167 Losses: train: 2.383512496948242, validate: 2.6853113174438477\n",
      "Epoch 2/10. Iteration 40900/47167 Losses: train: 2.408452272415161, validate: 2.68817138671875\n",
      "Epoch 2/10. Iteration 41000/47167 Losses: train: 2.5297162532806396, validate: 2.6849794387817383\n",
      "Epoch 2/10. Iteration 41100/47167 Losses: train: 2.679597854614258, validate: 2.6786632537841797\n",
      "Epoch 2/10. Iteration 41200/47167 Losses: train: 2.5790488719940186, validate: 2.675809383392334\n",
      "Epoch 2/10. Iteration 41300/47167 Losses: train: 2.425503969192505, validate: 2.6819918155670166\n",
      "Epoch 2/10. Iteration 41400/47167 Losses: train: 2.6485769748687744, validate: 2.6810739040374756\n",
      "Epoch 2/10. Iteration 41500/47167 Losses: train: 2.5064949989318848, validate: 2.6772842407226562\n",
      "Epoch 2/10. Iteration 41600/47167 Losses: train: 2.4047014713287354, validate: 2.684396505355835\n",
      "Epoch 2/10. Iteration 41700/47167 Losses: train: 2.401165246963501, validate: 2.6779837608337402\n",
      "Epoch 2/10. Iteration 41800/47167 Losses: train: 2.630666494369507, validate: 2.687471866607666\n",
      "Epoch 2/10. Iteration 41900/47167 Losses: train: 2.53067684173584, validate: 2.6757075786590576\n",
      "Epoch 2/10. Iteration 42000/47167 Losses: train: 2.474541425704956, validate: 2.6791322231292725\n",
      "Epoch 2/10. Iteration 42100/47167 Losses: train: 2.5101213455200195, validate: 2.6843783855438232\n",
      "Epoch 2/10. Iteration 42200/47167 Losses: train: 2.4961483478546143, validate: 2.680680990219116\n",
      "Epoch 2/10. Iteration 42300/47167 Losses: train: 2.3408043384552, validate: 2.6841042041778564\n",
      "Epoch 2/10. Iteration 42400/47167 Losses: train: 2.5134224891662598, validate: 2.674776315689087\n",
      "Epoch 2/10. Iteration 42500/47167 Losses: train: 2.525646924972534, validate: 2.6861374378204346\n",
      "Epoch 2/10. Iteration 42600/47167 Losses: train: 2.5318827629089355, validate: 2.683865547180176\n",
      "Epoch 2/10. Iteration 42700/47167 Losses: train: 2.6660001277923584, validate: 2.68631911277771\n",
      "Epoch 2/10. Iteration 42800/47167 Losses: train: 2.6688296794891357, validate: 2.6751387119293213\n",
      "Epoch 2/10. Iteration 42900/47167 Losses: train: 2.468564510345459, validate: 2.681151866912842\n",
      "Epoch 2/10. Iteration 43000/47167 Losses: train: 2.4112279415130615, validate: 2.689938545227051\n",
      "Epoch 2/10. Iteration 43100/47167 Losses: train: 2.553194046020508, validate: 2.691967010498047\n",
      "Epoch 2/10. Iteration 43200/47167 Losses: train: 2.6610662937164307, validate: 2.686924457550049\n",
      "Epoch 2/10. Iteration 43300/47167 Losses: train: 2.5108187198638916, validate: 2.6878674030303955\n",
      "Epoch 2/10. Iteration 43400/47167 Losses: train: 2.4335432052612305, validate: 2.6895551681518555\n",
      "Epoch 2/10. Iteration 43500/47167 Losses: train: 2.5962069034576416, validate: 2.681575298309326\n",
      "Epoch 2/10. Iteration 43600/47167 Losses: train: 2.5251474380493164, validate: 2.683770179748535\n",
      "Epoch 2/10. Iteration 43700/47167 Losses: train: 2.900434732437134, validate: 2.6829276084899902\n",
      "Epoch 2/10. Iteration 43800/47167 Losses: train: 2.4725289344787598, validate: 2.6767313480377197\n",
      "Epoch 2/10. Iteration 43900/47167 Losses: train: 2.599965810775757, validate: 2.680978298187256\n",
      "Epoch 2/10. Iteration 44000/47167 Losses: train: 2.5512924194335938, validate: 2.6878674030303955\n",
      "Epoch 2/10. Iteration 44100/47167 Losses: train: 2.554239511489868, validate: 2.687807321548462\n",
      "Epoch 2/10. Iteration 44200/47167 Losses: train: 2.58284068107605, validate: 2.677903890609741\n",
      "Epoch 2/10. Iteration 44300/47167 Losses: train: 2.533212423324585, validate: 2.6870641708374023\n",
      "Epoch 2/10. Iteration 44400/47167 Losses: train: 2.494823455810547, validate: 2.6744062900543213\n",
      "Epoch 2/10. Iteration 44500/47167 Losses: train: 2.509251832962036, validate: 2.682528018951416\n",
      "Epoch 2/10. Iteration 44600/47167 Losses: train: 2.6623435020446777, validate: 2.6748108863830566\n",
      "Epoch 2/10. Iteration 44700/47167 Losses: train: 2.5312137603759766, validate: 2.674724578857422\n",
      "Epoch 2/10. Iteration 44800/47167 Losses: train: 2.4827780723571777, validate: 2.677689552307129\n",
      "Epoch 2/10. Iteration 44900/47167 Losses: train: 2.516805410385132, validate: 2.6772639751434326\n",
      "Epoch 2/10. Iteration 45000/47167 Losses: train: 2.616856098175049, validate: 2.681865930557251\n",
      "Epoch 2/10. Iteration 45100/47167 Losses: train: 2.5737972259521484, validate: 2.6811249256134033\n",
      "Epoch 2/10. Iteration 45200/47167 Losses: train: 2.648564338684082, validate: 2.675623655319214\n",
      "Epoch 2/10. Iteration 45300/47167 Losses: train: 2.4480981826782227, validate: 2.6781911849975586\n",
      "Epoch 2/10. Iteration 45400/47167 Losses: train: 2.6793792247772217, validate: 2.678535223007202\n",
      "Epoch 2/10. Iteration 45500/47167 Losses: train: 2.6847915649414062, validate: 2.671032190322876\n",
      "Epoch 2/10. Iteration 45600/47167 Losses: train: 2.683748245239258, validate: 2.671203374862671\n",
      "Epoch 2/10. Iteration 45700/47167 Losses: train: 2.6702473163604736, validate: 2.6779870986938477\n",
      "Epoch 2/10. Iteration 45800/47167 Losses: train: 2.4679007530212402, validate: 2.6807174682617188\n",
      "Epoch 2/10. Iteration 45900/47167 Losses: train: 2.5681636333465576, validate: 2.6790926456451416\n",
      "Epoch 2/10. Iteration 46000/47167 Losses: train: 2.744375705718994, validate: 2.6719956398010254\n",
      "Epoch 2/10. Iteration 46100/47167 Losses: train: 2.4411261081695557, validate: 2.6752758026123047\n",
      "Epoch 2/10. Iteration 46200/47167 Losses: train: 2.2361505031585693, validate: 2.6766324043273926\n",
      "Epoch 2/10. Iteration 46300/47167 Losses: train: 2.5178375244140625, validate: 2.67311429977417\n",
      "Epoch 2/10. Iteration 46400/47167 Losses: train: 2.6094472408294678, validate: 2.6754555702209473\n",
      "Epoch 2/10. Iteration 46500/47167 Losses: train: 2.435723304748535, validate: 2.6822240352630615\n",
      "Epoch 2/10. Iteration 46600/47167 Losses: train: 2.4590842723846436, validate: 2.6793954372406006\n",
      "Epoch 2/10. Iteration 46700/47167 Losses: train: 2.7052276134490967, validate: 2.669631004333496\n",
      "Epoch 2/10. Iteration 46800/47167 Losses: train: 2.556954860687256, validate: 2.6739304065704346\n",
      "Epoch 2/10. Iteration 46900/47167 Losses: train: 2.3863937854766846, validate: 2.6685686111450195\n",
      "Epoch 2/10. Iteration 47000/47167 Losses: train: 2.5114526748657227, validate: 2.678536891937256\n",
      "Epoch 2/10. Iteration 47100/47167 Losses: train: 2.644808292388916, validate: 2.675117015838623\n",
      "Epoch 3/10. Iteration 100/47167 Losses: train: 2.3756041526794434, validate: 2.6764776706695557\n",
      "Epoch 3/10. Iteration 200/47167 Losses: train: 2.4777088165283203, validate: 2.6688404083251953\n",
      "Epoch 3/10. Iteration 300/47167 Losses: train: 2.4076929092407227, validate: 2.684927463531494\n",
      "Epoch 3/10. Iteration 400/47167 Losses: train: 2.448869228363037, validate: 2.6798019409179688\n",
      "Epoch 3/10. Iteration 500/47167 Losses: train: 2.4941961765289307, validate: 2.6706180572509766\n",
      "Epoch 3/10. Iteration 600/47167 Losses: train: 2.4816808700561523, validate: 2.6695566177368164\n",
      "Epoch 3/10. Iteration 700/47167 Losses: train: 2.5778098106384277, validate: 2.6734023094177246\n",
      "Epoch 3/10. Iteration 800/47167 Losses: train: 2.3378846645355225, validate: 2.6684601306915283\n",
      "Epoch 3/10. Iteration 900/47167 Losses: train: 2.387722969055176, validate: 2.665783643722534\n",
      "Epoch 3/10. Iteration 1000/47167 Losses: train: 2.4339523315429688, validate: 2.6706650257110596\n",
      "Epoch 3/10. Iteration 1100/47167 Losses: train: 2.5125601291656494, validate: 2.675006866455078\n",
      "Epoch 3/10. Iteration 1200/47167 Losses: train: 2.4473869800567627, validate: 2.6617753505706787\n",
      "Epoch 3/10. Iteration 1300/47167 Losses: train: 2.5030581951141357, validate: 2.6667490005493164\n",
      "Epoch 3/10. Iteration 1400/47167 Losses: train: 2.5133771896362305, validate: 2.6640443801879883\n",
      "Epoch 3/10. Iteration 1500/47167 Losses: train: 2.5076167583465576, validate: 2.6655466556549072\n",
      "Epoch 3/10. Iteration 1600/47167 Losses: train: 2.4916090965270996, validate: 2.662384271621704\n",
      "Epoch 3/10. Iteration 1700/47167 Losses: train: 2.523953676223755, validate: 2.6623082160949707\n",
      "Epoch 3/10. Iteration 1800/47167 Losses: train: 2.511272668838501, validate: 2.6639862060546875\n",
      "Epoch 3/10. Iteration 1900/47167 Losses: train: 2.5027711391448975, validate: 2.660940170288086\n",
      "Epoch 3/10. Iteration 2000/47167 Losses: train: 2.232456922531128, validate: 2.6649391651153564\n",
      "Epoch 3/10. Iteration 2100/47167 Losses: train: 2.4655675888061523, validate: 2.6700072288513184\n",
      "Epoch 3/10. Iteration 2200/47167 Losses: train: 2.4685730934143066, validate: 2.658400535583496\n",
      "Epoch 3/10. Iteration 2300/47167 Losses: train: 2.422969341278076, validate: 2.658491611480713\n",
      "Epoch 3/10. Iteration 2400/47167 Losses: train: 2.6213133335113525, validate: 2.664703607559204\n",
      "Epoch 3/10. Iteration 2500/47167 Losses: train: 2.616689682006836, validate: 2.6502060890197754\n",
      "Epoch 3/10. Iteration 2600/47167 Losses: train: 2.5013442039489746, validate: 2.65431547164917\n",
      "Epoch 3/10. Iteration 2700/47167 Losses: train: 2.6491446495056152, validate: 2.657539129257202\n",
      "Epoch 3/10. Iteration 2800/47167 Losses: train: 2.762775421142578, validate: 2.666929244995117\n",
      "Epoch 3/10. Iteration 2900/47167 Losses: train: 2.3126659393310547, validate: 2.668442487716675\n",
      "Epoch 3/10. Iteration 3000/47167 Losses: train: 2.5177628993988037, validate: 2.669123888015747\n",
      "Epoch 3/10. Iteration 3100/47167 Losses: train: 2.488237142562866, validate: 2.6682443618774414\n",
      "Epoch 3/10. Iteration 3200/47167 Losses: train: 2.4680254459381104, validate: 2.6619107723236084\n",
      "Epoch 3/10. Iteration 3300/47167 Losses: train: 2.3597424030303955, validate: 2.660130023956299\n",
      "Epoch 3/10. Iteration 3400/47167 Losses: train: 2.354917049407959, validate: 2.6662511825561523\n",
      "Epoch 3/10. Iteration 3500/47167 Losses: train: 2.444241762161255, validate: 2.6549410820007324\n",
      "Epoch 3/10. Iteration 3600/47167 Losses: train: 2.5784547328948975, validate: 2.6596436500549316\n",
      "Epoch 3/10. Iteration 3700/47167 Losses: train: 2.614309310913086, validate: 2.6643927097320557\n",
      "Epoch 3/10. Iteration 3800/47167 Losses: train: 2.445122241973877, validate: 2.6624066829681396\n",
      "Epoch 3/10. Iteration 3900/47167 Losses: train: 2.5718297958374023, validate: 2.667109251022339\n",
      "Epoch 3/10. Iteration 4000/47167 Losses: train: 2.659755229949951, validate: 2.6643097400665283\n",
      "Epoch 3/10. Iteration 4100/47167 Losses: train: 2.757523536682129, validate: 2.6551530361175537\n",
      "Epoch 3/10. Iteration 4200/47167 Losses: train: 2.785752296447754, validate: 2.66042423248291\n",
      "Epoch 3/10. Iteration 4300/47167 Losses: train: 2.378046751022339, validate: 2.6616053581237793\n",
      "Epoch 3/10. Iteration 4400/47167 Losses: train: 2.4766504764556885, validate: 2.652367353439331\n",
      "Epoch 3/10. Iteration 4500/47167 Losses: train: 2.715376138687134, validate: 2.653916358947754\n",
      "Epoch 3/10. Iteration 4600/47167 Losses: train: 2.649595260620117, validate: 2.658085823059082\n",
      "Epoch 3/10. Iteration 4700/47167 Losses: train: 2.7142810821533203, validate: 2.658475637435913\n",
      "Epoch 3/10. Iteration 4800/47167 Losses: train: 2.424865245819092, validate: 2.668304443359375\n",
      "Epoch 3/10. Iteration 4900/47167 Losses: train: 2.4178757667541504, validate: 2.662633180618286\n",
      "Epoch 3/10. Iteration 5000/47167 Losses: train: 2.56117844581604, validate: 2.6591575145721436\n",
      "Epoch 3/10. Iteration 5100/47167 Losses: train: 2.538752317428589, validate: 2.651266574859619\n",
      "Epoch 3/10. Iteration 5200/47167 Losses: train: 2.362018585205078, validate: 2.6633598804473877\n",
      "Epoch 3/10. Iteration 5300/47167 Losses: train: 2.5274548530578613, validate: 2.647486686706543\n",
      "Epoch 3/10. Iteration 5400/47167 Losses: train: 2.5219318866729736, validate: 2.6550869941711426\n",
      "Epoch 3/10. Iteration 5500/47167 Losses: train: 2.4005558490753174, validate: 2.6614627838134766\n",
      "Epoch 3/10. Iteration 5600/47167 Losses: train: 2.4341115951538086, validate: 2.6592934131622314\n",
      "Epoch 3/10. Iteration 5700/47167 Losses: train: 2.5358588695526123, validate: 2.6587696075439453\n",
      "Epoch 3/10. Iteration 5800/47167 Losses: train: 2.430169105529785, validate: 2.668846607208252\n",
      "Epoch 3/10. Iteration 5900/47167 Losses: train: 2.3973278999328613, validate: 2.665616750717163\n",
      "Epoch 3/10. Iteration 6000/47167 Losses: train: 2.418911933898926, validate: 2.651526927947998\n",
      "Epoch 3/10. Iteration 6100/47167 Losses: train: 2.334965944290161, validate: 2.655137300491333\n",
      "Epoch 3/10. Iteration 6200/47167 Losses: train: 2.3276543617248535, validate: 2.6617159843444824\n",
      "Epoch 3/10. Iteration 6300/47167 Losses: train: 2.381736993789673, validate: 2.6509432792663574\n",
      "Epoch 3/10. Iteration 6400/47167 Losses: train: 2.450166702270508, validate: 2.648850202560425\n",
      "Epoch 3/10. Iteration 6500/47167 Losses: train: 2.3606700897216797, validate: 2.6543171405792236\n",
      "Epoch 3/10. Iteration 6600/47167 Losses: train: 2.354351282119751, validate: 2.6493983268737793\n",
      "Epoch 3/10. Iteration 6700/47167 Losses: train: 2.6539058685302734, validate: 2.6577720642089844\n",
      "Epoch 3/10. Iteration 6800/47167 Losses: train: 2.395850658416748, validate: 2.652353525161743\n",
      "Epoch 3/10. Iteration 6900/47167 Losses: train: 2.4381258487701416, validate: 2.6547467708587646\n",
      "Epoch 3/10. Iteration 7000/47167 Losses: train: 2.523016929626465, validate: 2.665722608566284\n",
      "Epoch 3/10. Iteration 7100/47167 Losses: train: 2.424959182739258, validate: 2.65488862991333\n",
      "Epoch 3/10. Iteration 7200/47167 Losses: train: 2.43327260017395, validate: 2.657627582550049\n",
      "Epoch 3/10. Iteration 7300/47167 Losses: train: 2.4555251598358154, validate: 2.6534767150878906\n",
      "Epoch 3/10. Iteration 7400/47167 Losses: train: 2.6329879760742188, validate: 2.64749813079834\n",
      "Epoch 3/10. Iteration 7500/47167 Losses: train: 2.5193066596984863, validate: 2.6516308784484863\n",
      "Epoch 3/10. Iteration 7600/47167 Losses: train: 2.4025888442993164, validate: 2.6552469730377197\n",
      "Epoch 3/10. Iteration 7700/47167 Losses: train: 2.6817076206207275, validate: 2.6505637168884277\n",
      "Epoch 3/10. Iteration 7800/47167 Losses: train: 2.539802312850952, validate: 2.6490447521209717\n",
      "Epoch 3/10. Iteration 7900/47167 Losses: train: 2.6481845378875732, validate: 2.6468710899353027\n",
      "Epoch 3/10. Iteration 8000/47167 Losses: train: 2.648791790008545, validate: 2.6564502716064453\n",
      "Epoch 3/10. Iteration 8100/47167 Losses: train: 2.426112651824951, validate: 2.6492812633514404\n",
      "Epoch 3/10. Iteration 8200/47167 Losses: train: 2.530052423477173, validate: 2.6519908905029297\n",
      "Epoch 3/10. Iteration 8300/47167 Losses: train: 2.4674975872039795, validate: 2.6431667804718018\n",
      "Epoch 3/10. Iteration 8400/47167 Losses: train: 2.4747915267944336, validate: 2.6584699153900146\n",
      "Epoch 3/10. Iteration 8500/47167 Losses: train: 2.535240888595581, validate: 2.6506147384643555\n",
      "Epoch 3/10. Iteration 8600/47167 Losses: train: 2.5419418811798096, validate: 2.649984836578369\n",
      "Epoch 3/10. Iteration 8700/47167 Losses: train: 2.593935012817383, validate: 2.651712656021118\n",
      "Epoch 3/10. Iteration 8800/47167 Losses: train: 2.4390182495117188, validate: 2.6516366004943848\n",
      "Epoch 3/10. Iteration 8900/47167 Losses: train: 2.6078524589538574, validate: 2.648284435272217\n",
      "Epoch 3/10. Iteration 9000/47167 Losses: train: 2.355856418609619, validate: 2.649419069290161\n",
      "Epoch 3/10. Iteration 9100/47167 Losses: train: 2.4831507205963135, validate: 2.649481773376465\n",
      "Epoch 3/10. Iteration 9200/47167 Losses: train: 2.5733883380889893, validate: 2.6527373790740967\n",
      "Epoch 3/10. Iteration 9300/47167 Losses: train: 2.4490554332733154, validate: 2.649524450302124\n",
      "Epoch 3/10. Iteration 9400/47167 Losses: train: 2.509326696395874, validate: 2.655282974243164\n",
      "Epoch 3/10. Iteration 9500/47167 Losses: train: 2.6719698905944824, validate: 2.6489226818084717\n",
      "Epoch 3/10. Iteration 9600/47167 Losses: train: 2.539247989654541, validate: 2.6506567001342773\n",
      "Epoch 3/10. Iteration 9700/47167 Losses: train: 2.4305496215820312, validate: 2.6476125717163086\n",
      "Epoch 3/10. Iteration 9800/47167 Losses: train: 2.5172972679138184, validate: 2.643759250640869\n",
      "Epoch 3/10. Iteration 9900/47167 Losses: train: 2.4489455223083496, validate: 2.6487302780151367\n",
      "Epoch 3/10. Iteration 10000/47167 Losses: train: 2.4024317264556885, validate: 2.6416914463043213\n",
      "Epoch 3/10. Iteration 10100/47167 Losses: train: 2.6991851329803467, validate: 2.6515769958496094\n",
      "Epoch 3/10. Iteration 10200/47167 Losses: train: 2.4114294052124023, validate: 2.654049873352051\n",
      "Epoch 3/10. Iteration 10300/47167 Losses: train: 2.303632974624634, validate: 2.657266616821289\n",
      "Epoch 3/10. Iteration 10400/47167 Losses: train: 2.4574356079101562, validate: 2.646319627761841\n",
      "Epoch 3/10. Iteration 10500/47167 Losses: train: 2.650024890899658, validate: 2.6469831466674805\n",
      "Epoch 3/10. Iteration 10600/47167 Losses: train: 2.611758232116699, validate: 2.647304058074951\n",
      "Epoch 3/10. Iteration 10700/47167 Losses: train: 2.4343185424804688, validate: 2.6514382362365723\n",
      "Epoch 3/10. Iteration 10800/47167 Losses: train: 2.3964288234710693, validate: 2.652728319168091\n",
      "Epoch 3/10. Iteration 10900/47167 Losses: train: 2.4655163288116455, validate: 2.6557679176330566\n",
      "Epoch 3/10. Iteration 11000/47167 Losses: train: 2.5801644325256348, validate: 2.6461799144744873\n",
      "Epoch 3/10. Iteration 11100/47167 Losses: train: 2.506617546081543, validate: 2.63932204246521\n",
      "Epoch 3/10. Iteration 11200/47167 Losses: train: 2.4799978733062744, validate: 2.648956537246704\n",
      "Epoch 3/10. Iteration 11300/47167 Losses: train: 2.5511410236358643, validate: 2.652320146560669\n",
      "Epoch 3/10. Iteration 11400/47167 Losses: train: 2.558889865875244, validate: 2.6409032344818115\n",
      "Epoch 3/10. Iteration 11500/47167 Losses: train: 2.566270351409912, validate: 2.651468276977539\n",
      "Epoch 3/10. Iteration 11600/47167 Losses: train: 2.4955828189849854, validate: 2.643972873687744\n",
      "Epoch 3/10. Iteration 11700/47167 Losses: train: 2.6539931297302246, validate: 2.6493403911590576\n",
      "Epoch 3/10. Iteration 11800/47167 Losses: train: 2.451138973236084, validate: 2.6425282955169678\n",
      "Epoch 3/10. Iteration 11900/47167 Losses: train: 2.3517849445343018, validate: 2.645045757293701\n",
      "Epoch 3/10. Iteration 12000/47167 Losses: train: 2.4907097816467285, validate: 2.64456844329834\n",
      "Epoch 3/10. Iteration 12100/47167 Losses: train: 2.422379732131958, validate: 2.6411399841308594\n",
      "Epoch 3/10. Iteration 12200/47167 Losses: train: 2.4695212841033936, validate: 2.6446831226348877\n",
      "Epoch 3/10. Iteration 12300/47167 Losses: train: 2.2763726711273193, validate: 2.644861936569214\n",
      "Epoch 3/10. Iteration 12400/47167 Losses: train: 2.3227181434631348, validate: 2.647899627685547\n",
      "Epoch 3/10. Iteration 12500/47167 Losses: train: 2.47111439704895, validate: 2.65032958984375\n",
      "Epoch 3/10. Iteration 12600/47167 Losses: train: 2.4243950843811035, validate: 2.6559650897979736\n",
      "Epoch 3/10. Iteration 12700/47167 Losses: train: 2.660081148147583, validate: 2.648179054260254\n",
      "Epoch 3/10. Iteration 12800/47167 Losses: train: 2.658787488937378, validate: 2.6441643238067627\n",
      "Epoch 3/10. Iteration 12900/47167 Losses: train: 2.4553463459014893, validate: 2.648538827896118\n",
      "Epoch 3/10. Iteration 13000/47167 Losses: train: 2.482638359069824, validate: 2.6506059169769287\n",
      "Epoch 3/10. Iteration 13100/47167 Losses: train: 2.5308310985565186, validate: 2.6466751098632812\n",
      "Epoch 3/10. Iteration 13200/47167 Losses: train: 2.470991849899292, validate: 2.6437225341796875\n",
      "Epoch 3/10. Iteration 13300/47167 Losses: train: 2.4053642749786377, validate: 2.656513214111328\n",
      "Epoch 3/10. Iteration 13400/47167 Losses: train: 2.4912734031677246, validate: 2.646639347076416\n",
      "Epoch 3/10. Iteration 13500/47167 Losses: train: 2.5030722618103027, validate: 2.6511003971099854\n",
      "Epoch 3/10. Iteration 13600/47167 Losses: train: 2.6682355403900146, validate: 2.6409645080566406\n",
      "Epoch 3/10. Iteration 13700/47167 Losses: train: 2.4718103408813477, validate: 2.6459062099456787\n",
      "Epoch 3/10. Iteration 13800/47167 Losses: train: 2.673799514770508, validate: 2.6468818187713623\n",
      "Epoch 3/10. Iteration 13900/47167 Losses: train: 2.538449287414551, validate: 2.64739990234375\n",
      "Epoch 3/10. Iteration 14000/47167 Losses: train: 2.6289539337158203, validate: 2.640200614929199\n",
      "Epoch 3/10. Iteration 14100/47167 Losses: train: 2.7395505905151367, validate: 2.6425654888153076\n",
      "Epoch 3/10. Iteration 14200/47167 Losses: train: 2.4362680912017822, validate: 2.6428191661834717\n",
      "Epoch 3/10. Iteration 14300/47167 Losses: train: 2.4942479133605957, validate: 2.645380973815918\n",
      "Epoch 3/10. Iteration 14400/47167 Losses: train: 2.43782377243042, validate: 2.6427156925201416\n",
      "Epoch 3/10. Iteration 14500/47167 Losses: train: 2.495074510574341, validate: 2.646049976348877\n",
      "Epoch 3/10. Iteration 14600/47167 Losses: train: 2.3737688064575195, validate: 2.6401028633117676\n",
      "Epoch 3/10. Iteration 14700/47167 Losses: train: 2.840704917907715, validate: 2.644139528274536\n",
      "Epoch 3/10. Iteration 14800/47167 Losses: train: 2.526054859161377, validate: 2.6416261196136475\n",
      "Epoch 3/10. Iteration 14900/47167 Losses: train: 2.4702682495117188, validate: 2.6409757137298584\n",
      "Epoch 3/10. Iteration 15000/47167 Losses: train: 2.578245162963867, validate: 2.6403846740722656\n",
      "Epoch 3/10. Iteration 15100/47167 Losses: train: 2.410160779953003, validate: 2.63861083984375\n",
      "Epoch 3/10. Iteration 15200/47167 Losses: train: 2.3322761058807373, validate: 2.6378211975097656\n",
      "Epoch 3/10. Iteration 15300/47167 Losses: train: 2.5701069831848145, validate: 2.638896942138672\n",
      "Epoch 3/10. Iteration 15400/47167 Losses: train: 2.393650531768799, validate: 2.635751485824585\n",
      "Epoch 3/10. Iteration 15500/47167 Losses: train: 2.2610182762145996, validate: 2.637188673019409\n",
      "Epoch 3/10. Iteration 15600/47167 Losses: train: 2.3484861850738525, validate: 2.6369802951812744\n",
      "Epoch 3/10. Iteration 15700/47167 Losses: train: 2.6541740894317627, validate: 2.641768217086792\n",
      "Epoch 3/10. Iteration 15800/47167 Losses: train: 2.3124210834503174, validate: 2.6300432682037354\n",
      "Epoch 3/10. Iteration 15900/47167 Losses: train: 2.4658830165863037, validate: 2.633638381958008\n",
      "Epoch 3/10. Iteration 16000/47167 Losses: train: 2.3915250301361084, validate: 2.642545223236084\n",
      "Epoch 3/10. Iteration 16100/47167 Losses: train: 2.375491142272949, validate: 2.6372172832489014\n",
      "Epoch 3/10. Iteration 16200/47167 Losses: train: 2.484677791595459, validate: 2.644253969192505\n",
      "Epoch 3/10. Iteration 16300/47167 Losses: train: 2.4970622062683105, validate: 2.641686201095581\n",
      "Epoch 3/10. Iteration 16400/47167 Losses: train: 2.566770553588867, validate: 2.6360602378845215\n",
      "Epoch 3/10. Iteration 16500/47167 Losses: train: 2.361745834350586, validate: 2.6371750831604004\n",
      "Epoch 3/10. Iteration 16600/47167 Losses: train: 2.5503976345062256, validate: 2.633405923843384\n",
      "Epoch 3/10. Iteration 16700/47167 Losses: train: 2.498030662536621, validate: 2.635812997817993\n",
      "Epoch 3/10. Iteration 16800/47167 Losses: train: 2.5786361694335938, validate: 2.634047269821167\n",
      "Epoch 3/10. Iteration 16900/47167 Losses: train: 2.3515357971191406, validate: 2.6291325092315674\n",
      "Epoch 3/10. Iteration 17000/47167 Losses: train: 2.2707927227020264, validate: 2.626950740814209\n",
      "Epoch 3/10. Iteration 17100/47167 Losses: train: 2.534776210784912, validate: 2.631706714630127\n",
      "Epoch 3/10. Iteration 17200/47167 Losses: train: 2.3044681549072266, validate: 2.632310390472412\n",
      "Epoch 3/10. Iteration 17300/47167 Losses: train: 2.5516562461853027, validate: 2.6392877101898193\n",
      "Epoch 3/10. Iteration 17400/47167 Losses: train: 2.1971068382263184, validate: 2.633574962615967\n",
      "Epoch 3/10. Iteration 17500/47167 Losses: train: 2.4572501182556152, validate: 2.6400294303894043\n",
      "Epoch 3/10. Iteration 17600/47167 Losses: train: 2.323671340942383, validate: 2.6345443725585938\n",
      "Epoch 3/10. Iteration 17700/47167 Losses: train: 2.4088218212127686, validate: 2.63676118850708\n",
      "Epoch 3/10. Iteration 17800/47167 Losses: train: 2.4655721187591553, validate: 2.638684034347534\n",
      "Epoch 3/10. Iteration 17900/47167 Losses: train: 2.387314796447754, validate: 2.6306703090667725\n",
      "Epoch 3/10. Iteration 18000/47167 Losses: train: 2.2905547618865967, validate: 2.6431500911712646\n",
      "Epoch 3/10. Iteration 18100/47167 Losses: train: 2.4599199295043945, validate: 2.634176254272461\n",
      "Epoch 3/10. Iteration 18200/47167 Losses: train: 2.403947591781616, validate: 2.6336326599121094\n",
      "Epoch 3/10. Iteration 18300/47167 Losses: train: 2.4526820182800293, validate: 2.6326181888580322\n",
      "Epoch 3/10. Iteration 18400/47167 Losses: train: 2.5052266120910645, validate: 2.6363043785095215\n",
      "Epoch 3/10. Iteration 18500/47167 Losses: train: 2.409886121749878, validate: 2.626647472381592\n",
      "Epoch 3/10. Iteration 18600/47167 Losses: train: 2.757392168045044, validate: 2.6238725185394287\n",
      "Epoch 3/10. Iteration 18700/47167 Losses: train: 2.39152193069458, validate: 2.621774673461914\n",
      "Epoch 3/10. Iteration 18800/47167 Losses: train: 2.403170347213745, validate: 2.6378531455993652\n",
      "Epoch 3/10. Iteration 18900/47167 Losses: train: 2.5084073543548584, validate: 2.621595621109009\n",
      "Epoch 3/10. Iteration 19000/47167 Losses: train: 2.6845693588256836, validate: 2.62784481048584\n",
      "Epoch 3/10. Iteration 19100/47167 Losses: train: 2.4438741207122803, validate: 2.6345021724700928\n",
      "Epoch 3/10. Iteration 19200/47167 Losses: train: 2.492823362350464, validate: 2.6255836486816406\n",
      "Epoch 3/10. Iteration 19300/47167 Losses: train: 2.6949965953826904, validate: 2.631381034851074\n",
      "Epoch 3/10. Iteration 19400/47167 Losses: train: 2.350874423980713, validate: 2.625894069671631\n",
      "Epoch 3/10. Iteration 19500/47167 Losses: train: 2.41965913772583, validate: 2.6297428607940674\n",
      "Epoch 3/10. Iteration 19600/47167 Losses: train: 2.407352924346924, validate: 2.630957841873169\n",
      "Epoch 3/10. Iteration 19700/47167 Losses: train: 2.5170977115631104, validate: 2.628024101257324\n",
      "Epoch 3/10. Iteration 19800/47167 Losses: train: 2.3921196460723877, validate: 2.6355605125427246\n",
      "Epoch 3/10. Iteration 19900/47167 Losses: train: 2.5940473079681396, validate: 2.636026382446289\n",
      "Epoch 3/10. Iteration 20000/47167 Losses: train: 2.4886155128479004, validate: 2.6347508430480957\n",
      "Epoch 3/10. Iteration 20100/47167 Losses: train: 2.5079076290130615, validate: 2.630310535430908\n",
      "Epoch 3/10. Iteration 20200/47167 Losses: train: 2.6479599475860596, validate: 2.6250522136688232\n",
      "Epoch 3/10. Iteration 20300/47167 Losses: train: 2.4000015258789062, validate: 2.629190683364868\n",
      "Epoch 3/10. Iteration 20400/47167 Losses: train: 2.4493367671966553, validate: 2.6194612979888916\n",
      "Epoch 3/10. Iteration 20500/47167 Losses: train: 2.433535099029541, validate: 2.6173994541168213\n",
      "Epoch 3/10. Iteration 20600/47167 Losses: train: 2.5287373065948486, validate: 2.6167399883270264\n",
      "Epoch 3/10. Iteration 20700/47167 Losses: train: 2.394996404647827, validate: 2.6277592182159424\n",
      "Epoch 3/10. Iteration 20800/47167 Losses: train: 2.6058406829833984, validate: 2.617990016937256\n",
      "Epoch 3/10. Iteration 20900/47167 Losses: train: 2.516887664794922, validate: 2.6197943687438965\n",
      "Epoch 3/10. Iteration 21000/47167 Losses: train: 2.5073978900909424, validate: 2.6250336170196533\n",
      "Epoch 3/10. Iteration 21100/47167 Losses: train: 2.3215982913970947, validate: 2.6206281185150146\n",
      "Epoch 3/10. Iteration 21200/47167 Losses: train: 2.506065845489502, validate: 2.6271510124206543\n",
      "Epoch 3/10. Iteration 21300/47167 Losses: train: 2.5928263664245605, validate: 2.6303868293762207\n",
      "Epoch 3/10. Iteration 21400/47167 Losses: train: 2.4834628105163574, validate: 2.6305484771728516\n",
      "Epoch 3/10. Iteration 21500/47167 Losses: train: 2.6768970489501953, validate: 2.633720636367798\n",
      "Epoch 3/10. Iteration 21600/47167 Losses: train: 2.390331983566284, validate: 2.632179021835327\n",
      "Epoch 3/10. Iteration 21700/47167 Losses: train: 2.7405526638031006, validate: 2.6224541664123535\n",
      "Epoch 3/10. Iteration 21800/47167 Losses: train: 2.399129629135132, validate: 2.632327079772949\n",
      "Epoch 3/10. Iteration 21900/47167 Losses: train: 2.4924609661102295, validate: 2.6308412551879883\n",
      "Epoch 3/10. Iteration 22000/47167 Losses: train: 2.2961301803588867, validate: 2.6194093227386475\n",
      "Epoch 3/10. Iteration 22100/47167 Losses: train: 2.4288699626922607, validate: 2.6255719661712646\n",
      "Epoch 3/10. Iteration 22200/47167 Losses: train: 2.5863168239593506, validate: 2.6323399543762207\n",
      "Epoch 3/10. Iteration 22300/47167 Losses: train: 2.5064244270324707, validate: 2.6326990127563477\n",
      "Epoch 3/10. Iteration 22400/47167 Losses: train: 2.5461106300354004, validate: 2.6251139640808105\n",
      "Epoch 3/10. Iteration 22500/47167 Losses: train: 2.4552464485168457, validate: 2.6237876415252686\n",
      "Epoch 3/10. Iteration 22600/47167 Losses: train: 2.575239658355713, validate: 2.6237967014312744\n",
      "Epoch 3/10. Iteration 22700/47167 Losses: train: 2.4168827533721924, validate: 2.622194290161133\n",
      "Epoch 3/10. Iteration 22800/47167 Losses: train: 2.411688804626465, validate: 2.6349761486053467\n",
      "Epoch 3/10. Iteration 22900/47167 Losses: train: 2.469505786895752, validate: 2.6297123432159424\n",
      "Epoch 3/10. Iteration 23000/47167 Losses: train: 2.3277761936187744, validate: 2.6265223026275635\n",
      "Epoch 3/10. Iteration 23100/47167 Losses: train: 2.600950002670288, validate: 2.6259047985076904\n",
      "Epoch 3/10. Iteration 23200/47167 Losses: train: 2.5409727096557617, validate: 2.619753360748291\n",
      "Epoch 3/10. Iteration 23300/47167 Losses: train: 2.341458320617676, validate: 2.6204044818878174\n",
      "Epoch 3/10. Iteration 23400/47167 Losses: train: 2.7780847549438477, validate: 2.61250376701355\n",
      "Epoch 3/10. Iteration 23500/47167 Losses: train: 2.4203829765319824, validate: 2.6190497875213623\n",
      "Epoch 3/10. Iteration 23600/47167 Losses: train: 2.5849552154541016, validate: 2.6278271675109863\n",
      "Epoch 3/10. Iteration 23700/47167 Losses: train: 2.497318983078003, validate: 2.615063190460205\n",
      "Epoch 3/10. Iteration 23800/47167 Losses: train: 2.4937331676483154, validate: 2.6112077236175537\n",
      "Epoch 3/10. Iteration 23900/47167 Losses: train: 2.7432684898376465, validate: 2.611258029937744\n",
      "Epoch 3/10. Iteration 24000/47167 Losses: train: 2.4559764862060547, validate: 2.623324394226074\n",
      "Epoch 3/10. Iteration 24100/47167 Losses: train: 2.600451946258545, validate: 2.6161370277404785\n",
      "Epoch 3/10. Iteration 24200/47167 Losses: train: 2.39473032951355, validate: 2.619013786315918\n",
      "Epoch 3/10. Iteration 24300/47167 Losses: train: 2.5896904468536377, validate: 2.6143083572387695\n",
      "Epoch 3/10. Iteration 24400/47167 Losses: train: 2.325192928314209, validate: 2.6242077350616455\n",
      "Epoch 3/10. Iteration 24500/47167 Losses: train: 2.623584747314453, validate: 2.6208243370056152\n",
      "Epoch 3/10. Iteration 24600/47167 Losses: train: 2.511808395385742, validate: 2.6069376468658447\n",
      "Epoch 3/10. Iteration 24700/47167 Losses: train: 2.5340089797973633, validate: 2.6219635009765625\n",
      "Epoch 3/10. Iteration 24800/47167 Losses: train: 2.471115827560425, validate: 2.620830535888672\n",
      "Epoch 3/10. Iteration 24900/47167 Losses: train: 2.3645453453063965, validate: 2.6152255535125732\n",
      "Epoch 3/10. Iteration 25000/47167 Losses: train: 2.5637776851654053, validate: 2.619051694869995\n",
      "Epoch 3/10. Iteration 25100/47167 Losses: train: 2.644270658493042, validate: 2.626572608947754\n",
      "Epoch 3/10. Iteration 25200/47167 Losses: train: 2.5232386589050293, validate: 2.61299467086792\n",
      "Epoch 3/10. Iteration 25300/47167 Losses: train: 2.4617247581481934, validate: 2.618349552154541\n",
      "Epoch 3/10. Iteration 25400/47167 Losses: train: 2.6333107948303223, validate: 2.6174659729003906\n",
      "Epoch 3/10. Iteration 25500/47167 Losses: train: 2.5345957279205322, validate: 2.6216187477111816\n",
      "Epoch 3/10. Iteration 25600/47167 Losses: train: 2.45803165435791, validate: 2.623690128326416\n",
      "Epoch 3/10. Iteration 25700/47167 Losses: train: 2.459981679916382, validate: 2.6165947914123535\n",
      "Epoch 3/10. Iteration 25800/47167 Losses: train: 2.4599149227142334, validate: 2.624354124069214\n",
      "Epoch 3/10. Iteration 25900/47167 Losses: train: 2.404552459716797, validate: 2.60394287109375\n",
      "Epoch 3/10. Iteration 26000/47167 Losses: train: 2.3791117668151855, validate: 2.6239218711853027\n",
      "Epoch 3/10. Iteration 26100/47167 Losses: train: 2.6006526947021484, validate: 2.6241257190704346\n",
      "Epoch 3/10. Iteration 26200/47167 Losses: train: 2.332855224609375, validate: 2.6204535961151123\n",
      "Epoch 3/10. Iteration 26300/47167 Losses: train: 2.487488269805908, validate: 2.6134488582611084\n",
      "Epoch 3/10. Iteration 26400/47167 Losses: train: 2.3701257705688477, validate: 2.614821434020996\n",
      "Epoch 3/10. Iteration 26500/47167 Losses: train: 2.4996485710144043, validate: 2.6121225357055664\n",
      "Epoch 3/10. Iteration 26600/47167 Losses: train: 2.5445003509521484, validate: 2.614928960800171\n",
      "Epoch 3/10. Iteration 26700/47167 Losses: train: 2.578599214553833, validate: 2.616248607635498\n",
      "Epoch 3/10. Iteration 26800/47167 Losses: train: 2.3743879795074463, validate: 2.615339517593384\n",
      "Epoch 3/10. Iteration 26900/47167 Losses: train: 2.5235116481781006, validate: 2.6139965057373047\n",
      "Epoch 3/10. Iteration 27000/47167 Losses: train: 2.7067325115203857, validate: 2.6149001121520996\n",
      "Epoch 3/10. Iteration 27100/47167 Losses: train: 2.3577218055725098, validate: 2.607520341873169\n",
      "Epoch 3/10. Iteration 27200/47167 Losses: train: 2.292762279510498, validate: 2.6164019107818604\n",
      "Epoch 3/10. Iteration 27300/47167 Losses: train: 2.5860326290130615, validate: 2.6310014724731445\n",
      "Epoch 3/10. Iteration 27400/47167 Losses: train: 2.4505670070648193, validate: 2.616257667541504\n",
      "Epoch 3/10. Iteration 27500/47167 Losses: train: 2.6246190071105957, validate: 2.6124486923217773\n",
      "Epoch 3/10. Iteration 27600/47167 Losses: train: 2.3450708389282227, validate: 2.6187777519226074\n",
      "Epoch 3/10. Iteration 27700/47167 Losses: train: 2.5467984676361084, validate: 2.6155831813812256\n",
      "Epoch 3/10. Iteration 27800/47167 Losses: train: 2.356912851333618, validate: 2.61712646484375\n",
      "Epoch 3/10. Iteration 27900/47167 Losses: train: 2.4926373958587646, validate: 2.610671043395996\n",
      "Epoch 3/10. Iteration 28000/47167 Losses: train: 2.379570484161377, validate: 2.612743377685547\n",
      "Epoch 3/10. Iteration 28100/47167 Losses: train: 2.557102918624878, validate: 2.6157703399658203\n",
      "Epoch 3/10. Iteration 28200/47167 Losses: train: 2.5147478580474854, validate: 2.6167659759521484\n",
      "Epoch 3/10. Iteration 28300/47167 Losses: train: 2.525991916656494, validate: 2.6103897094726562\n",
      "Epoch 3/10. Iteration 28400/47167 Losses: train: 2.621856927871704, validate: 2.6114723682403564\n",
      "Epoch 3/10. Iteration 28500/47167 Losses: train: 2.4013266563415527, validate: 2.60898756980896\n",
      "Epoch 3/10. Iteration 28600/47167 Losses: train: 2.336113929748535, validate: 2.605048656463623\n",
      "Epoch 3/10. Iteration 28700/47167 Losses: train: 2.634615421295166, validate: 2.604046106338501\n",
      "Epoch 3/10. Iteration 28800/47167 Losses: train: 2.4318151473999023, validate: 2.6145989894866943\n",
      "Epoch 3/10. Iteration 28900/47167 Losses: train: 2.590484857559204, validate: 2.6153995990753174\n",
      "Epoch 3/10. Iteration 29000/47167 Losses: train: 2.6269397735595703, validate: 2.607520580291748\n",
      "Epoch 3/10. Iteration 29100/47167 Losses: train: 2.440023899078369, validate: 2.614738702774048\n",
      "Epoch 3/10. Iteration 29200/47167 Losses: train: 2.5708236694335938, validate: 2.608397960662842\n",
      "Epoch 3/10. Iteration 29300/47167 Losses: train: 2.5690054893493652, validate: 2.61187481880188\n",
      "Epoch 3/10. Iteration 29400/47167 Losses: train: 2.4937028884887695, validate: 2.605613946914673\n",
      "Epoch 3/10. Iteration 29500/47167 Losses: train: 2.3040058612823486, validate: 2.61869215965271\n",
      "Epoch 3/10. Iteration 29600/47167 Losses: train: 2.309720277786255, validate: 2.6126046180725098\n",
      "Epoch 3/10. Iteration 29700/47167 Losses: train: 2.6282613277435303, validate: 2.6200497150421143\n",
      "Epoch 3/10. Iteration 29800/47167 Losses: train: 2.4753713607788086, validate: 2.6114144325256348\n",
      "Epoch 3/10. Iteration 29900/47167 Losses: train: 2.483055353164673, validate: 2.6150288581848145\n",
      "Epoch 3/10. Iteration 30000/47167 Losses: train: 2.4447667598724365, validate: 2.6099627017974854\n",
      "Epoch 3/10. Iteration 30100/47167 Losses: train: 2.682865619659424, validate: 2.5992114543914795\n",
      "Epoch 3/10. Iteration 30200/47167 Losses: train: 2.3971495628356934, validate: 2.6067745685577393\n",
      "Epoch 3/10. Iteration 30300/47167 Losses: train: 2.5771775245666504, validate: 2.6098599433898926\n",
      "Epoch 3/10. Iteration 30400/47167 Losses: train: 2.466643810272217, validate: 2.6058993339538574\n",
      "Epoch 3/10. Iteration 30500/47167 Losses: train: 2.3682119846343994, validate: 2.6075706481933594\n",
      "Epoch 3/10. Iteration 30600/47167 Losses: train: 2.549912214279175, validate: 2.616305351257324\n",
      "Epoch 3/10. Iteration 30700/47167 Losses: train: 2.5100653171539307, validate: 2.621628522872925\n",
      "Epoch 3/10. Iteration 30800/47167 Losses: train: 2.515075922012329, validate: 2.609290599822998\n",
      "Epoch 3/10. Iteration 30900/47167 Losses: train: 2.632606029510498, validate: 2.6108033657073975\n",
      "Epoch 3/10. Iteration 31000/47167 Losses: train: 2.3760600090026855, validate: 2.617845058441162\n",
      "Epoch 3/10. Iteration 31100/47167 Losses: train: 2.685875177383423, validate: 2.6115808486938477\n",
      "Epoch 3/10. Iteration 31200/47167 Losses: train: 2.4629406929016113, validate: 2.61689829826355\n",
      "Epoch 3/10. Iteration 31300/47167 Losses: train: 2.4392621517181396, validate: 2.6133270263671875\n",
      "Epoch 3/10. Iteration 31400/47167 Losses: train: 2.4698586463928223, validate: 2.605163097381592\n",
      "Epoch 3/10. Iteration 31500/47167 Losses: train: 2.4952430725097656, validate: 2.6122968196868896\n",
      "Epoch 3/10. Iteration 31600/47167 Losses: train: 2.3889853954315186, validate: 2.6056737899780273\n",
      "Epoch 3/10. Iteration 31700/47167 Losses: train: 2.3437247276306152, validate: 2.616034984588623\n",
      "Epoch 3/10. Iteration 31800/47167 Losses: train: 2.4671976566314697, validate: 2.60331130027771\n",
      "Epoch 3/10. Iteration 31900/47167 Losses: train: 2.608693838119507, validate: 2.6172516345977783\n",
      "Epoch 3/10. Iteration 32000/47167 Losses: train: 2.3187508583068848, validate: 2.5978193283081055\n",
      "Epoch 3/10. Iteration 32100/47167 Losses: train: 2.6215953826904297, validate: 2.6005406379699707\n",
      "Epoch 3/10. Iteration 32200/47167 Losses: train: 2.508336305618286, validate: 2.606719493865967\n",
      "Epoch 3/10. Iteration 32300/47167 Losses: train: 2.563955307006836, validate: 2.6150906085968018\n",
      "Epoch 3/10. Iteration 32400/47167 Losses: train: 2.4109983444213867, validate: 2.605748414993286\n",
      "Epoch 3/10. Iteration 32500/47167 Losses: train: 2.4800004959106445, validate: 2.600229024887085\n",
      "Epoch 3/10. Iteration 32600/47167 Losses: train: 2.675251007080078, validate: 2.601057291030884\n",
      "Epoch 3/10. Iteration 32700/47167 Losses: train: 2.5579681396484375, validate: 2.593764066696167\n",
      "Epoch 3/10. Iteration 32800/47167 Losses: train: 2.2856881618499756, validate: 2.602342367172241\n",
      "Epoch 3/10. Iteration 32900/47167 Losses: train: 2.682101249694824, validate: 2.6101858615875244\n",
      "Epoch 3/10. Iteration 33000/47167 Losses: train: 2.3198447227478027, validate: 2.610576629638672\n",
      "Epoch 3/10. Iteration 33100/47167 Losses: train: 2.746542453765869, validate: 2.5972492694854736\n",
      "Epoch 3/10. Iteration 33200/47167 Losses: train: 2.544398546218872, validate: 2.6085047721862793\n",
      "Epoch 3/10. Iteration 33300/47167 Losses: train: 2.3589489459991455, validate: 2.595017671585083\n",
      "Epoch 3/10. Iteration 33400/47167 Losses: train: 2.350414752960205, validate: 2.602801561355591\n",
      "Epoch 3/10. Iteration 33500/47167 Losses: train: 2.5215201377868652, validate: 2.5988166332244873\n",
      "Epoch 3/10. Iteration 33600/47167 Losses: train: 2.242827892303467, validate: 2.611058473587036\n",
      "Epoch 3/10. Iteration 33700/47167 Losses: train: 2.299346446990967, validate: 2.6013360023498535\n",
      "Epoch 3/10. Iteration 33800/47167 Losses: train: 2.4050116539001465, validate: 2.6079909801483154\n",
      "Epoch 3/10. Iteration 33900/47167 Losses: train: 2.676426649093628, validate: 2.6047239303588867\n",
      "Epoch 3/10. Iteration 34000/47167 Losses: train: 2.4086496829986572, validate: 2.597684383392334\n",
      "Epoch 3/10. Iteration 34100/47167 Losses: train: 2.4304146766662598, validate: 2.6118345260620117\n",
      "Epoch 3/10. Iteration 34200/47167 Losses: train: 2.441612482070923, validate: 2.595264434814453\n",
      "Epoch 3/10. Iteration 34300/47167 Losses: train: 2.39107346534729, validate: 2.606214761734009\n",
      "Epoch 3/10. Iteration 34400/47167 Losses: train: 2.4454305171966553, validate: 2.595478057861328\n",
      "Epoch 3/10. Iteration 34500/47167 Losses: train: 2.4613211154937744, validate: 2.598527669906616\n",
      "Epoch 3/10. Iteration 34600/47167 Losses: train: 2.4487357139587402, validate: 2.593963384628296\n",
      "Epoch 3/10. Iteration 34700/47167 Losses: train: 2.372910261154175, validate: 2.5960676670074463\n",
      "Epoch 3/10. Iteration 34800/47167 Losses: train: 2.4739253520965576, validate: 2.5881707668304443\n",
      "Epoch 3/10. Iteration 34900/47167 Losses: train: 2.5293309688568115, validate: 2.58583664894104\n",
      "Epoch 3/10. Iteration 35000/47167 Losses: train: 2.5146706104278564, validate: 2.5995686054229736\n",
      "Epoch 3/10. Iteration 35100/47167 Losses: train: 2.6980202198028564, validate: 2.6078131198883057\n",
      "Epoch 3/10. Iteration 35200/47167 Losses: train: 2.549041509628296, validate: 2.5951292514801025\n",
      "Epoch 3/10. Iteration 35300/47167 Losses: train: 2.572772979736328, validate: 2.59582257270813\n",
      "Epoch 3/10. Iteration 35400/47167 Losses: train: 2.575129985809326, validate: 2.598357915878296\n",
      "Epoch 3/10. Iteration 35500/47167 Losses: train: 2.2631149291992188, validate: 2.6001830101013184\n",
      "Epoch 3/10. Iteration 35600/47167 Losses: train: 2.570005416870117, validate: 2.603766918182373\n",
      "Epoch 3/10. Iteration 35700/47167 Losses: train: 2.271505832672119, validate: 2.5941474437713623\n",
      "Epoch 3/10. Iteration 35800/47167 Losses: train: 2.573017120361328, validate: 2.5885679721832275\n",
      "Epoch 3/10. Iteration 35900/47167 Losses: train: 2.4051320552825928, validate: 2.5944788455963135\n",
      "Epoch 3/10. Iteration 36000/47167 Losses: train: 2.432955741882324, validate: 2.603388786315918\n",
      "Epoch 3/10. Iteration 36100/47167 Losses: train: 2.229097366333008, validate: 2.588371992111206\n",
      "Epoch 3/10. Iteration 36200/47167 Losses: train: 2.3592724800109863, validate: 2.592440128326416\n",
      "Epoch 3/10. Iteration 36300/47167 Losses: train: 2.4463610649108887, validate: 2.613492012023926\n",
      "Epoch 3/10. Iteration 36400/47167 Losses: train: 2.418853282928467, validate: 2.5908315181732178\n",
      "Epoch 3/10. Iteration 36500/47167 Losses: train: 2.474719524383545, validate: 2.589179039001465\n",
      "Epoch 3/10. Iteration 36600/47167 Losses: train: 2.552948236465454, validate: 2.5887653827667236\n",
      "Epoch 3/10. Iteration 36700/47167 Losses: train: 2.450092077255249, validate: 2.5901284217834473\n",
      "Epoch 3/10. Iteration 36800/47167 Losses: train: 2.4231607913970947, validate: 2.5955610275268555\n",
      "Epoch 3/10. Iteration 36900/47167 Losses: train: 2.499213695526123, validate: 2.5952019691467285\n",
      "Epoch 3/10. Iteration 37000/47167 Losses: train: 2.5814032554626465, validate: 2.591362714767456\n",
      "Epoch 3/10. Iteration 37100/47167 Losses: train: 2.530855894088745, validate: 2.6017894744873047\n",
      "Epoch 3/10. Iteration 37200/47167 Losses: train: 2.3910958766937256, validate: 2.590808868408203\n",
      "Epoch 3/10. Iteration 37300/47167 Losses: train: 2.4373295307159424, validate: 2.598363161087036\n",
      "Epoch 3/10. Iteration 37400/47167 Losses: train: 2.477590322494507, validate: 2.5848193168640137\n",
      "Epoch 3/10. Iteration 37500/47167 Losses: train: 2.4654054641723633, validate: 2.5937962532043457\n",
      "Epoch 3/10. Iteration 37600/47167 Losses: train: 2.606233596801758, validate: 2.592670440673828\n",
      "Epoch 3/10. Iteration 37700/47167 Losses: train: 2.5780489444732666, validate: 2.589991807937622\n",
      "Epoch 3/10. Iteration 37800/47167 Losses: train: 2.4060046672821045, validate: 2.5874757766723633\n",
      "Epoch 3/10. Iteration 37900/47167 Losses: train: 2.44277024269104, validate: 2.5985751152038574\n",
      "Epoch 3/10. Iteration 38000/47167 Losses: train: 2.6246814727783203, validate: 2.590020179748535\n",
      "Epoch 3/10. Iteration 38100/47167 Losses: train: 2.6884782314300537, validate: 2.589938163757324\n",
      "Epoch 3/10. Iteration 38200/47167 Losses: train: 2.3256125450134277, validate: 2.5964319705963135\n",
      "Epoch 3/10. Iteration 38300/47167 Losses: train: 2.3626348972320557, validate: 2.5929813385009766\n",
      "Epoch 3/10. Iteration 38400/47167 Losses: train: 2.6953933238983154, validate: 2.5829644203186035\n",
      "Epoch 3/10. Iteration 38500/47167 Losses: train: 2.4353113174438477, validate: 2.586554765701294\n",
      "Epoch 3/10. Iteration 38600/47167 Losses: train: 2.475517749786377, validate: 2.594301462173462\n",
      "Epoch 3/10. Iteration 38700/47167 Losses: train: 2.4589321613311768, validate: 2.5813238620758057\n",
      "Epoch 3/10. Iteration 38800/47167 Losses: train: 2.2956929206848145, validate: 2.5877976417541504\n",
      "Epoch 3/10. Iteration 38900/47167 Losses: train: 2.4367520809173584, validate: 2.598701238632202\n",
      "Epoch 3/10. Iteration 39000/47167 Losses: train: 2.587371349334717, validate: 2.5923213958740234\n",
      "Epoch 3/10. Iteration 39100/47167 Losses: train: 2.372696876525879, validate: 2.5790932178497314\n",
      "Epoch 3/10. Iteration 39200/47167 Losses: train: 2.4050824642181396, validate: 2.5729241371154785\n",
      "Epoch 3/10. Iteration 39300/47167 Losses: train: 2.576968193054199, validate: 2.5799977779388428\n",
      "Epoch 3/10. Iteration 39400/47167 Losses: train: 2.4355196952819824, validate: 2.581926107406616\n",
      "Epoch 3/10. Iteration 39500/47167 Losses: train: 2.3027820587158203, validate: 2.5850117206573486\n",
      "Epoch 3/10. Iteration 39600/47167 Losses: train: 2.349093437194824, validate: 2.5859055519104004\n",
      "Epoch 3/10. Iteration 39700/47167 Losses: train: 2.491908073425293, validate: 2.5853617191314697\n",
      "Epoch 3/10. Iteration 39800/47167 Losses: train: 2.4915642738342285, validate: 2.5824711322784424\n",
      "Epoch 3/10. Iteration 39900/47167 Losses: train: 2.314709186553955, validate: 2.590947151184082\n",
      "Epoch 3/10. Iteration 40000/47167 Losses: train: 2.432507276535034, validate: 2.5834953784942627\n",
      "Epoch 3/10. Iteration 40100/47167 Losses: train: 2.303741931915283, validate: 2.5874812602996826\n",
      "Epoch 3/10. Iteration 40200/47167 Losses: train: 2.532132387161255, validate: 2.578697681427002\n",
      "Epoch 3/10. Iteration 40300/47167 Losses: train: 2.546379327774048, validate: 2.582371950149536\n",
      "Epoch 3/10. Iteration 40400/47167 Losses: train: 2.4312708377838135, validate: 2.589430570602417\n",
      "Epoch 3/10. Iteration 40500/47167 Losses: train: 2.3716256618499756, validate: 2.5914645195007324\n",
      "Epoch 3/10. Iteration 40600/47167 Losses: train: 2.456549882888794, validate: 2.5926573276519775\n",
      "Epoch 3/10. Iteration 40700/47167 Losses: train: 2.3056657314300537, validate: 2.5902392864227295\n",
      "Epoch 3/10. Iteration 40800/47167 Losses: train: 2.5681231021881104, validate: 2.592759370803833\n",
      "Epoch 3/10. Iteration 40900/47167 Losses: train: 2.5464324951171875, validate: 2.584650993347168\n",
      "Epoch 3/10. Iteration 41000/47167 Losses: train: 2.369227886199951, validate: 2.5883209705352783\n",
      "Epoch 3/10. Iteration 41100/47167 Losses: train: 2.3365440368652344, validate: 2.593675374984741\n",
      "Epoch 3/10. Iteration 41200/47167 Losses: train: 2.335634231567383, validate: 2.588291645050049\n",
      "Epoch 3/10. Iteration 41300/47167 Losses: train: 2.3196473121643066, validate: 2.58762526512146\n",
      "Epoch 3/10. Iteration 41400/47167 Losses: train: 2.470475435256958, validate: 2.58083438873291\n",
      "Epoch 3/10. Iteration 41500/47167 Losses: train: 2.420037031173706, validate: 2.5844995975494385\n",
      "Epoch 3/10. Iteration 41600/47167 Losses: train: 2.512861490249634, validate: 2.596079111099243\n",
      "Epoch 3/10. Iteration 41700/47167 Losses: train: 2.427931070327759, validate: 2.5803282260894775\n",
      "Epoch 3/10. Iteration 41800/47167 Losses: train: 2.415691375732422, validate: 2.590087890625\n",
      "Epoch 3/10. Iteration 41900/47167 Losses: train: 2.4289944171905518, validate: 2.5990166664123535\n",
      "Epoch 3/10. Iteration 42000/47167 Losses: train: 2.4027366638183594, validate: 2.594951629638672\n",
      "Epoch 3/10. Iteration 42100/47167 Losses: train: 2.4587817192077637, validate: 2.584343194961548\n",
      "Epoch 3/10. Iteration 42200/47167 Losses: train: 2.559957504272461, validate: 2.5845038890838623\n",
      "Epoch 3/10. Iteration 42300/47167 Losses: train: 2.462872266769409, validate: 2.5891895294189453\n",
      "Epoch 3/10. Iteration 42400/47167 Losses: train: 2.3638617992401123, validate: 2.5813846588134766\n",
      "Epoch 3/10. Iteration 42500/47167 Losses: train: 2.62814998626709, validate: 2.5830891132354736\n",
      "Epoch 3/10. Iteration 42600/47167 Losses: train: 2.5956437587738037, validate: 2.5816650390625\n",
      "Epoch 3/10. Iteration 42700/47167 Losses: train: 2.4762930870056152, validate: 2.5800094604492188\n",
      "Epoch 3/10. Iteration 42800/47167 Losses: train: 2.313552141189575, validate: 2.591191530227661\n",
      "Epoch 3/10. Iteration 42900/47167 Losses: train: 2.469282627105713, validate: 2.585186004638672\n",
      "Epoch 3/10. Iteration 43000/47167 Losses: train: 2.5373482704162598, validate: 2.586388349533081\n",
      "Epoch 3/10. Iteration 43100/47167 Losses: train: 2.483532428741455, validate: 2.583625316619873\n",
      "Epoch 3/10. Iteration 43200/47167 Losses: train: 2.535330295562744, validate: 2.589360237121582\n",
      "Epoch 3/10. Iteration 43300/47167 Losses: train: 2.534827470779419, validate: 2.588691234588623\n",
      "Epoch 3/10. Iteration 43400/47167 Losses: train: 2.098954677581787, validate: 2.5964012145996094\n",
      "Epoch 3/10. Iteration 43500/47167 Losses: train: 2.5592682361602783, validate: 2.5921630859375\n",
      "Epoch 3/10. Iteration 43600/47167 Losses: train: 2.646942138671875, validate: 2.586711883544922\n",
      "Epoch 3/10. Iteration 43700/47167 Losses: train: 2.418598175048828, validate: 2.586022138595581\n",
      "Epoch 3/10. Iteration 43800/47167 Losses: train: 2.5142440795898438, validate: 2.5826668739318848\n",
      "Epoch 3/10. Iteration 43900/47167 Losses: train: 2.3863649368286133, validate: 2.5806734561920166\n",
      "Epoch 3/10. Iteration 44000/47167 Losses: train: 2.238882303237915, validate: 2.579252004623413\n",
      "Epoch 3/10. Iteration 44100/47167 Losses: train: 2.434095621109009, validate: 2.5762290954589844\n",
      "Epoch 3/10. Iteration 44200/47167 Losses: train: 2.39107346534729, validate: 2.5795648097991943\n",
      "Epoch 3/10. Iteration 44300/47167 Losses: train: 2.3557586669921875, validate: 2.5865108966827393\n",
      "Epoch 3/10. Iteration 44400/47167 Losses: train: 2.434333324432373, validate: 2.5783674716949463\n",
      "Epoch 3/10. Iteration 44500/47167 Losses: train: 2.4330575466156006, validate: 2.5815978050231934\n",
      "Epoch 3/10. Iteration 44600/47167 Losses: train: 2.511018991470337, validate: 2.587135076522827\n",
      "Epoch 3/10. Iteration 44700/47167 Losses: train: 2.5172412395477295, validate: 2.5796566009521484\n",
      "Epoch 3/10. Iteration 44800/47167 Losses: train: 2.3138904571533203, validate: 2.568711042404175\n",
      "Epoch 3/10. Iteration 44900/47167 Losses: train: 2.683370351791382, validate: 2.571310043334961\n",
      "Epoch 3/10. Iteration 45000/47167 Losses: train: 2.2823312282562256, validate: 2.578216552734375\n",
      "Epoch 3/10. Iteration 45100/47167 Losses: train: 2.4652504920959473, validate: 2.5762412548065186\n",
      "Epoch 3/10. Iteration 45200/47167 Losses: train: 2.4153711795806885, validate: 2.5766890048980713\n",
      "Epoch 3/10. Iteration 45300/47167 Losses: train: 2.4333419799804688, validate: 2.572014331817627\n",
      "Epoch 3/10. Iteration 45400/47167 Losses: train: 2.5487239360809326, validate: 2.575566291809082\n",
      "Epoch 3/10. Iteration 45500/47167 Losses: train: 2.669384717941284, validate: 2.568665027618408\n",
      "Epoch 3/10. Iteration 45600/47167 Losses: train: 2.4016313552856445, validate: 2.578735113143921\n",
      "Epoch 3/10. Iteration 45700/47167 Losses: train: 2.2190182209014893, validate: 2.575731039047241\n",
      "Epoch 3/10. Iteration 45800/47167 Losses: train: 2.5767340660095215, validate: 2.572157144546509\n",
      "Epoch 3/10. Iteration 45900/47167 Losses: train: 2.482133388519287, validate: 2.576420783996582\n",
      "Epoch 3/10. Iteration 46000/47167 Losses: train: 2.404479503631592, validate: 2.5720255374908447\n",
      "Epoch 3/10. Iteration 46100/47167 Losses: train: 2.540903329849243, validate: 2.577089548110962\n",
      "Epoch 3/10. Iteration 46200/47167 Losses: train: 2.567298650741577, validate: 2.5787370204925537\n",
      "Epoch 3/10. Iteration 46300/47167 Losses: train: 2.5658392906188965, validate: 2.5762462615966797\n",
      "Epoch 3/10. Iteration 46400/47167 Losses: train: 2.5874874591827393, validate: 2.5836379528045654\n",
      "Epoch 3/10. Iteration 46500/47167 Losses: train: 2.5370402336120605, validate: 2.575242280960083\n",
      "Epoch 3/10. Iteration 46600/47167 Losses: train: 2.4687724113464355, validate: 2.5687787532806396\n",
      "Epoch 3/10. Iteration 46700/47167 Losses: train: 2.4290130138397217, validate: 2.5669517517089844\n",
      "Epoch 3/10. Iteration 46800/47167 Losses: train: 2.594714403152466, validate: 2.5742859840393066\n",
      "Epoch 3/10. Iteration 46900/47167 Losses: train: 2.4135398864746094, validate: 2.572126626968384\n",
      "Epoch 3/10. Iteration 47000/47167 Losses: train: 2.5570240020751953, validate: 2.575917959213257\n",
      "Epoch 3/10. Iteration 47100/47167 Losses: train: 2.3554742336273193, validate: 2.568725824356079\n",
      "Epoch 4/10. Iteration 100/47167 Losses: train: 2.358560562133789, validate: 2.569305896759033\n",
      "Epoch 4/10. Iteration 200/47167 Losses: train: 2.4354257583618164, validate: 2.5728824138641357\n",
      "Epoch 4/10. Iteration 300/47167 Losses: train: 2.334766387939453, validate: 2.5648298263549805\n",
      "Epoch 4/10. Iteration 400/47167 Losses: train: 2.38896107673645, validate: 2.580287456512451\n",
      "Epoch 4/10. Iteration 500/47167 Losses: train: 2.318098306655884, validate: 2.56770396232605\n",
      "Epoch 4/10. Iteration 600/47167 Losses: train: 2.3814518451690674, validate: 2.566357374191284\n",
      "Epoch 4/10. Iteration 700/47167 Losses: train: 2.6289165019989014, validate: 2.566730260848999\n",
      "Epoch 4/10. Iteration 800/47167 Losses: train: 2.514841079711914, validate: 2.564732789993286\n",
      "Epoch 4/10. Iteration 900/47167 Losses: train: 2.3501718044281006, validate: 2.5766704082489014\n",
      "Epoch 4/10. Iteration 1000/47167 Losses: train: 2.292475700378418, validate: 2.5645315647125244\n",
      "Epoch 4/10. Iteration 1100/47167 Losses: train: 2.516352891921997, validate: 2.566180944442749\n",
      "Epoch 4/10. Iteration 1200/47167 Losses: train: 2.288172721862793, validate: 2.571709394454956\n",
      "Epoch 4/10. Iteration 1300/47167 Losses: train: 2.3520872592926025, validate: 2.5785670280456543\n",
      "Epoch 4/10. Iteration 1400/47167 Losses: train: 2.437878370285034, validate: 2.5763163566589355\n",
      "Epoch 4/10. Iteration 1500/47167 Losses: train: 2.5368006229400635, validate: 2.5700457096099854\n",
      "Epoch 4/10. Iteration 1600/47167 Losses: train: 2.321932554244995, validate: 2.5733554363250732\n",
      "Epoch 4/10. Iteration 1700/47167 Losses: train: 2.5450727939605713, validate: 2.5735933780670166\n",
      "Epoch 4/10. Iteration 1800/47167 Losses: train: 2.326667308807373, validate: 2.57110333442688\n",
      "Epoch 4/10. Iteration 1900/47167 Losses: train: 2.2665271759033203, validate: 2.571889638900757\n",
      "Epoch 4/10. Iteration 2000/47167 Losses: train: 2.27667498588562, validate: 2.5703556537628174\n",
      "Epoch 4/10. Iteration 2100/47167 Losses: train: 2.2904880046844482, validate: 2.576206922531128\n",
      "Epoch 4/10. Iteration 2200/47167 Losses: train: 2.3619418144226074, validate: 2.5748047828674316\n",
      "Epoch 4/10. Iteration 2300/47167 Losses: train: 2.442046880722046, validate: 2.565002202987671\n",
      "Epoch 4/10. Iteration 2400/47167 Losses: train: 2.547837257385254, validate: 2.570726156234741\n",
      "Epoch 4/10. Iteration 2500/47167 Losses: train: 2.416916847229004, validate: 2.5823402404785156\n",
      "Epoch 4/10. Iteration 2600/47167 Losses: train: 2.435023307800293, validate: 2.5697500705718994\n",
      "Epoch 4/10. Iteration 2700/47167 Losses: train: 2.508061170578003, validate: 2.570124387741089\n",
      "Epoch 4/10. Iteration 2800/47167 Losses: train: 2.475865602493286, validate: 2.586695909500122\n",
      "Epoch 4/10. Iteration 2900/47167 Losses: train: 2.34856915473938, validate: 2.5771360397338867\n",
      "Epoch 4/10. Iteration 3000/47167 Losses: train: 2.4230072498321533, validate: 2.572721481323242\n",
      "Epoch 4/10. Iteration 3100/47167 Losses: train: 2.4914703369140625, validate: 2.573035717010498\n",
      "Epoch 4/10. Iteration 3200/47167 Losses: train: 2.3816025257110596, validate: 2.5715150833129883\n",
      "Epoch 4/10. Iteration 3300/47167 Losses: train: 2.5577199459075928, validate: 2.5682127475738525\n",
      "Epoch 4/10. Iteration 3400/47167 Losses: train: 2.2473416328430176, validate: 2.572316884994507\n",
      "Epoch 4/10. Iteration 3500/47167 Losses: train: 2.376042366027832, validate: 2.57558274269104\n",
      "Epoch 4/10. Iteration 3600/47167 Losses: train: 2.253385543823242, validate: 2.57659649848938\n",
      "Epoch 4/10. Iteration 3700/47167 Losses: train: 2.383366107940674, validate: 2.5721898078918457\n",
      "Epoch 4/10. Iteration 3800/47167 Losses: train: 2.2991559505462646, validate: 2.580458402633667\n",
      "Epoch 4/10. Iteration 3900/47167 Losses: train: 2.7041525840759277, validate: 2.5814368724823\n",
      "Epoch 4/10. Iteration 4000/47167 Losses: train: 2.481898784637451, validate: 2.5801594257354736\n",
      "Epoch 4/10. Iteration 4100/47167 Losses: train: 2.399919033050537, validate: 2.570221424102783\n",
      "Epoch 4/10. Iteration 4200/47167 Losses: train: 2.2419075965881348, validate: 2.5680370330810547\n",
      "Epoch 4/10. Iteration 4300/47167 Losses: train: 2.322589635848999, validate: 2.561628580093384\n",
      "Epoch 4/10. Iteration 4400/47167 Losses: train: 2.313361406326294, validate: 2.5721352100372314\n",
      "Epoch 4/10. Iteration 4500/47167 Losses: train: 2.4724087715148926, validate: 2.56984543800354\n",
      "Epoch 4/10. Iteration 4600/47167 Losses: train: 2.47361421585083, validate: 2.568772792816162\n",
      "Epoch 4/10. Iteration 4700/47167 Losses: train: 2.305230140686035, validate: 2.5797178745269775\n",
      "Epoch 4/10. Iteration 4800/47167 Losses: train: 2.3813347816467285, validate: 2.5667624473571777\n",
      "Epoch 4/10. Iteration 4900/47167 Losses: train: 2.2510523796081543, validate: 2.561859607696533\n",
      "Epoch 4/10. Iteration 5000/47167 Losses: train: 2.4087095260620117, validate: 2.576263666152954\n",
      "Epoch 4/10. Iteration 5100/47167 Losses: train: 2.5302088260650635, validate: 2.5547056198120117\n",
      "Epoch 4/10. Iteration 5200/47167 Losses: train: 2.3924853801727295, validate: 2.566096305847168\n",
      "Epoch 4/10. Iteration 5300/47167 Losses: train: 2.4019596576690674, validate: 2.568634033203125\n",
      "Epoch 4/10. Iteration 5400/47167 Losses: train: 2.466921091079712, validate: 2.5654828548431396\n",
      "Epoch 4/10. Iteration 5500/47167 Losses: train: 2.4091756343841553, validate: 2.573413610458374\n",
      "Epoch 4/10. Iteration 5600/47167 Losses: train: 2.475965738296509, validate: 2.567667245864868\n",
      "Epoch 4/10. Iteration 5700/47167 Losses: train: 2.555852174758911, validate: 2.5654780864715576\n",
      "Epoch 4/10. Iteration 5800/47167 Losses: train: 2.252352714538574, validate: 2.5703604221343994\n",
      "Epoch 4/10. Iteration 5900/47167 Losses: train: 2.423173666000366, validate: 2.5665981769561768\n",
      "Epoch 4/10. Iteration 6000/47167 Losses: train: 2.4511804580688477, validate: 2.563446283340454\n",
      "Epoch 4/10. Iteration 6100/47167 Losses: train: 2.5404889583587646, validate: 2.5691308975219727\n",
      "Epoch 4/10. Iteration 6200/47167 Losses: train: 2.48946213722229, validate: 2.5565507411956787\n",
      "Epoch 4/10. Iteration 6300/47167 Losses: train: 2.288151264190674, validate: 2.5675127506256104\n",
      "Epoch 4/10. Iteration 6400/47167 Losses: train: 2.416677713394165, validate: 2.5712716579437256\n",
      "Epoch 4/10. Iteration 6500/47167 Losses: train: 2.45906138420105, validate: 2.5632119178771973\n",
      "Epoch 4/10. Iteration 6600/47167 Losses: train: 2.302649736404419, validate: 2.5662217140197754\n",
      "Epoch 4/10. Iteration 6700/47167 Losses: train: 2.5330519676208496, validate: 2.570352792739868\n",
      "Epoch 4/10. Iteration 6800/47167 Losses: train: 2.2736291885375977, validate: 2.571631908416748\n",
      "Epoch 4/10. Iteration 6900/47167 Losses: train: 2.4741854667663574, validate: 2.5629734992980957\n",
      "Epoch 4/10. Iteration 7000/47167 Losses: train: 2.3572161197662354, validate: 2.568284749984741\n",
      "Epoch 4/10. Iteration 7100/47167 Losses: train: 2.2721774578094482, validate: 2.5667059421539307\n",
      "Epoch 4/10. Iteration 7200/47167 Losses: train: 2.3291454315185547, validate: 2.568931818008423\n",
      "Epoch 4/10. Iteration 7300/47167 Losses: train: 2.500908613204956, validate: 2.573322296142578\n",
      "Epoch 4/10. Iteration 7400/47167 Losses: train: 2.252216339111328, validate: 2.566274404525757\n",
      "Epoch 4/10. Iteration 7500/47167 Losses: train: 2.3436524868011475, validate: 2.5605268478393555\n",
      "Epoch 4/10. Iteration 7600/47167 Losses: train: 2.5229978561401367, validate: 2.563004493713379\n",
      "Epoch 4/10. Iteration 7700/47167 Losses: train: 2.3515517711639404, validate: 2.564039468765259\n",
      "Epoch 4/10. Iteration 7800/47167 Losses: train: 2.5037753582000732, validate: 2.5503172874450684\n",
      "Epoch 4/10. Iteration 7900/47167 Losses: train: 2.441359758377075, validate: 2.565565824508667\n",
      "Epoch 4/10. Iteration 8000/47167 Losses: train: 2.361232280731201, validate: 2.577298879623413\n",
      "Epoch 4/10. Iteration 8100/47167 Losses: train: 2.3675642013549805, validate: 2.557328939437866\n",
      "Epoch 4/10. Iteration 8200/47167 Losses: train: 2.511413812637329, validate: 2.5632989406585693\n",
      "Epoch 4/10. Iteration 8300/47167 Losses: train: 2.3434224128723145, validate: 2.5616698265075684\n",
      "Epoch 4/10. Iteration 8400/47167 Losses: train: 2.4458959102630615, validate: 2.5659446716308594\n",
      "Epoch 4/10. Iteration 8500/47167 Losses: train: 2.4368672370910645, validate: 2.5565388202667236\n",
      "Epoch 4/10. Iteration 8600/47167 Losses: train: 2.286111831665039, validate: 2.568150281906128\n",
      "Epoch 4/10. Iteration 8700/47167 Losses: train: 2.4422595500946045, validate: 2.5671298503875732\n",
      "Epoch 4/10. Iteration 8800/47167 Losses: train: 2.4391400814056396, validate: 2.5633928775787354\n",
      "Epoch 4/10. Iteration 8900/47167 Losses: train: 2.469622850418091, validate: 2.566350221633911\n",
      "Epoch 4/10. Iteration 9000/47167 Losses: train: 2.3019800186157227, validate: 2.557572841644287\n",
      "Epoch 4/10. Iteration 9100/47167 Losses: train: 2.4106545448303223, validate: 2.5645103454589844\n",
      "Epoch 4/10. Iteration 9200/47167 Losses: train: 2.3435564041137695, validate: 2.562610387802124\n",
      "Epoch 4/10. Iteration 9300/47167 Losses: train: 2.2847466468811035, validate: 2.556049346923828\n",
      "Epoch 4/10. Iteration 9400/47167 Losses: train: 2.424437999725342, validate: 2.5682382583618164\n",
      "Epoch 4/10. Iteration 9500/47167 Losses: train: 2.449781656265259, validate: 2.557866096496582\n",
      "Epoch 4/10. Iteration 9600/47167 Losses: train: 2.4372470378875732, validate: 2.555332660675049\n",
      "Epoch 4/10. Iteration 9700/47167 Losses: train: 2.2949442863464355, validate: 2.560823678970337\n",
      "Epoch 4/10. Iteration 9800/47167 Losses: train: 2.449674129486084, validate: 2.5582399368286133\n",
      "Epoch 4/10. Iteration 9900/47167 Losses: train: 2.4215519428253174, validate: 2.559542655944824\n",
      "Epoch 4/10. Iteration 10000/47167 Losses: train: 2.379194736480713, validate: 2.559570789337158\n",
      "Epoch 4/10. Iteration 10100/47167 Losses: train: 2.251465082168579, validate: 2.5590884685516357\n",
      "Epoch 4/10. Iteration 10200/47167 Losses: train: 2.4083950519561768, validate: 2.5690643787384033\n",
      "Epoch 4/10. Iteration 10300/47167 Losses: train: 2.4586105346679688, validate: 2.5635504722595215\n",
      "Epoch 4/10. Iteration 10400/47167 Losses: train: 2.6441311836242676, validate: 2.5483720302581787\n",
      "Epoch 4/10. Iteration 10500/47167 Losses: train: 2.224473714828491, validate: 2.567275047302246\n",
      "Epoch 4/10. Iteration 10600/47167 Losses: train: 2.3517909049987793, validate: 2.5585825443267822\n",
      "Epoch 4/10. Iteration 10700/47167 Losses: train: 2.4042739868164062, validate: 2.574195384979248\n",
      "Epoch 4/10. Iteration 10800/47167 Losses: train: 2.479180335998535, validate: 2.5586259365081787\n",
      "Epoch 4/10. Iteration 10900/47167 Losses: train: 2.3969147205352783, validate: 2.5629255771636963\n",
      "Epoch 4/10. Iteration 11000/47167 Losses: train: 2.4823157787323, validate: 2.5705363750457764\n",
      "Epoch 4/10. Iteration 11100/47167 Losses: train: 2.547795534133911, validate: 2.556288719177246\n",
      "Epoch 4/10. Iteration 11200/47167 Losses: train: 2.424450397491455, validate: 2.5558478832244873\n",
      "Epoch 4/10. Iteration 11300/47167 Losses: train: 2.3468565940856934, validate: 2.5524868965148926\n",
      "Epoch 4/10. Iteration 11400/47167 Losses: train: 2.3915603160858154, validate: 2.5661418437957764\n",
      "Epoch 4/10. Iteration 11500/47167 Losses: train: 2.525053024291992, validate: 2.565234899520874\n",
      "Epoch 4/10. Iteration 11600/47167 Losses: train: 2.197425603866577, validate: 2.574751853942871\n",
      "Epoch 4/10. Iteration 11700/47167 Losses: train: 2.322953701019287, validate: 2.561631202697754\n",
      "Epoch 4/10. Iteration 11800/47167 Losses: train: 2.478672742843628, validate: 2.5631275177001953\n",
      "Epoch 4/10. Iteration 11900/47167 Losses: train: 2.3747198581695557, validate: 2.5678272247314453\n",
      "Epoch 4/10. Iteration 12000/47167 Losses: train: 2.23233962059021, validate: 2.5569987297058105\n",
      "Epoch 4/10. Iteration 12100/47167 Losses: train: 2.59110689163208, validate: 2.568634033203125\n",
      "Epoch 4/10. Iteration 12200/47167 Losses: train: 2.3580877780914307, validate: 2.5661962032318115\n",
      "Epoch 4/10. Iteration 12300/47167 Losses: train: 2.4292805194854736, validate: 2.557753086090088\n",
      "Epoch 4/10. Iteration 12400/47167 Losses: train: 2.37270450592041, validate: 2.5789496898651123\n",
      "Epoch 4/10. Iteration 12500/47167 Losses: train: 2.455111503601074, validate: 2.5584967136383057\n",
      "Epoch 4/10. Iteration 12600/47167 Losses: train: 2.520314931869507, validate: 2.569338798522949\n",
      "Epoch 4/10. Iteration 12700/47167 Losses: train: 2.4040558338165283, validate: 2.564879894256592\n",
      "Epoch 4/10. Iteration 12800/47167 Losses: train: 2.52028489112854, validate: 2.558985710144043\n",
      "Epoch 4/10. Iteration 12900/47167 Losses: train: 2.4486303329467773, validate: 2.5550060272216797\n",
      "Epoch 4/10. Iteration 13000/47167 Losses: train: 2.3509702682495117, validate: 2.564279794692993\n",
      "Epoch 4/10. Iteration 13100/47167 Losses: train: 2.32820200920105, validate: 2.556138753890991\n",
      "Epoch 4/10. Iteration 13200/47167 Losses: train: 2.2666256427764893, validate: 2.5635085105895996\n",
      "Epoch 4/10. Iteration 13300/47167 Losses: train: 2.3278117179870605, validate: 2.5613255500793457\n",
      "Epoch 4/10. Iteration 13400/47167 Losses: train: 2.4975099563598633, validate: 2.5645627975463867\n",
      "Epoch 4/10. Iteration 13500/47167 Losses: train: 2.452599048614502, validate: 2.5581963062286377\n",
      "Epoch 4/10. Iteration 13600/47167 Losses: train: 2.4314465522766113, validate: 2.5562849044799805\n",
      "Epoch 4/10. Iteration 13700/47167 Losses: train: 2.5321125984191895, validate: 2.569427251815796\n",
      "Epoch 4/10. Iteration 13800/47167 Losses: train: 2.5268571376800537, validate: 2.5705597400665283\n",
      "Epoch 4/10. Iteration 13900/47167 Losses: train: 2.3618826866149902, validate: 2.556641101837158\n",
      "Epoch 4/10. Iteration 14000/47167 Losses: train: 2.435163736343384, validate: 2.5624940395355225\n",
      "Epoch 4/10. Iteration 14100/47167 Losses: train: 2.441134214401245, validate: 2.562178134918213\n",
      "Epoch 4/10. Iteration 14200/47167 Losses: train: 2.3503384590148926, validate: 2.5640366077423096\n",
      "Epoch 4/10. Iteration 14300/47167 Losses: train: 2.3815484046936035, validate: 2.5612235069274902\n",
      "Epoch 4/10. Iteration 14400/47167 Losses: train: 2.4961087703704834, validate: 2.547213315963745\n",
      "Epoch 4/10. Iteration 14500/47167 Losses: train: 2.7129950523376465, validate: 2.554755210876465\n",
      "Epoch 4/10. Iteration 14600/47167 Losses: train: 2.3829994201660156, validate: 2.555777072906494\n",
      "Epoch 4/10. Iteration 14700/47167 Losses: train: 2.55647611618042, validate: 2.5617618560791016\n",
      "Epoch 4/10. Iteration 14800/47167 Losses: train: 2.4673116207122803, validate: 2.5649406909942627\n",
      "Epoch 4/10. Iteration 14900/47167 Losses: train: 2.34096097946167, validate: 2.5665862560272217\n",
      "Epoch 4/10. Iteration 15000/47167 Losses: train: 2.1829631328582764, validate: 2.5707242488861084\n",
      "Epoch 4/10. Iteration 15100/47167 Losses: train: 2.500230550765991, validate: 2.5635428428649902\n",
      "Epoch 4/10. Iteration 15200/47167 Losses: train: 2.3056435585021973, validate: 2.564359188079834\n",
      "Epoch 4/10. Iteration 15300/47167 Losses: train: 2.4006195068359375, validate: 2.5591554641723633\n",
      "Epoch 4/10. Iteration 15400/47167 Losses: train: 2.377103805541992, validate: 2.5542149543762207\n",
      "Epoch 4/10. Iteration 15500/47167 Losses: train: 2.222111463546753, validate: 2.5488176345825195\n",
      "Epoch 4/10. Iteration 15600/47167 Losses: train: 2.432919502258301, validate: 2.5536415576934814\n",
      "Epoch 4/10. Iteration 15700/47167 Losses: train: 2.4980194568634033, validate: 2.5541067123413086\n",
      "Epoch 4/10. Iteration 15800/47167 Losses: train: 2.3415675163269043, validate: 2.549246072769165\n",
      "Epoch 4/10. Iteration 15900/47167 Losses: train: 2.3955531120300293, validate: 2.562347888946533\n",
      "Epoch 4/10. Iteration 16000/47167 Losses: train: 2.3123767375946045, validate: 2.558851718902588\n",
      "Epoch 4/10. Iteration 16100/47167 Losses: train: 2.4736833572387695, validate: 2.5602989196777344\n",
      "Epoch 4/10. Iteration 16200/47167 Losses: train: 2.4005801677703857, validate: 2.555680274963379\n",
      "Epoch 4/10. Iteration 16300/47167 Losses: train: 2.451425790786743, validate: 2.5417449474334717\n",
      "Epoch 4/10. Iteration 16400/47167 Losses: train: 2.2273898124694824, validate: 2.5507452487945557\n",
      "Epoch 4/10. Iteration 16500/47167 Losses: train: 2.539336681365967, validate: 2.5477211475372314\n",
      "Epoch 4/10. Iteration 16600/47167 Losses: train: 2.506884813308716, validate: 2.562136173248291\n",
      "Epoch 4/10. Iteration 16700/47167 Losses: train: 2.291142702102661, validate: 2.558685541152954\n",
      "Epoch 4/10. Iteration 16800/47167 Losses: train: 2.4884204864501953, validate: 2.563985824584961\n",
      "Epoch 4/10. Iteration 16900/47167 Losses: train: 2.19173002243042, validate: 2.5506770610809326\n",
      "Epoch 4/10. Iteration 17000/47167 Losses: train: 2.3928332328796387, validate: 2.548107624053955\n",
      "Epoch 4/10. Iteration 17100/47167 Losses: train: 2.115882635116577, validate: 2.541860342025757\n",
      "Epoch 4/10. Iteration 17200/47167 Losses: train: 2.2847609519958496, validate: 2.544556140899658\n",
      "Epoch 4/10. Iteration 17300/47167 Losses: train: 2.5264687538146973, validate: 2.54854416847229\n",
      "Epoch 4/10. Iteration 17400/47167 Losses: train: 2.5046334266662598, validate: 2.545842409133911\n",
      "Epoch 4/10. Iteration 17500/47167 Losses: train: 2.451327323913574, validate: 2.5544886589050293\n",
      "Epoch 4/10. Iteration 17600/47167 Losses: train: 2.4658584594726562, validate: 2.548948049545288\n",
      "Epoch 4/10. Iteration 17700/47167 Losses: train: 2.313714027404785, validate: 2.5484073162078857\n",
      "Epoch 4/10. Iteration 17800/47167 Losses: train: 2.2893848419189453, validate: 2.5528345108032227\n",
      "Epoch 4/10. Iteration 17900/47167 Losses: train: 2.367767810821533, validate: 2.5460593700408936\n",
      "Epoch 4/10. Iteration 18000/47167 Losses: train: 2.3451900482177734, validate: 2.5420749187469482\n",
      "Epoch 4/10. Iteration 18100/47167 Losses: train: 2.378371477127075, validate: 2.5515120029449463\n",
      "Epoch 4/10. Iteration 18200/47167 Losses: train: 2.475314140319824, validate: 2.5568253993988037\n",
      "Epoch 4/10. Iteration 18300/47167 Losses: train: 2.3842618465423584, validate: 2.5490646362304688\n",
      "Epoch 4/10. Iteration 18400/47167 Losses: train: 2.307372808456421, validate: 2.549241781234741\n",
      "Epoch 4/10. Iteration 18500/47167 Losses: train: 2.3989086151123047, validate: 2.5483179092407227\n",
      "Epoch 4/10. Iteration 18600/47167 Losses: train: 2.431173324584961, validate: 2.550096273422241\n",
      "Epoch 4/10. Iteration 18700/47167 Losses: train: 2.251603603363037, validate: 2.5473437309265137\n",
      "Epoch 4/10. Iteration 18800/47167 Losses: train: 2.6254844665527344, validate: 2.5491907596588135\n",
      "Epoch 4/10. Iteration 18900/47167 Losses: train: 2.5332369804382324, validate: 2.5580854415893555\n",
      "Epoch 4/10. Iteration 19000/47167 Losses: train: 2.7181732654571533, validate: 2.53903865814209\n",
      "Epoch 4/10. Iteration 19100/47167 Losses: train: 2.3921756744384766, validate: 2.5459136962890625\n",
      "Epoch 4/10. Iteration 19200/47167 Losses: train: 2.240851402282715, validate: 2.5485904216766357\n",
      "Epoch 4/10. Iteration 19300/47167 Losses: train: 2.3832027912139893, validate: 2.5507352352142334\n",
      "Epoch 4/10. Iteration 19400/47167 Losses: train: 2.235543727874756, validate: 2.555513858795166\n",
      "Epoch 4/10. Iteration 19500/47167 Losses: train: 2.4485301971435547, validate: 2.5495283603668213\n",
      "Epoch 4/10. Iteration 19600/47167 Losses: train: 2.350766181945801, validate: 2.550507068634033\n",
      "Epoch 4/10. Iteration 19700/47167 Losses: train: 2.3058223724365234, validate: 2.5583975315093994\n",
      "Epoch 4/10. Iteration 19800/47167 Losses: train: 2.2985358238220215, validate: 2.5565226078033447\n",
      "Epoch 4/10. Iteration 19900/47167 Losses: train: 2.3573641777038574, validate: 2.5546770095825195\n",
      "Epoch 4/10. Iteration 20000/47167 Losses: train: 2.4005227088928223, validate: 2.55145001411438\n",
      "Epoch 4/10. Iteration 20100/47167 Losses: train: 2.4388487339019775, validate: 2.5517194271087646\n",
      "Epoch 4/10. Iteration 20200/47167 Losses: train: 2.215942859649658, validate: 2.5529723167419434\n",
      "Epoch 4/10. Iteration 20300/47167 Losses: train: 2.36979079246521, validate: 2.5429961681365967\n",
      "Epoch 4/10. Iteration 20400/47167 Losses: train: 2.532550096511841, validate: 2.5393354892730713\n",
      "Epoch 4/10. Iteration 20500/47167 Losses: train: 2.43520450592041, validate: 2.5455784797668457\n",
      "Epoch 4/10. Iteration 20600/47167 Losses: train: 2.3786048889160156, validate: 2.542875289916992\n",
      "Epoch 4/10. Iteration 20700/47167 Losses: train: 2.3854238986968994, validate: 2.5374746322631836\n",
      "Epoch 4/10. Iteration 20800/47167 Losses: train: 2.52701735496521, validate: 2.5499980449676514\n",
      "Epoch 4/10. Iteration 20900/47167 Losses: train: 2.3555562496185303, validate: 2.5413191318511963\n",
      "Epoch 4/10. Iteration 21000/47167 Losses: train: 2.4593698978424072, validate: 2.5611469745635986\n",
      "Epoch 4/10. Iteration 21100/47167 Losses: train: 2.596956491470337, validate: 2.5448930263519287\n",
      "Epoch 4/10. Iteration 21200/47167 Losses: train: 2.416293144226074, validate: 2.538325309753418\n",
      "Epoch 4/10. Iteration 21300/47167 Losses: train: 2.3913919925689697, validate: 2.545269012451172\n",
      "Epoch 4/10. Iteration 21400/47167 Losses: train: 2.248044013977051, validate: 2.5370595455169678\n",
      "Epoch 4/10. Iteration 21500/47167 Losses: train: 2.3453869819641113, validate: 2.5379300117492676\n",
      "Epoch 4/10. Iteration 21600/47167 Losses: train: 2.483731746673584, validate: 2.5405266284942627\n",
      "Epoch 4/10. Iteration 21700/47167 Losses: train: 2.304908514022827, validate: 2.542757511138916\n",
      "Epoch 4/10. Iteration 21800/47167 Losses: train: 2.433495283126831, validate: 2.533844470977783\n",
      "Epoch 4/10. Iteration 21900/47167 Losses: train: 2.642382860183716, validate: 2.544996500015259\n",
      "Epoch 4/10. Iteration 22000/47167 Losses: train: 2.327848196029663, validate: 2.54032301902771\n",
      "Epoch 4/10. Iteration 22100/47167 Losses: train: 2.487957239151001, validate: 2.541978120803833\n",
      "Epoch 4/10. Iteration 22200/47167 Losses: train: 2.303334951400757, validate: 2.5489580631256104\n",
      "Epoch 4/10. Iteration 22300/47167 Losses: train: 2.4779913425445557, validate: 2.5394959449768066\n",
      "Epoch 4/10. Iteration 22400/47167 Losses: train: 2.3907089233398438, validate: 2.542710781097412\n",
      "Epoch 4/10. Iteration 22500/47167 Losses: train: 2.1818735599517822, validate: 2.548352003097534\n",
      "Epoch 4/10. Iteration 22600/47167 Losses: train: 2.3979830741882324, validate: 2.5439906120300293\n",
      "Epoch 4/10. Iteration 22700/47167 Losses: train: 2.335257053375244, validate: 2.5415501594543457\n",
      "Epoch 4/10. Iteration 22800/47167 Losses: train: 2.460820436477661, validate: 2.538893461227417\n",
      "Epoch 4/10. Iteration 22900/47167 Losses: train: 2.3841195106506348, validate: 2.5445950031280518\n",
      "Epoch 4/10. Iteration 23000/47167 Losses: train: 2.3235011100769043, validate: 2.53961181640625\n",
      "Epoch 4/10. Iteration 23100/47167 Losses: train: 2.460177421569824, validate: 2.537936210632324\n",
      "Epoch 4/10. Iteration 23200/47167 Losses: train: 2.397758722305298, validate: 2.5397226810455322\n",
      "Epoch 4/10. Iteration 23300/47167 Losses: train: 2.4010889530181885, validate: 2.5344812870025635\n",
      "Epoch 4/10. Iteration 23400/47167 Losses: train: 2.3651726245880127, validate: 2.539400100708008\n",
      "Epoch 4/10. Iteration 23500/47167 Losses: train: 2.4868907928466797, validate: 2.543347120285034\n",
      "Epoch 4/10. Iteration 23600/47167 Losses: train: 2.4826555252075195, validate: 2.5460286140441895\n",
      "Epoch 4/10. Iteration 23700/47167 Losses: train: 2.31384539604187, validate: 2.549577474594116\n",
      "Epoch 4/10. Iteration 23800/47167 Losses: train: 2.4981350898742676, validate: 2.5439517498016357\n",
      "Epoch 4/10. Iteration 23900/47167 Losses: train: 2.098121404647827, validate: 2.546132802963257\n",
      "Epoch 4/10. Iteration 24000/47167 Losses: train: 2.3885929584503174, validate: 2.545736074447632\n",
      "Epoch 4/10. Iteration 24100/47167 Losses: train: 2.311051607131958, validate: 2.5474281311035156\n",
      "Epoch 4/10. Iteration 24200/47167 Losses: train: 2.2941641807556152, validate: 2.5490527153015137\n",
      "Epoch 4/10. Iteration 24300/47167 Losses: train: 2.3749775886535645, validate: 2.5440175533294678\n",
      "Epoch 4/10. Iteration 24400/47167 Losses: train: 2.294295072555542, validate: 2.5481138229370117\n",
      "Epoch 4/10. Iteration 24500/47167 Losses: train: 2.395455837249756, validate: 2.560103416442871\n",
      "Epoch 4/10. Iteration 24600/47167 Losses: train: 2.340351104736328, validate: 2.542126417160034\n",
      "Epoch 4/10. Iteration 24700/47167 Losses: train: 2.4759390354156494, validate: 2.536900758743286\n",
      "Epoch 4/10. Iteration 24800/47167 Losses: train: 2.370651960372925, validate: 2.5440852642059326\n",
      "Epoch 4/10. Iteration 24900/47167 Losses: train: 2.3517181873321533, validate: 2.5389153957366943\n",
      "Epoch 4/10. Iteration 25000/47167 Losses: train: 2.2978932857513428, validate: 2.528320550918579\n",
      "Epoch 4/10. Iteration 25100/47167 Losses: train: 2.233873128890991, validate: 2.545098304748535\n",
      "Epoch 4/10. Iteration 25200/47167 Losses: train: 2.2435784339904785, validate: 2.520528793334961\n",
      "Epoch 4/10. Iteration 25300/47167 Losses: train: 2.5505824089050293, validate: 2.5299081802368164\n",
      "Epoch 4/10. Iteration 25400/47167 Losses: train: 2.3365538120269775, validate: 2.5272209644317627\n",
      "Epoch 4/10. Iteration 25500/47167 Losses: train: 2.6142640113830566, validate: 2.536630392074585\n",
      "Epoch 4/10. Iteration 25600/47167 Losses: train: 2.4866974353790283, validate: 2.5403246879577637\n",
      "Epoch 4/10. Iteration 25700/47167 Losses: train: 2.2999675273895264, validate: 2.5345895290374756\n",
      "Epoch 4/10. Iteration 25800/47167 Losses: train: 2.527613639831543, validate: 2.530151844024658\n",
      "Epoch 4/10. Iteration 25900/47167 Losses: train: 2.271453857421875, validate: 2.52681040763855\n",
      "Epoch 4/10. Iteration 26000/47167 Losses: train: 2.2821338176727295, validate: 2.5375208854675293\n",
      "Epoch 4/10. Iteration 26100/47167 Losses: train: 2.323931932449341, validate: 2.52603816986084\n",
      "Epoch 4/10. Iteration 26200/47167 Losses: train: 2.457378625869751, validate: 2.5283844470977783\n",
      "Epoch 4/10. Iteration 26300/47167 Losses: train: 2.574765205383301, validate: 2.526496171951294\n",
      "Epoch 4/10. Iteration 26400/47167 Losses: train: 2.5582003593444824, validate: 2.5411605834960938\n",
      "Epoch 4/10. Iteration 26500/47167 Losses: train: 2.522054433822632, validate: 2.534337282180786\n",
      "Epoch 4/10. Iteration 26600/47167 Losses: train: 2.6244471073150635, validate: 2.5365676879882812\n",
      "Epoch 4/10. Iteration 26700/47167 Losses: train: 2.2520077228546143, validate: 2.5259478092193604\n",
      "Epoch 4/10. Iteration 26800/47167 Losses: train: 2.409529209136963, validate: 2.5360002517700195\n",
      "Epoch 4/10. Iteration 26900/47167 Losses: train: 2.710681915283203, validate: 2.536461591720581\n",
      "Epoch 4/10. Iteration 27000/47167 Losses: train: 2.3659451007843018, validate: 2.5384557247161865\n",
      "Epoch 4/10. Iteration 27100/47167 Losses: train: 2.4508767127990723, validate: 2.531379222869873\n",
      "Epoch 4/10. Iteration 27200/47167 Losses: train: 2.4527792930603027, validate: 2.534052848815918\n",
      "Epoch 4/10. Iteration 27300/47167 Losses: train: 2.4234344959259033, validate: 2.537677526473999\n",
      "Epoch 4/10. Iteration 27400/47167 Losses: train: 2.262052297592163, validate: 2.538813591003418\n",
      "Epoch 4/10. Iteration 27500/47167 Losses: train: 2.235914468765259, validate: 2.5359838008880615\n",
      "Epoch 4/10. Iteration 27600/47167 Losses: train: 2.2230842113494873, validate: 2.542665719985962\n",
      "Epoch 4/10. Iteration 27700/47167 Losses: train: 2.471308469772339, validate: 2.539182662963867\n",
      "Epoch 4/10. Iteration 27800/47167 Losses: train: 2.5824074745178223, validate: 2.5358924865722656\n",
      "Epoch 4/10. Iteration 27900/47167 Losses: train: 2.3521952629089355, validate: 2.54844069480896\n",
      "Epoch 4/10. Iteration 28000/47167 Losses: train: 2.3664581775665283, validate: 2.529900550842285\n",
      "Epoch 4/10. Iteration 28100/47167 Losses: train: 2.289501667022705, validate: 2.535247564315796\n",
      "Epoch 4/10. Iteration 28200/47167 Losses: train: 2.4313201904296875, validate: 2.5345826148986816\n",
      "Epoch 4/10. Iteration 28300/47167 Losses: train: 2.4386394023895264, validate: 2.5381369590759277\n",
      "Epoch 4/10. Iteration 28400/47167 Losses: train: 2.3686583042144775, validate: 2.5293779373168945\n",
      "Epoch 4/10. Iteration 28500/47167 Losses: train: 2.1234912872314453, validate: 2.5380373001098633\n",
      "Epoch 4/10. Iteration 28600/47167 Losses: train: 2.233445167541504, validate: 2.533424139022827\n",
      "Epoch 4/10. Iteration 28700/47167 Losses: train: 2.5263609886169434, validate: 2.5359888076782227\n",
      "Epoch 4/10. Iteration 28800/47167 Losses: train: 2.396836757659912, validate: 2.537051200866699\n",
      "Epoch 4/10. Iteration 28900/47167 Losses: train: 2.4341683387756348, validate: 2.5326364040374756\n",
      "Epoch 4/10. Iteration 29000/47167 Losses: train: 2.5050787925720215, validate: 2.529902696609497\n",
      "Epoch 4/10. Iteration 29100/47167 Losses: train: 2.3825559616088867, validate: 2.5269863605499268\n",
      "Epoch 4/10. Iteration 29200/47167 Losses: train: 2.2060062885284424, validate: 2.523702621459961\n",
      "Epoch 4/10. Iteration 29300/47167 Losses: train: 2.409741163253784, validate: 2.523944139480591\n",
      "Epoch 4/10. Iteration 29400/47167 Losses: train: 2.372159481048584, validate: 2.5344491004943848\n",
      "Epoch 4/10. Iteration 29500/47167 Losses: train: 2.478187084197998, validate: 2.5311379432678223\n",
      "Epoch 4/10. Iteration 29600/47167 Losses: train: 2.5679404735565186, validate: 2.5331451892852783\n",
      "Epoch 4/10. Iteration 29700/47167 Losses: train: 2.106006622314453, validate: 2.531512498855591\n",
      "Epoch 4/10. Iteration 29800/47167 Losses: train: 2.4720988273620605, validate: 2.5223333835601807\n",
      "Epoch 4/10. Iteration 29900/47167 Losses: train: 2.386167287826538, validate: 2.5280637741088867\n",
      "Epoch 4/10. Iteration 30000/47167 Losses: train: 2.426614761352539, validate: 2.534661054611206\n",
      "Epoch 4/10. Iteration 30100/47167 Losses: train: 2.4154083728790283, validate: 2.5253217220306396\n",
      "Epoch 4/10. Iteration 30200/47167 Losses: train: 2.4402098655700684, validate: 2.5257062911987305\n",
      "Epoch 4/10. Iteration 30300/47167 Losses: train: 2.614637851715088, validate: 2.527076482772827\n",
      "Epoch 4/10. Iteration 30400/47167 Losses: train: 2.388485908508301, validate: 2.5348639488220215\n",
      "Epoch 4/10. Iteration 30500/47167 Losses: train: 2.392937660217285, validate: 2.525075912475586\n",
      "Epoch 4/10. Iteration 30600/47167 Losses: train: 2.4113802909851074, validate: 2.521456718444824\n",
      "Epoch 4/10. Iteration 30700/47167 Losses: train: 2.5706400871276855, validate: 2.5242040157318115\n",
      "Epoch 4/10. Iteration 30800/47167 Losses: train: 2.336888551712036, validate: 2.5241076946258545\n",
      "Epoch 4/10. Iteration 30900/47167 Losses: train: 2.4779176712036133, validate: 2.5339865684509277\n",
      "Epoch 4/10. Iteration 31000/47167 Losses: train: 2.46305775642395, validate: 2.526312828063965\n",
      "Epoch 4/10. Iteration 31100/47167 Losses: train: 2.3755671977996826, validate: 2.5082645416259766\n",
      "Epoch 4/10. Iteration 31200/47167 Losses: train: 2.567709445953369, validate: 2.5223753452301025\n",
      "Epoch 4/10. Iteration 31300/47167 Losses: train: 2.3538999557495117, validate: 2.529052734375\n",
      "Epoch 4/10. Iteration 31400/47167 Losses: train: 2.411498546600342, validate: 2.5358450412750244\n",
      "Epoch 4/10. Iteration 31500/47167 Losses: train: 2.415971517562866, validate: 2.540484666824341\n",
      "Epoch 4/10. Iteration 31600/47167 Losses: train: 2.256484270095825, validate: 2.53645920753479\n",
      "Epoch 4/10. Iteration 31700/47167 Losses: train: 2.4211857318878174, validate: 2.5353550910949707\n",
      "Epoch 4/10. Iteration 31800/47167 Losses: train: 2.3840630054473877, validate: 2.53360915184021\n",
      "Epoch 4/10. Iteration 31900/47167 Losses: train: 2.3195481300354004, validate: 2.5372543334960938\n",
      "Epoch 4/10. Iteration 32000/47167 Losses: train: 2.4763035774230957, validate: 2.529096841812134\n",
      "Epoch 4/10. Iteration 32100/47167 Losses: train: 2.55000638961792, validate: 2.524446725845337\n",
      "Epoch 4/10. Iteration 32200/47167 Losses: train: 2.4291820526123047, validate: 2.528663396835327\n",
      "Epoch 4/10. Iteration 32300/47167 Losses: train: 2.4129271507263184, validate: 2.524359941482544\n",
      "Epoch 4/10. Iteration 32400/47167 Losses: train: 2.434903860092163, validate: 2.5209901332855225\n",
      "Epoch 4/10. Iteration 32500/47167 Losses: train: 2.548471450805664, validate: 2.5303046703338623\n",
      "Epoch 4/10. Iteration 32600/47167 Losses: train: 2.4857635498046875, validate: 2.5245630741119385\n",
      "Epoch 4/10. Iteration 32700/47167 Losses: train: 2.4170608520507812, validate: 2.528663158416748\n",
      "Epoch 4/10. Iteration 32800/47167 Losses: train: 2.3689520359039307, validate: 2.533559560775757\n",
      "Epoch 4/10. Iteration 32900/47167 Losses: train: 2.40483021736145, validate: 2.527773141860962\n",
      "Epoch 4/10. Iteration 33000/47167 Losses: train: 2.4187734127044678, validate: 2.5261173248291016\n",
      "Epoch 4/10. Iteration 33100/47167 Losses: train: 2.388291358947754, validate: 2.5275330543518066\n",
      "Epoch 4/10. Iteration 33200/47167 Losses: train: 2.4011504650115967, validate: 2.5328996181488037\n",
      "Epoch 4/10. Iteration 33300/47167 Losses: train: 2.4165408611297607, validate: 2.531525135040283\n",
      "Epoch 4/10. Iteration 33400/47167 Losses: train: 2.3523757457733154, validate: 2.5222952365875244\n",
      "Epoch 4/10. Iteration 33500/47167 Losses: train: 2.3115782737731934, validate: 2.523689031600952\n",
      "Epoch 4/10. Iteration 33600/47167 Losses: train: 2.2984964847564697, validate: 2.530212879180908\n",
      "Epoch 4/10. Iteration 33700/47167 Losses: train: 2.3514657020568848, validate: 2.532982349395752\n",
      "Epoch 4/10. Iteration 33800/47167 Losses: train: 2.3751518726348877, validate: 2.52349591255188\n",
      "Epoch 4/10. Iteration 33900/47167 Losses: train: 2.349499464035034, validate: 2.5234014987945557\n",
      "Epoch 4/10. Iteration 34000/47167 Losses: train: 2.438774585723877, validate: 2.5221798419952393\n",
      "Epoch 4/10. Iteration 34100/47167 Losses: train: 2.331756353378296, validate: 2.533062219619751\n",
      "Epoch 4/10. Iteration 34200/47167 Losses: train: 2.460514545440674, validate: 2.514404296875\n",
      "Epoch 4/10. Iteration 34300/47167 Losses: train: 2.201343297958374, validate: 2.524245500564575\n",
      "Epoch 4/10. Iteration 34400/47167 Losses: train: 2.51518177986145, validate: 2.512380838394165\n",
      "Epoch 4/10. Iteration 34500/47167 Losses: train: 2.464395046234131, validate: 2.5225327014923096\n",
      "Epoch 4/10. Iteration 34600/47167 Losses: train: 2.5269887447357178, validate: 2.526951789855957\n",
      "Epoch 4/10. Iteration 34700/47167 Losses: train: 2.281416416168213, validate: 2.525681734085083\n",
      "Epoch 4/10. Iteration 34800/47167 Losses: train: 2.3513312339782715, validate: 2.53015398979187\n",
      "Epoch 4/10. Iteration 34900/47167 Losses: train: 2.433992385864258, validate: 2.5237178802490234\n",
      "Epoch 4/10. Iteration 35000/47167 Losses: train: 2.439223051071167, validate: 2.5301976203918457\n",
      "Epoch 4/10. Iteration 35100/47167 Losses: train: 2.2376809120178223, validate: 2.5294735431671143\n",
      "Epoch 4/10. Iteration 35200/47167 Losses: train: 2.408036470413208, validate: 2.5304698944091797\n",
      "Epoch 4/10. Iteration 35300/47167 Losses: train: 2.4110593795776367, validate: 2.523338794708252\n",
      "Epoch 4/10. Iteration 35400/47167 Losses: train: 2.2493014335632324, validate: 2.5243337154388428\n",
      "Epoch 4/10. Iteration 35500/47167 Losses: train: 2.179722785949707, validate: 2.5236740112304688\n",
      "Epoch 4/10. Iteration 35600/47167 Losses: train: 2.43118953704834, validate: 2.52313494682312\n",
      "Epoch 4/10. Iteration 35700/47167 Losses: train: 2.4280331134796143, validate: 2.5136702060699463\n",
      "Epoch 4/10. Iteration 35800/47167 Losses: train: 2.414641857147217, validate: 2.522005319595337\n",
      "Epoch 4/10. Iteration 35900/47167 Losses: train: 2.4616832733154297, validate: 2.524090528488159\n",
      "Epoch 4/10. Iteration 36000/47167 Losses: train: 2.2313311100006104, validate: 2.5190932750701904\n",
      "Epoch 4/10. Iteration 36100/47167 Losses: train: 2.4916839599609375, validate: 2.518298387527466\n",
      "Epoch 4/10. Iteration 36200/47167 Losses: train: 2.3125762939453125, validate: 2.519713878631592\n",
      "Epoch 4/10. Iteration 36300/47167 Losses: train: 2.311715602874756, validate: 2.520305633544922\n",
      "Epoch 4/10. Iteration 36400/47167 Losses: train: 2.4821720123291016, validate: 2.5218701362609863\n",
      "Epoch 4/10. Iteration 36500/47167 Losses: train: 2.3964498043060303, validate: 2.5249032974243164\n",
      "Epoch 4/10. Iteration 36600/47167 Losses: train: 2.44014573097229, validate: 2.5065128803253174\n",
      "Epoch 4/10. Iteration 36700/47167 Losses: train: 2.3912909030914307, validate: 2.530287504196167\n",
      "Epoch 4/10. Iteration 36800/47167 Losses: train: 2.4526970386505127, validate: 2.5288078784942627\n",
      "Epoch 4/10. Iteration 36900/47167 Losses: train: 2.2615411281585693, validate: 2.5192344188690186\n",
      "Epoch 4/10. Iteration 37000/47167 Losses: train: 2.532996892929077, validate: 2.5170302391052246\n",
      "Epoch 4/10. Iteration 37100/47167 Losses: train: 2.4017012119293213, validate: 2.515761137008667\n",
      "Epoch 4/10. Iteration 37200/47167 Losses: train: 2.4337594509124756, validate: 2.500516414642334\n",
      "Epoch 4/10. Iteration 37300/47167 Losses: train: 2.4391863346099854, validate: 2.52219557762146\n",
      "Epoch 4/10. Iteration 37400/47167 Losses: train: 2.429508686065674, validate: 2.5201992988586426\n",
      "Epoch 4/10. Iteration 37500/47167 Losses: train: 2.2954044342041016, validate: 2.5141255855560303\n",
      "Epoch 4/10. Iteration 37600/47167 Losses: train: 2.3604576587677, validate: 2.523892641067505\n",
      "Epoch 4/10. Iteration 37700/47167 Losses: train: 2.271615743637085, validate: 2.5309460163116455\n",
      "Epoch 4/10. Iteration 37800/47167 Losses: train: 2.3396689891815186, validate: 2.5207040309906006\n",
      "Epoch 4/10. Iteration 37900/47167 Losses: train: 2.163614273071289, validate: 2.516798496246338\n",
      "Epoch 4/10. Iteration 38000/47167 Losses: train: 2.318901300430298, validate: 2.51265025138855\n",
      "Epoch 4/10. Iteration 38100/47167 Losses: train: 2.4554882049560547, validate: 2.5093581676483154\n",
      "Epoch 4/10. Iteration 38200/47167 Losses: train: 2.196350574493408, validate: 2.516449213027954\n",
      "Epoch 4/10. Iteration 38300/47167 Losses: train: 2.2218427658081055, validate: 2.517824172973633\n",
      "Epoch 4/10. Iteration 38400/47167 Losses: train: 2.448176145553589, validate: 2.5240416526794434\n",
      "Epoch 4/10. Iteration 38500/47167 Losses: train: 2.306627035140991, validate: 2.5153353214263916\n",
      "Epoch 4/10. Iteration 38600/47167 Losses: train: 2.5188333988189697, validate: 2.510540008544922\n",
      "Epoch 4/10. Iteration 38700/47167 Losses: train: 2.4228129386901855, validate: 2.5251290798187256\n",
      "Epoch 4/10. Iteration 38800/47167 Losses: train: 2.3342676162719727, validate: 2.5245909690856934\n",
      "Epoch 4/10. Iteration 38900/47167 Losses: train: 2.4679758548736572, validate: 2.5292553901672363\n",
      "Epoch 4/10. Iteration 39000/47167 Losses: train: 2.5165326595306396, validate: 2.5298783779144287\n",
      "Epoch 4/10. Iteration 39100/47167 Losses: train: 2.5905494689941406, validate: 2.528324842453003\n",
      "Epoch 4/10. Iteration 39200/47167 Losses: train: 2.286839246749878, validate: 2.5258476734161377\n",
      "Epoch 4/10. Iteration 39300/47167 Losses: train: 2.4939637184143066, validate: 2.522338390350342\n",
      "Epoch 4/10. Iteration 39400/47167 Losses: train: 2.5053136348724365, validate: 2.5121641159057617\n",
      "Epoch 4/10. Iteration 39500/47167 Losses: train: 2.3875725269317627, validate: 2.5199215412139893\n",
      "Epoch 4/10. Iteration 39600/47167 Losses: train: 2.4252936840057373, validate: 2.525397300720215\n",
      "Epoch 4/10. Iteration 39700/47167 Losses: train: 2.3672385215759277, validate: 2.51931095123291\n",
      "Epoch 4/10. Iteration 39800/47167 Losses: train: 2.3435451984405518, validate: 2.5213606357574463\n",
      "Epoch 4/10. Iteration 39900/47167 Losses: train: 2.434213876724243, validate: 2.5261144638061523\n",
      "Epoch 4/10. Iteration 40000/47167 Losses: train: 2.4517412185668945, validate: 2.5185129642486572\n",
      "Epoch 4/10. Iteration 40100/47167 Losses: train: 2.4255740642547607, validate: 2.518433094024658\n",
      "Epoch 4/10. Iteration 40200/47167 Losses: train: 2.325371742248535, validate: 2.521514892578125\n",
      "Epoch 4/10. Iteration 40300/47167 Losses: train: 2.2571935653686523, validate: 2.523892641067505\n",
      "Epoch 4/10. Iteration 40400/47167 Losses: train: 2.2789418697357178, validate: 2.5169246196746826\n",
      "Epoch 4/10. Iteration 40500/47167 Losses: train: 2.280738592147827, validate: 2.5214664936065674\n",
      "Epoch 4/10. Iteration 40600/47167 Losses: train: 2.7449147701263428, validate: 2.5215067863464355\n",
      "Epoch 4/10. Iteration 40700/47167 Losses: train: 2.502812623977661, validate: 2.5138611793518066\n",
      "Epoch 4/10. Iteration 40800/47167 Losses: train: 2.220319986343384, validate: 2.5188159942626953\n",
      "Epoch 4/10. Iteration 40900/47167 Losses: train: 2.2531259059906006, validate: 2.519907236099243\n",
      "Epoch 4/10. Iteration 41000/47167 Losses: train: 2.31048321723938, validate: 2.526742935180664\n",
      "Epoch 4/10. Iteration 41100/47167 Losses: train: 2.159191846847534, validate: 2.5201098918914795\n",
      "Epoch 4/10. Iteration 41200/47167 Losses: train: 2.4416701793670654, validate: 2.5243349075317383\n",
      "Epoch 4/10. Iteration 41300/47167 Losses: train: 2.547593832015991, validate: 2.522376537322998\n",
      "Epoch 4/10. Iteration 41400/47167 Losses: train: 2.4324681758880615, validate: 2.5248045921325684\n",
      "Epoch 4/10. Iteration 41500/47167 Losses: train: 2.460217237472534, validate: 2.5152766704559326\n",
      "Epoch 4/10. Iteration 41600/47167 Losses: train: 2.465991973876953, validate: 2.520862102508545\n",
      "Epoch 4/10. Iteration 41700/47167 Losses: train: 2.3928136825561523, validate: 2.518021821975708\n",
      "Epoch 4/10. Iteration 41800/47167 Losses: train: 2.3935232162475586, validate: 2.529411792755127\n",
      "Epoch 4/10. Iteration 41900/47167 Losses: train: 2.63118314743042, validate: 2.521233558654785\n",
      "Epoch 4/10. Iteration 42000/47167 Losses: train: 2.4816102981567383, validate: 2.5189573764801025\n",
      "Epoch 4/10. Iteration 42100/47167 Losses: train: 2.2467191219329834, validate: 2.51224684715271\n",
      "Epoch 4/10. Iteration 42200/47167 Losses: train: 2.3804686069488525, validate: 2.5181024074554443\n",
      "Epoch 4/10. Iteration 42300/47167 Losses: train: 2.298001766204834, validate: 2.519972562789917\n",
      "Epoch 4/10. Iteration 42400/47167 Losses: train: 2.186722993850708, validate: 2.5199522972106934\n",
      "Epoch 4/10. Iteration 42500/47167 Losses: train: 2.3963654041290283, validate: 2.522078514099121\n",
      "Epoch 4/10. Iteration 42600/47167 Losses: train: 2.514498472213745, validate: 2.5316975116729736\n",
      "Epoch 4/10. Iteration 42700/47167 Losses: train: 2.5032479763031006, validate: 2.519089937210083\n",
      "Epoch 4/10. Iteration 42800/47167 Losses: train: 2.3793745040893555, validate: 2.5242059230804443\n",
      "Epoch 4/10. Iteration 42900/47167 Losses: train: 2.4680123329162598, validate: 2.5316929817199707\n",
      "Epoch 4/10. Iteration 43000/47167 Losses: train: 2.570640802383423, validate: 2.522258758544922\n",
      "Epoch 4/10. Iteration 43100/47167 Losses: train: 2.417163610458374, validate: 2.5245237350463867\n",
      "Epoch 4/10. Iteration 43200/47167 Losses: train: 2.270214319229126, validate: 2.51947283744812\n",
      "Epoch 4/10. Iteration 43300/47167 Losses: train: 2.489193916320801, validate: 2.5083606243133545\n",
      "Epoch 4/10. Iteration 43400/47167 Losses: train: 2.3544976711273193, validate: 2.5112907886505127\n",
      "Epoch 4/10. Iteration 43500/47167 Losses: train: 2.614318609237671, validate: 2.5306694507598877\n",
      "Epoch 4/10. Iteration 43600/47167 Losses: train: 2.5083303451538086, validate: 2.5219123363494873\n",
      "Epoch 4/10. Iteration 43700/47167 Losses: train: 2.427074670791626, validate: 2.512970447540283\n",
      "Epoch 4/10. Iteration 43800/47167 Losses: train: 2.504301071166992, validate: 2.512188673019409\n",
      "Epoch 4/10. Iteration 43900/47167 Losses: train: 2.5429115295410156, validate: 2.5090975761413574\n",
      "Epoch 4/10. Iteration 44000/47167 Losses: train: 2.4871950149536133, validate: 2.5107192993164062\n",
      "Epoch 4/10. Iteration 44100/47167 Losses: train: 2.218209743499756, validate: 2.5114614963531494\n",
      "Epoch 4/10. Iteration 44200/47167 Losses: train: 2.3948798179626465, validate: 2.514317035675049\n",
      "Epoch 4/10. Iteration 44300/47167 Losses: train: 2.4755475521087646, validate: 2.5110092163085938\n",
      "Epoch 4/10. Iteration 44400/47167 Losses: train: 2.6020538806915283, validate: 2.5147199630737305\n",
      "Epoch 4/10. Iteration 44500/47167 Losses: train: 2.4052419662475586, validate: 2.5020570755004883\n",
      "Epoch 4/10. Iteration 44600/47167 Losses: train: 2.380643606185913, validate: 2.507040023803711\n",
      "Epoch 4/10. Iteration 44700/47167 Losses: train: 2.3245584964752197, validate: 2.501920700073242\n",
      "Epoch 4/10. Iteration 44800/47167 Losses: train: 2.4600603580474854, validate: 2.5204718112945557\n",
      "Epoch 4/10. Iteration 44900/47167 Losses: train: 2.380122423171997, validate: 2.512953996658325\n",
      "Epoch 4/10. Iteration 45000/47167 Losses: train: 2.329360246658325, validate: 2.5091397762298584\n",
      "Epoch 4/10. Iteration 45100/47167 Losses: train: 2.341357707977295, validate: 2.5185546875\n",
      "Epoch 4/10. Iteration 45200/47167 Losses: train: 2.533517599105835, validate: 2.5087099075317383\n",
      "Epoch 4/10. Iteration 45300/47167 Losses: train: 2.4505393505096436, validate: 2.5198514461517334\n",
      "Epoch 4/10. Iteration 45400/47167 Losses: train: 2.582794189453125, validate: 2.517763614654541\n",
      "Epoch 4/10. Iteration 45500/47167 Losses: train: 2.127145528793335, validate: 2.517400026321411\n",
      "Epoch 4/10. Iteration 45600/47167 Losses: train: 2.3862974643707275, validate: 2.51200270652771\n",
      "Epoch 4/10. Iteration 45700/47167 Losses: train: 2.2474684715270996, validate: 2.5081427097320557\n",
      "Epoch 4/10. Iteration 45800/47167 Losses: train: 2.5376009941101074, validate: 2.510524034500122\n",
      "Epoch 4/10. Iteration 45900/47167 Losses: train: 2.4464592933654785, validate: 2.5138492584228516\n",
      "Epoch 4/10. Iteration 46000/47167 Losses: train: 2.4170103073120117, validate: 2.5110957622528076\n",
      "Epoch 4/10. Iteration 46100/47167 Losses: train: 2.173215866088867, validate: 2.520972728729248\n",
      "Epoch 4/10. Iteration 46200/47167 Losses: train: 2.378201961517334, validate: 2.5082714557647705\n",
      "Epoch 4/10. Iteration 46300/47167 Losses: train: 2.4068193435668945, validate: 2.5073418617248535\n",
      "Epoch 4/10. Iteration 46400/47167 Losses: train: 2.2825772762298584, validate: 2.5011579990386963\n",
      "Epoch 4/10. Iteration 46500/47167 Losses: train: 2.445694923400879, validate: 2.517059564590454\n",
      "Epoch 4/10. Iteration 46600/47167 Losses: train: 2.3237643241882324, validate: 2.5135695934295654\n",
      "Epoch 4/10. Iteration 46700/47167 Losses: train: 2.275752067565918, validate: 2.499542713165283\n",
      "Epoch 4/10. Iteration 46800/47167 Losses: train: 2.506265163421631, validate: 2.5181877613067627\n",
      "Epoch 4/10. Iteration 46900/47167 Losses: train: 2.3135409355163574, validate: 2.504997491836548\n",
      "Epoch 4/10. Iteration 47000/47167 Losses: train: 2.3766486644744873, validate: 2.5041563510894775\n",
      "Epoch 4/10. Iteration 47100/47167 Losses: train: 2.3839080333709717, validate: 2.5090065002441406\n",
      "Epoch 5/10. Iteration 100/47167 Losses: train: 2.4633922576904297, validate: 2.5113067626953125\n",
      "Epoch 5/10. Iteration 200/47167 Losses: train: 2.247190475463867, validate: 2.50665545463562\n",
      "Epoch 5/10. Iteration 300/47167 Losses: train: 2.247642993927002, validate: 2.5083930492401123\n",
      "Epoch 5/10. Iteration 400/47167 Losses: train: 2.452143669128418, validate: 2.5095362663269043\n",
      "Epoch 5/10. Iteration 500/47167 Losses: train: 2.238682985305786, validate: 2.5107173919677734\n",
      "Epoch 5/10. Iteration 600/47167 Losses: train: 2.228778600692749, validate: 2.507312774658203\n",
      "Epoch 5/10. Iteration 700/47167 Losses: train: 2.225660562515259, validate: 2.5049939155578613\n",
      "Epoch 5/10. Iteration 800/47167 Losses: train: 2.211960554122925, validate: 2.5011205673217773\n",
      "Epoch 5/10. Iteration 900/47167 Losses: train: 2.2379310131073, validate: 2.4968247413635254\n",
      "Epoch 5/10. Iteration 1000/47167 Losses: train: 2.264540672302246, validate: 2.500180721282959\n",
      "Epoch 5/10. Iteration 1100/47167 Losses: train: 2.3765676021575928, validate: 2.499669313430786\n",
      "Epoch 5/10. Iteration 1200/47167 Losses: train: 2.4415552616119385, validate: 2.508167028427124\n",
      "Epoch 5/10. Iteration 1300/47167 Losses: train: 2.4195375442504883, validate: 2.508378744125366\n",
      "Epoch 5/10. Iteration 1400/47167 Losses: train: 2.393789052963257, validate: 2.498155355453491\n",
      "Epoch 5/10. Iteration 1500/47167 Losses: train: 2.393164873123169, validate: 2.505434989929199\n",
      "Epoch 5/10. Iteration 1600/47167 Losses: train: 2.426635980606079, validate: 2.5118565559387207\n",
      "Epoch 5/10. Iteration 1700/47167 Losses: train: 2.311691999435425, validate: 2.512543201446533\n",
      "Epoch 5/10. Iteration 1800/47167 Losses: train: 2.441322088241577, validate: 2.5098824501037598\n",
      "Epoch 5/10. Iteration 1900/47167 Losses: train: 2.1870806217193604, validate: 2.52008318901062\n",
      "Epoch 5/10. Iteration 2000/47167 Losses: train: 2.202017307281494, validate: 2.510216474533081\n",
      "Epoch 5/10. Iteration 2100/47167 Losses: train: 2.4413299560546875, validate: 2.5082428455352783\n",
      "Epoch 5/10. Iteration 2200/47167 Losses: train: 2.299076557159424, validate: 2.5239875316619873\n",
      "Epoch 5/10. Iteration 2300/47167 Losses: train: 2.157701253890991, validate: 2.508530616760254\n",
      "Epoch 5/10. Iteration 2400/47167 Losses: train: 2.3654110431671143, validate: 2.5096123218536377\n",
      "Epoch 5/10. Iteration 2500/47167 Losses: train: 2.34088134765625, validate: 2.5007991790771484\n",
      "Epoch 5/10. Iteration 2600/47167 Losses: train: 2.350151777267456, validate: 2.5053889751434326\n",
      "Epoch 5/10. Iteration 2700/47167 Losses: train: 2.273905038833618, validate: 2.507394552230835\n",
      "Epoch 5/10. Iteration 2800/47167 Losses: train: 2.296628713607788, validate: 2.5094878673553467\n",
      "Epoch 5/10. Iteration 2900/47167 Losses: train: 2.4861721992492676, validate: 2.5082643032073975\n",
      "Epoch 5/10. Iteration 3000/47167 Losses: train: 2.1883583068847656, validate: 2.501840829849243\n",
      "Epoch 5/10. Iteration 3100/47167 Losses: train: 2.4858694076538086, validate: 2.506141424179077\n",
      "Epoch 5/10. Iteration 3200/47167 Losses: train: 2.3055052757263184, validate: 2.5046818256378174\n",
      "Epoch 5/10. Iteration 3300/47167 Losses: train: 2.4655563831329346, validate: 2.5109682083129883\n",
      "Epoch 5/10. Iteration 3400/47167 Losses: train: 2.447190999984741, validate: 2.5186285972595215\n",
      "Epoch 5/10. Iteration 3500/47167 Losses: train: 2.3070709705352783, validate: 2.5025722980499268\n",
      "Epoch 5/10. Iteration 3600/47167 Losses: train: 2.730807304382324, validate: 2.5081722736358643\n",
      "Epoch 5/10. Iteration 3700/47167 Losses: train: 2.354569673538208, validate: 2.5014641284942627\n",
      "Epoch 5/10. Iteration 3800/47167 Losses: train: 2.2871737480163574, validate: 2.5106887817382812\n",
      "Epoch 5/10. Iteration 3900/47167 Losses: train: 2.251871347427368, validate: 2.5012238025665283\n",
      "Epoch 5/10. Iteration 4000/47167 Losses: train: 2.2325947284698486, validate: 2.509124994277954\n",
      "Epoch 5/10. Iteration 4100/47167 Losses: train: 2.2809016704559326, validate: 2.5057005882263184\n",
      "Epoch 5/10. Iteration 4200/47167 Losses: train: 2.332282066345215, validate: 2.510028600692749\n",
      "Epoch 5/10. Iteration 4300/47167 Losses: train: 2.232875108718872, validate: 2.500689744949341\n",
      "Epoch 5/10. Iteration 4400/47167 Losses: train: 2.155576705932617, validate: 2.518970012664795\n",
      "Epoch 5/10. Iteration 4500/47167 Losses: train: 2.320505142211914, validate: 2.503964424133301\n",
      "Epoch 5/10. Iteration 4600/47167 Losses: train: 2.3677408695220947, validate: 2.517735242843628\n",
      "Epoch 5/10. Iteration 4700/47167 Losses: train: 2.3291852474212646, validate: 2.4958717823028564\n",
      "Epoch 5/10. Iteration 4800/47167 Losses: train: 2.395476818084717, validate: 2.498725414276123\n",
      "Epoch 5/10. Iteration 4900/47167 Losses: train: 2.3419349193573, validate: 2.509430170059204\n",
      "Epoch 5/10. Iteration 5000/47167 Losses: train: 2.2276601791381836, validate: 2.49845290184021\n",
      "Epoch 5/10. Iteration 5100/47167 Losses: train: 2.3051164150238037, validate: 2.5013837814331055\n",
      "Epoch 5/10. Iteration 5200/47167 Losses: train: 2.4176716804504395, validate: 2.506312847137451\n",
      "Epoch 5/10. Iteration 5300/47167 Losses: train: 2.4973104000091553, validate: 2.4895665645599365\n",
      "Epoch 5/10. Iteration 5400/47167 Losses: train: 2.3995230197906494, validate: 2.5103747844696045\n",
      "Epoch 5/10. Iteration 5500/47167 Losses: train: 2.3918466567993164, validate: 2.498713970184326\n",
      "Epoch 5/10. Iteration 5600/47167 Losses: train: 2.37790846824646, validate: 2.4950690269470215\n",
      "Epoch 5/10. Iteration 5700/47167 Losses: train: 2.1938745975494385, validate: 2.4994711875915527\n",
      "Epoch 5/10. Iteration 5800/47167 Losses: train: 2.411959171295166, validate: 2.508394956588745\n",
      "Epoch 5/10. Iteration 5900/47167 Losses: train: 2.3582820892333984, validate: 2.5066699981689453\n",
      "Epoch 5/10. Iteration 6000/47167 Losses: train: 2.3031821250915527, validate: 2.501981258392334\n",
      "Epoch 5/10. Iteration 6100/47167 Losses: train: 2.1622586250305176, validate: 2.5103464126586914\n",
      "Epoch 5/10. Iteration 6200/47167 Losses: train: 2.3083128929138184, validate: 2.5054502487182617\n",
      "Epoch 5/10. Iteration 6300/47167 Losses: train: 2.447287082672119, validate: 2.5060126781463623\n",
      "Epoch 5/10. Iteration 6400/47167 Losses: train: 2.2860898971557617, validate: 2.5083351135253906\n",
      "Epoch 5/10. Iteration 6500/47167 Losses: train: 2.2547943592071533, validate: 2.4907052516937256\n",
      "Epoch 5/10. Iteration 6600/47167 Losses: train: 2.3452494144439697, validate: 2.4986772537231445\n",
      "Epoch 5/10. Iteration 6700/47167 Losses: train: 2.261608123779297, validate: 2.5074336528778076\n",
      "Epoch 5/10. Iteration 6800/47167 Losses: train: 2.301119565963745, validate: 2.5099334716796875\n",
      "Epoch 5/10. Iteration 6900/47167 Losses: train: 2.5652377605438232, validate: 2.5110867023468018\n",
      "Epoch 5/10. Iteration 7000/47167 Losses: train: 2.452547788619995, validate: 2.4958784580230713\n",
      "Epoch 5/10. Iteration 7100/47167 Losses: train: 2.351961612701416, validate: 2.5022175312042236\n",
      "Epoch 5/10. Iteration 7200/47167 Losses: train: 2.260590076446533, validate: 2.5052220821380615\n",
      "Epoch 5/10. Iteration 7300/47167 Losses: train: 2.3511056900024414, validate: 2.506556987762451\n",
      "Epoch 5/10. Iteration 7400/47167 Losses: train: 2.366122245788574, validate: 2.501629590988159\n",
      "Epoch 5/10. Iteration 7500/47167 Losses: train: 2.3033905029296875, validate: 2.5065109729766846\n",
      "Epoch 5/10. Iteration 7600/47167 Losses: train: 2.298982858657837, validate: 2.5106091499328613\n",
      "Epoch 5/10. Iteration 7700/47167 Losses: train: 2.4593896865844727, validate: 2.511725902557373\n",
      "Epoch 5/10. Iteration 7800/47167 Losses: train: 2.5242366790771484, validate: 2.505756378173828\n",
      "Epoch 5/10. Iteration 7900/47167 Losses: train: 2.3914246559143066, validate: 2.5076136589050293\n",
      "Epoch 5/10. Iteration 8000/47167 Losses: train: 2.487783908843994, validate: 2.5044355392456055\n",
      "Epoch 5/10. Iteration 8100/47167 Losses: train: 2.506359338760376, validate: 2.5056681632995605\n",
      "Epoch 5/10. Iteration 8200/47167 Losses: train: 2.214826822280884, validate: 2.5005197525024414\n",
      "Epoch 5/10. Iteration 8300/47167 Losses: train: 2.366142749786377, validate: 2.5053927898406982\n",
      "Epoch 5/10. Iteration 8400/47167 Losses: train: 2.3425023555755615, validate: 2.5117690563201904\n",
      "Epoch 5/10. Iteration 8500/47167 Losses: train: 2.357210636138916, validate: 2.5000767707824707\n",
      "Epoch 5/10. Iteration 8600/47167 Losses: train: 2.286841630935669, validate: 2.504868984222412\n",
      "Epoch 5/10. Iteration 8700/47167 Losses: train: 2.274111270904541, validate: 2.505371332168579\n",
      "Epoch 5/10. Iteration 8800/47167 Losses: train: 2.3003954887390137, validate: 2.508359909057617\n",
      "Epoch 5/10. Iteration 8900/47167 Losses: train: 2.325424909591675, validate: 2.493988275527954\n",
      "Epoch 5/10. Iteration 9000/47167 Losses: train: 2.4096693992614746, validate: 2.5057764053344727\n",
      "Epoch 5/10. Iteration 9100/47167 Losses: train: 2.4165890216827393, validate: 2.4953622817993164\n",
      "Epoch 5/10. Iteration 9200/47167 Losses: train: 2.4511477947235107, validate: 2.507662773132324\n",
      "Epoch 5/10. Iteration 9300/47167 Losses: train: 2.360166549682617, validate: 2.504796266555786\n",
      "Epoch 5/10. Iteration 9400/47167 Losses: train: 2.4079947471618652, validate: 2.510847806930542\n",
      "Epoch 5/10. Iteration 9500/47167 Losses: train: 2.312757968902588, validate: 2.504382610321045\n",
      "Epoch 5/10. Iteration 9600/47167 Losses: train: 2.3365674018859863, validate: 2.5141656398773193\n",
      "Epoch 5/10. Iteration 9700/47167 Losses: train: 2.3122172355651855, validate: 2.5067288875579834\n",
      "Epoch 5/10. Iteration 9800/47167 Losses: train: 2.349583387374878, validate: 2.5020463466644287\n",
      "Epoch 5/10. Iteration 9900/47167 Losses: train: 2.1438543796539307, validate: 2.506727695465088\n",
      "Epoch 5/10. Iteration 10000/47167 Losses: train: 2.2431910037994385, validate: 2.5119571685791016\n",
      "Epoch 5/10. Iteration 10100/47167 Losses: train: 2.4747819900512695, validate: 2.5072522163391113\n",
      "Epoch 5/10. Iteration 10200/47167 Losses: train: 2.4206831455230713, validate: 2.5129244327545166\n",
      "Epoch 5/10. Iteration 10300/47167 Losses: train: 2.386542320251465, validate: 2.5148332118988037\n",
      "Epoch 5/10. Iteration 10400/47167 Losses: train: 2.3328499794006348, validate: 2.4987707138061523\n",
      "Epoch 5/10. Iteration 10500/47167 Losses: train: 2.3588287830352783, validate: 2.4915521144866943\n",
      "Epoch 5/10. Iteration 10600/47167 Losses: train: 2.2766823768615723, validate: 2.5003533363342285\n",
      "Epoch 5/10. Iteration 10700/47167 Losses: train: 2.4679737091064453, validate: 2.5047783851623535\n",
      "Epoch 5/10. Iteration 10800/47167 Losses: train: 2.38704514503479, validate: 2.5017640590667725\n",
      "Epoch 5/10. Iteration 10900/47167 Losses: train: 2.4622905254364014, validate: 2.5037083625793457\n",
      "Epoch 5/10. Iteration 11000/47167 Losses: train: 2.3619799613952637, validate: 2.5145840644836426\n",
      "Epoch 5/10. Iteration 11100/47167 Losses: train: 2.4842073917388916, validate: 2.5085978507995605\n",
      "Epoch 5/10. Iteration 11200/47167 Losses: train: 2.2138519287109375, validate: 2.5005204677581787\n",
      "Epoch 5/10. Iteration 11300/47167 Losses: train: 2.319563150405884, validate: 2.501157760620117\n",
      "Epoch 5/10. Iteration 11400/47167 Losses: train: 2.256150722503662, validate: 2.505988597869873\n",
      "Epoch 5/10. Iteration 11500/47167 Losses: train: 2.412139654159546, validate: 2.5047409534454346\n",
      "Epoch 5/10. Iteration 11600/47167 Losses: train: 2.2511136531829834, validate: 2.5104618072509766\n",
      "Epoch 5/10. Iteration 11700/47167 Losses: train: 2.3560791015625, validate: 2.5006906986236572\n",
      "Epoch 5/10. Iteration 11800/47167 Losses: train: 2.3708739280700684, validate: 2.498046398162842\n",
      "Epoch 5/10. Iteration 11900/47167 Losses: train: 2.23280930519104, validate: 2.5085363388061523\n",
      "Epoch 5/10. Iteration 12000/47167 Losses: train: 2.3592777252197266, validate: 2.501274585723877\n",
      "Epoch 5/10. Iteration 12100/47167 Losses: train: 2.296481132507324, validate: 2.506108522415161\n",
      "Epoch 5/10. Iteration 12200/47167 Losses: train: 2.46886944770813, validate: 2.498558521270752\n",
      "Epoch 5/10. Iteration 12300/47167 Losses: train: 2.3597891330718994, validate: 2.5073821544647217\n",
      "Epoch 5/10. Iteration 12400/47167 Losses: train: 2.3580849170684814, validate: 2.506248712539673\n",
      "Epoch 5/10. Iteration 12500/47167 Losses: train: 2.284355401992798, validate: 2.506333589553833\n",
      "Epoch 5/10. Iteration 12600/47167 Losses: train: 2.4229302406311035, validate: 2.51357102394104\n",
      "Epoch 5/10. Iteration 12700/47167 Losses: train: 2.348029851913452, validate: 2.505378484725952\n",
      "Epoch 5/10. Iteration 12800/47167 Losses: train: 2.3169331550598145, validate: 2.5082154273986816\n",
      "Epoch 5/10. Iteration 12900/47167 Losses: train: 2.4029324054718018, validate: 2.505467414855957\n",
      "Epoch 5/10. Iteration 13000/47167 Losses: train: 2.2310898303985596, validate: 2.505950450897217\n",
      "Epoch 5/10. Iteration 13100/47167 Losses: train: 2.310227155685425, validate: 2.4981846809387207\n",
      "Epoch 5/10. Iteration 13200/47167 Losses: train: 2.3160881996154785, validate: 2.494191884994507\n",
      "Epoch 5/10. Iteration 13300/47167 Losses: train: 2.474529504776001, validate: 2.5074760913848877\n",
      "Epoch 5/10. Iteration 13400/47167 Losses: train: 2.3147733211517334, validate: 2.4984288215637207\n",
      "Epoch 5/10. Iteration 13500/47167 Losses: train: 2.2910611629486084, validate: 2.4984846115112305\n",
      "Epoch 5/10. Iteration 13600/47167 Losses: train: 2.323777198791504, validate: 2.505051851272583\n",
      "Epoch 5/10. Iteration 13700/47167 Losses: train: 2.314809799194336, validate: 2.50234055519104\n",
      "Epoch 5/10. Iteration 13800/47167 Losses: train: 2.4794700145721436, validate: 2.4985759258270264\n",
      "Epoch 5/10. Iteration 13900/47167 Losses: train: 2.370251178741455, validate: 2.49631404876709\n",
      "Epoch 5/10. Iteration 14000/47167 Losses: train: 2.196653366088867, validate: 2.5043678283691406\n",
      "Epoch 5/10. Iteration 14100/47167 Losses: train: 2.3339812755584717, validate: 2.5085442066192627\n",
      "Epoch 5/10. Iteration 14200/47167 Losses: train: 2.281327247619629, validate: 2.4967451095581055\n",
      "Epoch 5/10. Iteration 14300/47167 Losses: train: 2.171473264694214, validate: 2.503171443939209\n",
      "Epoch 5/10. Iteration 14400/47167 Losses: train: 2.2715046405792236, validate: 2.489165782928467\n",
      "Epoch 5/10. Iteration 14500/47167 Losses: train: 2.150320529937744, validate: 2.5029006004333496\n",
      "Epoch 5/10. Iteration 14600/47167 Losses: train: 2.361711025238037, validate: 2.49396014213562\n",
      "Epoch 5/10. Iteration 14700/47167 Losses: train: 2.265469551086426, validate: 2.490530014038086\n",
      "Epoch 5/10. Iteration 14800/47167 Losses: train: 2.3504064083099365, validate: 2.500454902648926\n",
      "Epoch 5/10. Iteration 14900/47167 Losses: train: 2.4177353382110596, validate: 2.499861478805542\n",
      "Epoch 5/10. Iteration 15000/47167 Losses: train: 2.219801187515259, validate: 2.5051376819610596\n",
      "Epoch 5/10. Iteration 15100/47167 Losses: train: 2.450901508331299, validate: 2.5020294189453125\n",
      "Epoch 5/10. Iteration 15200/47167 Losses: train: 2.227065086364746, validate: 2.4932689666748047\n",
      "Epoch 5/10. Iteration 15300/47167 Losses: train: 2.430638313293457, validate: 2.4837868213653564\n",
      "Epoch 5/10. Iteration 15400/47167 Losses: train: 2.3955085277557373, validate: 2.4950239658355713\n",
      "Epoch 5/10. Iteration 15500/47167 Losses: train: 2.2530152797698975, validate: 2.4999873638153076\n",
      "Epoch 5/10. Iteration 15600/47167 Losses: train: 2.399155616760254, validate: 2.498915195465088\n",
      "Epoch 5/10. Iteration 15700/47167 Losses: train: 2.190927505493164, validate: 2.5005717277526855\n",
      "Epoch 5/10. Iteration 15800/47167 Losses: train: 2.2193586826324463, validate: 2.4982571601867676\n",
      "Epoch 5/10. Iteration 15900/47167 Losses: train: 2.539630889892578, validate: 2.496659278869629\n",
      "Epoch 5/10. Iteration 16000/47167 Losses: train: 2.338987112045288, validate: 2.4886457920074463\n",
      "Epoch 5/10. Iteration 16100/47167 Losses: train: 2.474670886993408, validate: 2.4981651306152344\n",
      "Epoch 5/10. Iteration 16200/47167 Losses: train: 2.329953193664551, validate: 2.492443084716797\n",
      "Epoch 5/10. Iteration 16300/47167 Losses: train: 2.3810558319091797, validate: 2.495563268661499\n",
      "Epoch 5/10. Iteration 16400/47167 Losses: train: 2.2673985958099365, validate: 2.494988441467285\n",
      "Epoch 5/10. Iteration 16500/47167 Losses: train: 2.2333168983459473, validate: 2.497617721557617\n",
      "Epoch 5/10. Iteration 16600/47167 Losses: train: 2.463083267211914, validate: 2.493166208267212\n",
      "Epoch 5/10. Iteration 16700/47167 Losses: train: 2.317702293395996, validate: 2.497431516647339\n",
      "Epoch 5/10. Iteration 16800/47167 Losses: train: 2.472529888153076, validate: 2.49261736869812\n",
      "Epoch 5/10. Iteration 16900/47167 Losses: train: 2.349421977996826, validate: 2.495739221572876\n",
      "Epoch 5/10. Iteration 17000/47167 Losses: train: 2.2578694820404053, validate: 2.494112730026245\n",
      "Epoch 5/10. Iteration 17100/47167 Losses: train: 2.4079651832580566, validate: 2.4953384399414062\n",
      "Epoch 5/10. Iteration 17200/47167 Losses: train: 2.4139575958251953, validate: 2.485856056213379\n",
      "Epoch 5/10. Iteration 17300/47167 Losses: train: 2.4190673828125, validate: 2.4828672409057617\n",
      "Epoch 5/10. Iteration 17400/47167 Losses: train: 2.224092483520508, validate: 2.4914400577545166\n",
      "Epoch 5/10. Iteration 17500/47167 Losses: train: 2.1562325954437256, validate: 2.497889280319214\n",
      "Epoch 5/10. Iteration 17600/47167 Losses: train: 2.283695936203003, validate: 2.4857497215270996\n",
      "Epoch 5/10. Iteration 17700/47167 Losses: train: 2.263181447982788, validate: 2.4855456352233887\n",
      "Epoch 5/10. Iteration 17800/47167 Losses: train: 2.358924388885498, validate: 2.485485792160034\n",
      "Epoch 5/10. Iteration 17900/47167 Losses: train: 2.386037588119507, validate: 2.501298189163208\n",
      "Epoch 5/10. Iteration 18000/47167 Losses: train: 2.424881935119629, validate: 2.4843673706054688\n",
      "Epoch 5/10. Iteration 18100/47167 Losses: train: 2.351956605911255, validate: 2.485727548599243\n",
      "Epoch 5/10. Iteration 18200/47167 Losses: train: 2.292306900024414, validate: 2.490885019302368\n",
      "Epoch 5/10. Iteration 18300/47167 Losses: train: 2.2795042991638184, validate: 2.4873085021972656\n",
      "Epoch 5/10. Iteration 18400/47167 Losses: train: 2.210320472717285, validate: 2.4790875911712646\n",
      "Epoch 5/10. Iteration 18500/47167 Losses: train: 2.3610098361968994, validate: 2.4783694744110107\n",
      "Epoch 5/10. Iteration 18600/47167 Losses: train: 2.4117164611816406, validate: 2.483776092529297\n",
      "Epoch 5/10. Iteration 18700/47167 Losses: train: 2.3500890731811523, validate: 2.4846031665802\n",
      "Epoch 5/10. Iteration 18800/47167 Losses: train: 2.4820730686187744, validate: 2.476473569869995\n",
      "Epoch 5/10. Iteration 18900/47167 Losses: train: 2.4576947689056396, validate: 2.482179641723633\n",
      "Epoch 5/10. Iteration 19000/47167 Losses: train: 2.1664977073669434, validate: 2.4919958114624023\n",
      "Epoch 5/10. Iteration 19100/47167 Losses: train: 2.4482502937316895, validate: 2.488147020339966\n",
      "Epoch 5/10. Iteration 19200/47167 Losses: train: 2.2728657722473145, validate: 2.4838340282440186\n",
      "Epoch 5/10. Iteration 19300/47167 Losses: train: 2.3780550956726074, validate: 2.489880323410034\n",
      "Epoch 5/10. Iteration 19400/47167 Losses: train: 2.3661396503448486, validate: 2.4920146465301514\n",
      "Epoch 5/10. Iteration 19500/47167 Losses: train: 2.3251302242279053, validate: 2.49851393699646\n",
      "Epoch 5/10. Iteration 19600/47167 Losses: train: 2.590442657470703, validate: 2.495180368423462\n",
      "Epoch 5/10. Iteration 19700/47167 Losses: train: 2.349208116531372, validate: 2.490717887878418\n",
      "Epoch 5/10. Iteration 19800/47167 Losses: train: 2.5004091262817383, validate: 2.486117362976074\n",
      "Epoch 5/10. Iteration 19900/47167 Losses: train: 2.21942400932312, validate: 2.4866111278533936\n",
      "Epoch 5/10. Iteration 20000/47167 Losses: train: 2.283886671066284, validate: 2.4824366569519043\n",
      "Epoch 5/10. Iteration 20100/47167 Losses: train: 2.3384909629821777, validate: 2.4747512340545654\n",
      "Epoch 5/10. Iteration 20200/47167 Losses: train: 2.3665928840637207, validate: 2.477734088897705\n",
      "Epoch 5/10. Iteration 20300/47167 Losses: train: 2.379026412963867, validate: 2.479426383972168\n",
      "Epoch 5/10. Iteration 20400/47167 Losses: train: 2.4280688762664795, validate: 2.4869890213012695\n",
      "Epoch 5/10. Iteration 20500/47167 Losses: train: 2.3733325004577637, validate: 2.4808382987976074\n",
      "Epoch 5/10. Iteration 20600/47167 Losses: train: 2.4883627891540527, validate: 2.489206314086914\n",
      "Epoch 5/10. Iteration 20700/47167 Losses: train: 2.203430414199829, validate: 2.4912757873535156\n",
      "Epoch 5/10. Iteration 20800/47167 Losses: train: 2.359895706176758, validate: 2.494652032852173\n",
      "Epoch 5/10. Iteration 20900/47167 Losses: train: 2.355724334716797, validate: 2.508328676223755\n",
      "Epoch 5/10. Iteration 21000/47167 Losses: train: 2.4853451251983643, validate: 2.488892078399658\n",
      "Epoch 5/10. Iteration 21100/47167 Losses: train: 2.250976324081421, validate: 2.492859125137329\n",
      "Epoch 5/10. Iteration 21200/47167 Losses: train: 2.2786951065063477, validate: 2.495351552963257\n",
      "Epoch 5/10. Iteration 21300/47167 Losses: train: 2.4672722816467285, validate: 2.501329183578491\n",
      "Epoch 5/10. Iteration 21400/47167 Losses: train: 2.223037004470825, validate: 2.4988133907318115\n",
      "Epoch 5/10. Iteration 21500/47167 Losses: train: 2.3268306255340576, validate: 2.49823260307312\n",
      "Epoch 5/10. Iteration 21600/47167 Losses: train: 2.421405076980591, validate: 2.485776424407959\n",
      "Epoch 5/10. Iteration 21700/47167 Losses: train: 2.4737703800201416, validate: 2.5006442070007324\n",
      "Epoch 5/10. Iteration 21800/47167 Losses: train: 2.15584659576416, validate: 2.485548257827759\n",
      "Epoch 5/10. Iteration 21900/47167 Losses: train: 2.3716883659362793, validate: 2.4917476177215576\n",
      "Epoch 5/10. Iteration 22000/47167 Losses: train: 2.316696882247925, validate: 2.481389045715332\n",
      "Epoch 5/10. Iteration 22100/47167 Losses: train: 2.269199848175049, validate: 2.4986143112182617\n",
      "Epoch 5/10. Iteration 22200/47167 Losses: train: 2.1583364009857178, validate: 2.4923605918884277\n",
      "Epoch 5/10. Iteration 22300/47167 Losses: train: 2.1896955966949463, validate: 2.4915390014648438\n",
      "Epoch 5/10. Iteration 22400/47167 Losses: train: 2.2558162212371826, validate: 2.4822027683258057\n",
      "Epoch 5/10. Iteration 22500/47167 Losses: train: 2.4262354373931885, validate: 2.471389055252075\n",
      "Epoch 5/10. Iteration 22600/47167 Losses: train: 2.3229191303253174, validate: 2.480816602706909\n",
      "Epoch 5/10. Iteration 22700/47167 Losses: train: 2.2282862663269043, validate: 2.491436004638672\n",
      "Epoch 5/10. Iteration 22800/47167 Losses: train: 2.4114038944244385, validate: 2.4820237159729004\n",
      "Epoch 5/10. Iteration 22900/47167 Losses: train: 2.3974037170410156, validate: 2.4893085956573486\n",
      "Epoch 5/10. Iteration 23000/47167 Losses: train: 2.282179355621338, validate: 2.489149808883667\n",
      "Epoch 5/10. Iteration 23100/47167 Losses: train: 2.540855884552002, validate: 2.4942679405212402\n",
      "Epoch 5/10. Iteration 23200/47167 Losses: train: 2.2570042610168457, validate: 2.4917054176330566\n",
      "Epoch 5/10. Iteration 23300/47167 Losses: train: 2.4034886360168457, validate: 2.484384298324585\n",
      "Epoch 5/10. Iteration 23400/47167 Losses: train: 2.4387385845184326, validate: 2.489269733428955\n",
      "Epoch 5/10. Iteration 23500/47167 Losses: train: 2.3410189151763916, validate: 2.4822590351104736\n",
      "Epoch 5/10. Iteration 23600/47167 Losses: train: 2.2879433631896973, validate: 2.4940602779388428\n",
      "Epoch 5/10. Iteration 23700/47167 Losses: train: 2.3808438777923584, validate: 2.4802303314208984\n",
      "Epoch 5/10. Iteration 23800/47167 Losses: train: 2.5150551795959473, validate: 2.4771502017974854\n",
      "Epoch 5/10. Iteration 23900/47167 Losses: train: 2.295567274093628, validate: 2.471055507659912\n",
      "Epoch 5/10. Iteration 24000/47167 Losses: train: 2.215226650238037, validate: 2.485213279724121\n",
      "Epoch 5/10. Iteration 24100/47167 Losses: train: 2.335738182067871, validate: 2.4789021015167236\n",
      "Epoch 5/10. Iteration 24200/47167 Losses: train: 2.2704684734344482, validate: 2.4762213230133057\n",
      "Epoch 5/10. Iteration 24300/47167 Losses: train: 2.385601282119751, validate: 2.477169990539551\n",
      "Epoch 5/10. Iteration 24400/47167 Losses: train: 2.314922571182251, validate: 2.473900556564331\n",
      "Epoch 5/10. Iteration 24500/47167 Losses: train: 2.4292654991149902, validate: 2.484639883041382\n",
      "Epoch 5/10. Iteration 24600/47167 Losses: train: 2.430097818374634, validate: 2.4861278533935547\n",
      "Epoch 5/10. Iteration 24700/47167 Losses: train: 2.315850019454956, validate: 2.4822440147399902\n",
      "Epoch 5/10. Iteration 24800/47167 Losses: train: 2.419558525085449, validate: 2.4778640270233154\n",
      "Epoch 5/10. Iteration 24900/47167 Losses: train: 2.3300468921661377, validate: 2.4851717948913574\n",
      "Epoch 5/10. Iteration 25000/47167 Losses: train: 2.2115671634674072, validate: 2.472846508026123\n",
      "Epoch 5/10. Iteration 25100/47167 Losses: train: 2.4616856575012207, validate: 2.500157594680786\n",
      "Epoch 5/10. Iteration 25200/47167 Losses: train: 2.269942283630371, validate: 2.4859676361083984\n",
      "Epoch 5/10. Iteration 25300/47167 Losses: train: 2.3811635971069336, validate: 2.492318630218506\n",
      "Epoch 5/10. Iteration 25400/47167 Losses: train: 2.349245309829712, validate: 2.4929020404815674\n",
      "Epoch 5/10. Iteration 25500/47167 Losses: train: 2.285801887512207, validate: 2.4867122173309326\n",
      "Epoch 5/10. Iteration 25600/47167 Losses: train: 2.197035312652588, validate: 2.488433599472046\n",
      "Epoch 5/10. Iteration 25700/47167 Losses: train: 2.328827381134033, validate: 2.4849071502685547\n",
      "Epoch 5/10. Iteration 25800/47167 Losses: train: 2.2862660884857178, validate: 2.4991815090179443\n",
      "Epoch 5/10. Iteration 25900/47167 Losses: train: 2.1197216510772705, validate: 2.4881534576416016\n",
      "Epoch 5/10. Iteration 26000/47167 Losses: train: 2.325241804122925, validate: 2.4795870780944824\n",
      "Epoch 5/10. Iteration 26100/47167 Losses: train: 2.4368531703948975, validate: 2.476705551147461\n",
      "Epoch 5/10. Iteration 26200/47167 Losses: train: 2.359926700592041, validate: 2.4874250888824463\n",
      "Epoch 5/10. Iteration 26300/47167 Losses: train: 2.136594533920288, validate: 2.484516143798828\n",
      "Epoch 5/10. Iteration 26400/47167 Losses: train: 2.2054998874664307, validate: 2.481844663619995\n",
      "Epoch 5/10. Iteration 26500/47167 Losses: train: 2.286311149597168, validate: 2.475991725921631\n",
      "Epoch 5/10. Iteration 26600/47167 Losses: train: 2.297004461288452, validate: 2.4818553924560547\n",
      "Epoch 5/10. Iteration 26700/47167 Losses: train: 2.1768746376037598, validate: 2.492872714996338\n",
      "Epoch 5/10. Iteration 26800/47167 Losses: train: 2.261838674545288, validate: 2.4886393547058105\n",
      "Epoch 5/10. Iteration 26900/47167 Losses: train: 2.453183174133301, validate: 2.485785722732544\n",
      "Epoch 5/10. Iteration 27000/47167 Losses: train: 2.367779016494751, validate: 2.483917713165283\n",
      "Epoch 5/10. Iteration 27100/47167 Losses: train: 2.2874393463134766, validate: 2.4824628829956055\n",
      "Epoch 5/10. Iteration 27200/47167 Losses: train: 2.4363059997558594, validate: 2.485950469970703\n",
      "Epoch 5/10. Iteration 27300/47167 Losses: train: 2.2768030166625977, validate: 2.477020502090454\n",
      "Epoch 5/10. Iteration 27400/47167 Losses: train: 2.2902438640594482, validate: 2.4853389263153076\n",
      "Epoch 5/10. Iteration 27500/47167 Losses: train: 2.542280912399292, validate: 2.487654685974121\n",
      "Epoch 5/10. Iteration 27600/47167 Losses: train: 2.452332019805908, validate: 2.473557710647583\n",
      "Epoch 5/10. Iteration 27700/47167 Losses: train: 2.3258843421936035, validate: 2.4890148639678955\n",
      "Epoch 5/10. Iteration 27800/47167 Losses: train: 2.40116286277771, validate: 2.4836525917053223\n",
      "Epoch 5/10. Iteration 27900/47167 Losses: train: 2.4663949012756348, validate: 2.4886934757232666\n",
      "Epoch 5/10. Iteration 28000/47167 Losses: train: 2.4235167503356934, validate: 2.486720085144043\n",
      "Epoch 5/10. Iteration 28100/47167 Losses: train: 2.482440948486328, validate: 2.4805805683135986\n",
      "Epoch 5/10. Iteration 28200/47167 Losses: train: 2.3810298442840576, validate: 2.486128807067871\n",
      "Epoch 5/10. Iteration 28300/47167 Losses: train: 2.387293815612793, validate: 2.4756953716278076\n",
      "Epoch 5/10. Iteration 28400/47167 Losses: train: 2.425868272781372, validate: 2.480574369430542\n",
      "Epoch 5/10. Iteration 28500/47167 Losses: train: 2.4208874702453613, validate: 2.4851155281066895\n",
      "Epoch 5/10. Iteration 28600/47167 Losses: train: 2.3029823303222656, validate: 2.4847187995910645\n",
      "Epoch 5/10. Iteration 28700/47167 Losses: train: 2.2472400665283203, validate: 2.487884521484375\n",
      "Epoch 5/10. Iteration 28800/47167 Losses: train: 2.425363302230835, validate: 2.4796488285064697\n",
      "Epoch 5/10. Iteration 28900/47167 Losses: train: 2.185391902923584, validate: 2.465716600418091\n",
      "Epoch 5/10. Iteration 29000/47167 Losses: train: 2.2677667140960693, validate: 2.4747540950775146\n",
      "Epoch 5/10. Iteration 29100/47167 Losses: train: 2.284120798110962, validate: 2.476823568344116\n",
      "Epoch 5/10. Iteration 29200/47167 Losses: train: 2.3237497806549072, validate: 2.4702792167663574\n",
      "Epoch 5/10. Iteration 29300/47167 Losses: train: 2.3903932571411133, validate: 2.4709274768829346\n",
      "Epoch 5/10. Iteration 29400/47167 Losses: train: 2.4579946994781494, validate: 2.4689505100250244\n",
      "Epoch 5/10. Iteration 29500/47167 Losses: train: 2.48294997215271, validate: 2.475249767303467\n",
      "Epoch 5/10. Iteration 29600/47167 Losses: train: 2.3133902549743652, validate: 2.484548807144165\n",
      "Epoch 5/10. Iteration 29700/47167 Losses: train: 2.2523903846740723, validate: 2.469224691390991\n",
      "Epoch 5/10. Iteration 29800/47167 Losses: train: 2.272026538848877, validate: 2.474649667739868\n",
      "Epoch 5/10. Iteration 29900/47167 Losses: train: 2.3063580989837646, validate: 2.477450370788574\n",
      "Epoch 5/10. Iteration 30000/47167 Losses: train: 2.3527867794036865, validate: 2.4884092807769775\n",
      "Epoch 5/10. Iteration 30100/47167 Losses: train: 2.229724168777466, validate: 2.469872236251831\n",
      "Epoch 5/10. Iteration 30200/47167 Losses: train: 2.3707244396209717, validate: 2.4687719345092773\n",
      "Epoch 5/10. Iteration 30300/47167 Losses: train: 2.4745144844055176, validate: 2.471973180770874\n",
      "Epoch 5/10. Iteration 30400/47167 Losses: train: 2.262538433074951, validate: 2.470508337020874\n",
      "Epoch 5/10. Iteration 30500/47167 Losses: train: 2.401477575302124, validate: 2.4851629734039307\n",
      "Epoch 5/10. Iteration 30600/47167 Losses: train: 2.3713104724884033, validate: 2.4736974239349365\n",
      "Epoch 5/10. Iteration 30700/47167 Losses: train: 2.2948989868164062, validate: 2.473275899887085\n",
      "Epoch 5/10. Iteration 30800/47167 Losses: train: 2.2929811477661133, validate: 2.477998971939087\n",
      "Epoch 5/10. Iteration 30900/47167 Losses: train: 2.448030710220337, validate: 2.4727225303649902\n",
      "Epoch 5/10. Iteration 31000/47167 Losses: train: 2.2468137741088867, validate: 2.464081048965454\n",
      "Epoch 5/10. Iteration 31100/47167 Losses: train: 2.329921245574951, validate: 2.4747767448425293\n",
      "Epoch 5/10. Iteration 31200/47167 Losses: train: 2.285276174545288, validate: 2.472403049468994\n",
      "Epoch 5/10. Iteration 31300/47167 Losses: train: 2.3290274143218994, validate: 2.469541549682617\n",
      "Epoch 5/10. Iteration 31400/47167 Losses: train: 2.1964685916900635, validate: 2.470463514328003\n",
      "Epoch 5/10. Iteration 31500/47167 Losses: train: 2.30751895904541, validate: 2.4865946769714355\n",
      "Epoch 5/10. Iteration 31600/47167 Losses: train: 2.4192771911621094, validate: 2.470146656036377\n",
      "Epoch 5/10. Iteration 31700/47167 Losses: train: 2.3583407402038574, validate: 2.470764636993408\n",
      "Epoch 5/10. Iteration 31800/47167 Losses: train: 2.3241899013519287, validate: 2.4642632007598877\n",
      "Epoch 5/10. Iteration 31900/47167 Losses: train: 2.250704765319824, validate: 2.4677951335906982\n",
      "Epoch 5/10. Iteration 32000/47167 Losses: train: 2.2500953674316406, validate: 2.4676225185394287\n",
      "Epoch 5/10. Iteration 32100/47167 Losses: train: 2.404205083847046, validate: 2.459594249725342\n",
      "Epoch 5/10. Iteration 32200/47167 Losses: train: 2.329072952270508, validate: 2.4716432094573975\n",
      "Epoch 5/10. Iteration 32300/47167 Losses: train: 2.5454905033111572, validate: 2.4659509658813477\n",
      "Epoch 5/10. Iteration 32400/47167 Losses: train: 2.4573447704315186, validate: 2.4718871116638184\n",
      "Epoch 5/10. Iteration 32500/47167 Losses: train: 2.4573798179626465, validate: 2.4740400314331055\n",
      "Epoch 5/10. Iteration 32600/47167 Losses: train: 2.397897481918335, validate: 2.4657835960388184\n",
      "Epoch 5/10. Iteration 32700/47167 Losses: train: 2.4698007106781006, validate: 2.467479944229126\n",
      "Epoch 5/10. Iteration 32800/47167 Losses: train: 2.301020383834839, validate: 2.4655985832214355\n",
      "Epoch 5/10. Iteration 32900/47167 Losses: train: 2.329129457473755, validate: 2.463453769683838\n",
      "Epoch 5/10. Iteration 33000/47167 Losses: train: 2.2343502044677734, validate: 2.4817888736724854\n",
      "Epoch 5/10. Iteration 33100/47167 Losses: train: 2.245683431625366, validate: 2.470571756362915\n",
      "Epoch 5/10. Iteration 33200/47167 Losses: train: 2.1499645709991455, validate: 2.473592758178711\n",
      "Epoch 5/10. Iteration 33300/47167 Losses: train: 2.356724739074707, validate: 2.4761178493499756\n",
      "Epoch 5/10. Iteration 33400/47167 Losses: train: 2.436439275741577, validate: 2.473444938659668\n",
      "Epoch 5/10. Iteration 33500/47167 Losses: train: 2.1979856491088867, validate: 2.4786581993103027\n",
      "Epoch 5/10. Iteration 33600/47167 Losses: train: 2.59771990776062, validate: 2.477477788925171\n",
      "Epoch 5/10. Iteration 33700/47167 Losses: train: 2.527672290802002, validate: 2.4796574115753174\n",
      "Epoch 5/10. Iteration 33800/47167 Losses: train: 2.3581349849700928, validate: 2.470031976699829\n",
      "Epoch 5/10. Iteration 33900/47167 Losses: train: 2.4302000999450684, validate: 2.468534231185913\n",
      "Epoch 5/10. Iteration 34000/47167 Losses: train: 2.2162680625915527, validate: 2.4782371520996094\n",
      "Epoch 5/10. Iteration 34100/47167 Losses: train: 2.2161800861358643, validate: 2.474980354309082\n",
      "Epoch 5/10. Iteration 34200/47167 Losses: train: 2.4864437580108643, validate: 2.4720797538757324\n",
      "Epoch 5/10. Iteration 34300/47167 Losses: train: 2.4288763999938965, validate: 2.4704997539520264\n",
      "Epoch 5/10. Iteration 34400/47167 Losses: train: 2.3214097023010254, validate: 2.4718034267425537\n",
      "Epoch 5/10. Iteration 34500/47167 Losses: train: 2.2351834774017334, validate: 2.473834753036499\n",
      "Epoch 5/10. Iteration 34600/47167 Losses: train: 2.322197675704956, validate: 2.4711055755615234\n",
      "Epoch 5/10. Iteration 34700/47167 Losses: train: 2.372239828109741, validate: 2.4772019386291504\n",
      "Epoch 5/10. Iteration 34800/47167 Losses: train: 2.344376802444458, validate: 2.4678735733032227\n",
      "Epoch 5/10. Iteration 34900/47167 Losses: train: 2.333662986755371, validate: 2.477339267730713\n",
      "Epoch 5/10. Iteration 35000/47167 Losses: train: 2.1474504470825195, validate: 2.479556083679199\n",
      "Epoch 5/10. Iteration 35100/47167 Losses: train: 2.415247917175293, validate: 2.4708242416381836\n",
      "Epoch 5/10. Iteration 35200/47167 Losses: train: 2.2463431358337402, validate: 2.468264579772949\n",
      "Epoch 5/10. Iteration 35300/47167 Losses: train: 2.249469757080078, validate: 2.469569206237793\n",
      "Epoch 5/10. Iteration 35400/47167 Losses: train: 2.3556833267211914, validate: 2.469154119491577\n",
      "Epoch 5/10. Iteration 35500/47167 Losses: train: 2.3821218013763428, validate: 2.4802191257476807\n",
      "Epoch 5/10. Iteration 35600/47167 Losses: train: 2.2351648807525635, validate: 2.4804046154022217\n",
      "Epoch 5/10. Iteration 35700/47167 Losses: train: 2.30859375, validate: 2.4828789234161377\n",
      "Epoch 5/10. Iteration 35800/47167 Losses: train: 2.2334353923797607, validate: 2.478210687637329\n",
      "Epoch 5/10. Iteration 35900/47167 Losses: train: 2.394392728805542, validate: 2.471019744873047\n",
      "Epoch 5/10. Iteration 36000/47167 Losses: train: 2.2349438667297363, validate: 2.4772768020629883\n",
      "Epoch 5/10. Iteration 36100/47167 Losses: train: 2.2956111431121826, validate: 2.477806329727173\n",
      "Epoch 5/10. Iteration 36200/47167 Losses: train: 2.3612210750579834, validate: 2.4621670246124268\n",
      "Epoch 5/10. Iteration 36300/47167 Losses: train: 2.3968191146850586, validate: 2.4729015827178955\n",
      "Epoch 5/10. Iteration 36400/47167 Losses: train: 2.466583251953125, validate: 2.47944974899292\n",
      "Epoch 5/10. Iteration 36500/47167 Losses: train: 2.497471332550049, validate: 2.4769086837768555\n",
      "Epoch 5/10. Iteration 36600/47167 Losses: train: 2.369527816772461, validate: 2.469707489013672\n",
      "Epoch 5/10. Iteration 36700/47167 Losses: train: 2.2914865016937256, validate: 2.464477777481079\n",
      "Epoch 5/10. Iteration 36800/47167 Losses: train: 2.192596912384033, validate: 2.47210431098938\n",
      "Epoch 5/10. Iteration 36900/47167 Losses: train: 2.2633554935455322, validate: 2.4611387252807617\n",
      "Epoch 5/10. Iteration 37000/47167 Losses: train: 2.2911746501922607, validate: 2.4628398418426514\n",
      "Epoch 5/10. Iteration 37100/47167 Losses: train: 2.3809051513671875, validate: 2.471129894256592\n",
      "Epoch 5/10. Iteration 37200/47167 Losses: train: 2.184659719467163, validate: 2.4570984840393066\n",
      "Epoch 5/10. Iteration 37300/47167 Losses: train: 2.3603577613830566, validate: 2.4553098678588867\n",
      "Epoch 5/10. Iteration 37400/47167 Losses: train: 2.457421064376831, validate: 2.470763921737671\n",
      "Epoch 5/10. Iteration 37500/47167 Losses: train: 2.2604146003723145, validate: 2.4610118865966797\n",
      "Epoch 5/10. Iteration 37600/47167 Losses: train: 2.3767879009246826, validate: 2.4644036293029785\n",
      "Epoch 5/10. Iteration 37700/47167 Losses: train: 2.4888012409210205, validate: 2.458950996398926\n",
      "Epoch 5/10. Iteration 37800/47167 Losses: train: 2.31624436378479, validate: 2.465461015701294\n",
      "Epoch 5/10. Iteration 37900/47167 Losses: train: 2.353635787963867, validate: 2.4549999237060547\n",
      "Epoch 5/10. Iteration 38000/47167 Losses: train: 2.259767770767212, validate: 2.458289623260498\n",
      "Epoch 5/10. Iteration 38100/47167 Losses: train: 2.1305532455444336, validate: 2.4578781127929688\n",
      "Epoch 5/10. Iteration 38200/47167 Losses: train: 2.1636579036712646, validate: 2.4621922969818115\n",
      "Epoch 5/10. Iteration 38300/47167 Losses: train: 2.484748125076294, validate: 2.464107036590576\n",
      "Epoch 5/10. Iteration 38400/47167 Losses: train: 2.3414394855499268, validate: 2.461998224258423\n",
      "Epoch 5/10. Iteration 38500/47167 Losses: train: 2.337897777557373, validate: 2.467024803161621\n",
      "Epoch 5/10. Iteration 38600/47167 Losses: train: 2.277679443359375, validate: 2.4657061100006104\n",
      "Epoch 5/10. Iteration 38700/47167 Losses: train: 2.079458236694336, validate: 2.466061592102051\n",
      "Epoch 5/10. Iteration 38800/47167 Losses: train: 2.2499380111694336, validate: 2.463332414627075\n",
      "Epoch 5/10. Iteration 38900/47167 Losses: train: 2.299201011657715, validate: 2.465610980987549\n",
      "Epoch 5/10. Iteration 39000/47167 Losses: train: 2.259164333343506, validate: 2.4548511505126953\n",
      "Epoch 5/10. Iteration 39100/47167 Losses: train: 2.3629493713378906, validate: 2.469435453414917\n",
      "Epoch 5/10. Iteration 39200/47167 Losses: train: 2.4181344509124756, validate: 2.4745519161224365\n",
      "Epoch 5/10. Iteration 39300/47167 Losses: train: 2.49090838432312, validate: 2.459043264389038\n",
      "Epoch 5/10. Iteration 39400/47167 Losses: train: 2.3182997703552246, validate: 2.4739465713500977\n",
      "Epoch 5/10. Iteration 39500/47167 Losses: train: 2.306431770324707, validate: 2.4561927318573\n",
      "Epoch 5/10. Iteration 39600/47167 Losses: train: 2.211172342300415, validate: 2.454660415649414\n",
      "Epoch 5/10. Iteration 39700/47167 Losses: train: 2.290910482406616, validate: 2.466789484024048\n",
      "Epoch 5/10. Iteration 39800/47167 Losses: train: 2.246110439300537, validate: 2.4493231773376465\n",
      "Epoch 5/10. Iteration 39900/47167 Losses: train: 2.3456859588623047, validate: 2.463782548904419\n",
      "Epoch 5/10. Iteration 40000/47167 Losses: train: 2.335394859313965, validate: 2.4658193588256836\n",
      "Epoch 5/10. Iteration 40100/47167 Losses: train: 2.1915442943573, validate: 2.4605441093444824\n",
      "Epoch 5/10. Iteration 40200/47167 Losses: train: 2.374969720840454, validate: 2.4635140895843506\n",
      "Epoch 5/10. Iteration 40300/47167 Losses: train: 2.424243927001953, validate: 2.4704718589782715\n",
      "Epoch 5/10. Iteration 40400/47167 Losses: train: 2.464113473892212, validate: 2.455681562423706\n",
      "Epoch 5/10. Iteration 40500/47167 Losses: train: 2.499878406524658, validate: 2.471158266067505\n",
      "Epoch 5/10. Iteration 40600/47167 Losses: train: 2.453411340713501, validate: 2.470825672149658\n",
      "Epoch 5/10. Iteration 40700/47167 Losses: train: 2.3793866634368896, validate: 2.4581780433654785\n",
      "Epoch 5/10. Iteration 40800/47167 Losses: train: 2.238450527191162, validate: 2.4572620391845703\n",
      "Epoch 5/10. Iteration 40900/47167 Losses: train: 2.3993446826934814, validate: 2.456960916519165\n",
      "Epoch 5/10. Iteration 41000/47167 Losses: train: 2.1614224910736084, validate: 2.467481851577759\n",
      "Epoch 5/10. Iteration 41100/47167 Losses: train: 2.3925817012786865, validate: 2.476952314376831\n",
      "Epoch 5/10. Iteration 41200/47167 Losses: train: 2.422541856765747, validate: 2.4643971920013428\n",
      "Epoch 5/10. Iteration 41300/47167 Losses: train: 2.4398679733276367, validate: 2.472337245941162\n",
      "Epoch 5/10. Iteration 41400/47167 Losses: train: 2.2022910118103027, validate: 2.4581127166748047\n",
      "Epoch 5/10. Iteration 41500/47167 Losses: train: 2.353825092315674, validate: 2.4705684185028076\n",
      "Epoch 5/10. Iteration 41600/47167 Losses: train: 2.302673101425171, validate: 2.4446611404418945\n",
      "Epoch 5/10. Iteration 41700/47167 Losses: train: 2.0573081970214844, validate: 2.4671618938446045\n",
      "Epoch 5/10. Iteration 41800/47167 Losses: train: 2.359636068344116, validate: 2.4675562381744385\n",
      "Epoch 5/10. Iteration 41900/47167 Losses: train: 2.224299907684326, validate: 2.452925682067871\n",
      "Epoch 5/10. Iteration 42000/47167 Losses: train: 2.5254065990448, validate: 2.470921516418457\n",
      "Epoch 5/10. Iteration 42100/47167 Losses: train: 2.417827606201172, validate: 2.4613330364227295\n",
      "Epoch 5/10. Iteration 42200/47167 Losses: train: 2.159621000289917, validate: 2.4598259925842285\n",
      "Epoch 5/10. Iteration 42300/47167 Losses: train: 2.2968671321868896, validate: 2.4602434635162354\n",
      "Epoch 5/10. Iteration 42400/47167 Losses: train: 2.3487844467163086, validate: 2.4546804428100586\n",
      "Epoch 5/10. Iteration 42500/47167 Losses: train: 2.5231025218963623, validate: 2.4606099128723145\n",
      "Epoch 5/10. Iteration 42600/47167 Losses: train: 2.290757417678833, validate: 2.456321954727173\n",
      "Epoch 5/10. Iteration 42700/47167 Losses: train: 2.392651081085205, validate: 2.457895040512085\n",
      "Epoch 5/10. Iteration 42800/47167 Losses: train: 2.2419772148132324, validate: 2.4593305587768555\n",
      "Epoch 5/10. Iteration 42900/47167 Losses: train: 2.2915756702423096, validate: 2.457659959793091\n",
      "Epoch 5/10. Iteration 43000/47167 Losses: train: 2.2382466793060303, validate: 2.457488536834717\n",
      "Epoch 5/10. Iteration 43100/47167 Losses: train: 2.1062586307525635, validate: 2.460966110229492\n",
      "Epoch 5/10. Iteration 43200/47167 Losses: train: 2.399657726287842, validate: 2.4597349166870117\n",
      "Epoch 5/10. Iteration 43300/47167 Losses: train: 2.362104654312134, validate: 2.4543867111206055\n",
      "Epoch 5/10. Iteration 43400/47167 Losses: train: 2.132995843887329, validate: 2.46161150932312\n",
      "Epoch 5/10. Iteration 43500/47167 Losses: train: 2.467599630355835, validate: 2.4546070098876953\n",
      "Epoch 5/10. Iteration 43600/47167 Losses: train: 2.286607503890991, validate: 2.4612326622009277\n",
      "Epoch 5/10. Iteration 43700/47167 Losses: train: 2.216430187225342, validate: 2.47019624710083\n",
      "Epoch 5/10. Iteration 43800/47167 Losses: train: 2.2306900024414062, validate: 2.466447591781616\n",
      "Epoch 5/10. Iteration 43900/47167 Losses: train: 2.215517282485962, validate: 2.456479787826538\n",
      "Epoch 5/10. Iteration 44000/47167 Losses: train: 2.2676548957824707, validate: 2.4639408588409424\n",
      "Epoch 5/10. Iteration 44100/47167 Losses: train: 2.2231552600860596, validate: 2.462303400039673\n",
      "Epoch 5/10. Iteration 44200/47167 Losses: train: 2.412102460861206, validate: 2.4510109424591064\n",
      "Epoch 5/10. Iteration 44300/47167 Losses: train: 2.3327455520629883, validate: 2.466473340988159\n",
      "Epoch 5/10. Iteration 44400/47167 Losses: train: 2.4889185428619385, validate: 2.4583394527435303\n",
      "Epoch 5/10. Iteration 44500/47167 Losses: train: 2.2316837310791016, validate: 2.4473989009857178\n",
      "Epoch 5/10. Iteration 44600/47167 Losses: train: 2.315843105316162, validate: 2.4556784629821777\n",
      "Epoch 5/10. Iteration 44700/47167 Losses: train: 2.385032892227173, validate: 2.460585355758667\n",
      "Epoch 5/10. Iteration 44800/47167 Losses: train: 2.4117534160614014, validate: 2.4593021869659424\n",
      "Epoch 5/10. Iteration 44900/47167 Losses: train: 2.3904552459716797, validate: 2.4604058265686035\n",
      "Epoch 5/10. Iteration 45000/47167 Losses: train: 2.361619234085083, validate: 2.461503744125366\n",
      "Epoch 5/10. Iteration 45100/47167 Losses: train: 2.1772055625915527, validate: 2.460695505142212\n",
      "Epoch 5/10. Iteration 45200/47167 Losses: train: 2.41831374168396, validate: 2.459123134613037\n",
      "Epoch 5/10. Iteration 45300/47167 Losses: train: 2.3097147941589355, validate: 2.471707582473755\n",
      "Epoch 5/10. Iteration 45400/47167 Losses: train: 2.297177314758301, validate: 2.470233917236328\n",
      "Epoch 5/10. Iteration 45500/47167 Losses: train: 2.322667360305786, validate: 2.4490773677825928\n",
      "Epoch 5/10. Iteration 45600/47167 Losses: train: 2.1292569637298584, validate: 2.454787492752075\n",
      "Epoch 5/10. Iteration 45700/47167 Losses: train: 2.207460880279541, validate: 2.4478020668029785\n",
      "Epoch 5/10. Iteration 45800/47167 Losses: train: 2.3812472820281982, validate: 2.4513256549835205\n",
      "Epoch 5/10. Iteration 45900/47167 Losses: train: 2.3852410316467285, validate: 2.4575765132904053\n",
      "Epoch 5/10. Iteration 46000/47167 Losses: train: 2.514936923980713, validate: 2.46134614944458\n",
      "Epoch 5/10. Iteration 46100/47167 Losses: train: 2.189540147781372, validate: 2.452413320541382\n",
      "Epoch 5/10. Iteration 46200/47167 Losses: train: 2.4445502758026123, validate: 2.4511032104492188\n",
      "Epoch 5/10. Iteration 46300/47167 Losses: train: 2.3989765644073486, validate: 2.4587700366973877\n",
      "Epoch 5/10. Iteration 46400/47167 Losses: train: 2.3491315841674805, validate: 2.45182466506958\n",
      "Epoch 5/10. Iteration 46500/47167 Losses: train: 2.317734956741333, validate: 2.4436848163604736\n",
      "Epoch 5/10. Iteration 46600/47167 Losses: train: 2.48826003074646, validate: 2.4609389305114746\n",
      "Epoch 5/10. Iteration 46700/47167 Losses: train: 2.2460715770721436, validate: 2.4614248275756836\n",
      "Epoch 5/10. Iteration 46800/47167 Losses: train: 2.182886838912964, validate: 2.454606771469116\n",
      "Epoch 5/10. Iteration 46900/47167 Losses: train: 2.3085310459136963, validate: 2.4540796279907227\n",
      "Epoch 5/10. Iteration 47000/47167 Losses: train: 2.3506674766540527, validate: 2.4673004150390625\n",
      "Epoch 5/10. Iteration 47100/47167 Losses: train: 2.301330804824829, validate: 2.449629545211792\n",
      "Epoch 6/10. Iteration 100/47167 Losses: train: 2.2165088653564453, validate: 2.451948404312134\n",
      "Epoch 6/10. Iteration 200/47167 Losses: train: 2.283994436264038, validate: 2.4652597904205322\n",
      "Epoch 6/10. Iteration 300/47167 Losses: train: 2.2645962238311768, validate: 2.4650745391845703\n",
      "Epoch 6/10. Iteration 400/47167 Losses: train: 2.4176743030548096, validate: 2.4513468742370605\n",
      "Epoch 6/10. Iteration 500/47167 Losses: train: 2.2664504051208496, validate: 2.455794334411621\n",
      "Epoch 6/10. Iteration 600/47167 Losses: train: 2.1962192058563232, validate: 2.4556071758270264\n",
      "Epoch 6/10. Iteration 700/47167 Losses: train: 2.2076847553253174, validate: 2.457397699356079\n",
      "Epoch 6/10. Iteration 800/47167 Losses: train: 2.1867246627807617, validate: 2.4636287689208984\n",
      "Epoch 6/10. Iteration 900/47167 Losses: train: 2.3373708724975586, validate: 2.4608397483825684\n",
      "Epoch 6/10. Iteration 1000/47167 Losses: train: 2.2802560329437256, validate: 2.455822229385376\n",
      "Epoch 6/10. Iteration 1100/47167 Losses: train: 2.095186233520508, validate: 2.4703288078308105\n",
      "Epoch 6/10. Iteration 1200/47167 Losses: train: 2.1701462268829346, validate: 2.4569356441497803\n",
      "Epoch 6/10. Iteration 1300/47167 Losses: train: 2.395817995071411, validate: 2.4570133686065674\n",
      "Epoch 6/10. Iteration 1400/47167 Losses: train: 2.3369476795196533, validate: 2.466977596282959\n",
      "Epoch 6/10. Iteration 1500/47167 Losses: train: 2.266766309738159, validate: 2.454453945159912\n",
      "Epoch 6/10. Iteration 1600/47167 Losses: train: 2.3087143898010254, validate: 2.452070951461792\n",
      "Epoch 6/10. Iteration 1700/47167 Losses: train: 2.526416301727295, validate: 2.470121383666992\n",
      "Epoch 6/10. Iteration 1800/47167 Losses: train: 2.15590238571167, validate: 2.464592218399048\n",
      "Epoch 6/10. Iteration 1900/47167 Losses: train: 2.213381290435791, validate: 2.456178903579712\n",
      "Epoch 6/10. Iteration 2000/47167 Losses: train: 2.104926347732544, validate: 2.4620392322540283\n",
      "Epoch 6/10. Iteration 2100/47167 Losses: train: 2.4399209022521973, validate: 2.4653255939483643\n",
      "Epoch 6/10. Iteration 2200/47167 Losses: train: 2.3968706130981445, validate: 2.4567878246307373\n",
      "Epoch 6/10. Iteration 2300/47167 Losses: train: 2.287781238555908, validate: 2.4623780250549316\n",
      "Epoch 6/10. Iteration 2400/47167 Losses: train: 2.287691116333008, validate: 2.4566922187805176\n",
      "Epoch 6/10. Iteration 2500/47167 Losses: train: 2.318455696105957, validate: 2.4492642879486084\n",
      "Epoch 6/10. Iteration 2600/47167 Losses: train: 2.3262815475463867, validate: 2.462634325027466\n",
      "Epoch 6/10. Iteration 2700/47167 Losses: train: 2.2822957038879395, validate: 2.4677648544311523\n",
      "Epoch 6/10. Iteration 2800/47167 Losses: train: 2.3070852756500244, validate: 2.4660232067108154\n",
      "Epoch 6/10. Iteration 2900/47167 Losses: train: 2.2534520626068115, validate: 2.4677412509918213\n",
      "Epoch 6/10. Iteration 3000/47167 Losses: train: 2.5160086154937744, validate: 2.4653849601745605\n",
      "Epoch 6/10. Iteration 3100/47167 Losses: train: 2.2193431854248047, validate: 2.4594950675964355\n",
      "Epoch 6/10. Iteration 3200/47167 Losses: train: 2.388631582260132, validate: 2.4660749435424805\n",
      "Epoch 6/10. Iteration 3300/47167 Losses: train: 2.2427172660827637, validate: 2.454427480697632\n",
      "Epoch 6/10. Iteration 3400/47167 Losses: train: 2.4091956615448, validate: 2.4530813694000244\n",
      "Epoch 6/10. Iteration 3500/47167 Losses: train: 2.082670211791992, validate: 2.4607667922973633\n",
      "Epoch 6/10. Iteration 3600/47167 Losses: train: 2.2489006519317627, validate: 2.464378833770752\n",
      "Epoch 6/10. Iteration 3700/47167 Losses: train: 2.0996716022491455, validate: 2.464076519012451\n",
      "Epoch 6/10. Iteration 3800/47167 Losses: train: 2.171884536743164, validate: 2.465761184692383\n",
      "Epoch 6/10. Iteration 3900/47167 Losses: train: 2.0637764930725098, validate: 2.4593093395233154\n",
      "Epoch 6/10. Iteration 4000/47167 Losses: train: 2.246293067932129, validate: 2.452244281768799\n",
      "Epoch 6/10. Iteration 4100/47167 Losses: train: 2.305213212966919, validate: 2.454200267791748\n",
      "Epoch 6/10. Iteration 4200/47167 Losses: train: 2.396381378173828, validate: 2.4659535884857178\n",
      "Epoch 6/10. Iteration 4300/47167 Losses: train: 2.3178083896636963, validate: 2.4619994163513184\n",
      "Epoch 6/10. Iteration 4400/47167 Losses: train: 2.3441948890686035, validate: 2.4610671997070312\n",
      "Epoch 6/10. Iteration 4500/47167 Losses: train: 2.360532522201538, validate: 2.465488910675049\n",
      "Epoch 6/10. Iteration 4600/47167 Losses: train: 2.269599437713623, validate: 2.4686572551727295\n",
      "Epoch 6/10. Iteration 4700/47167 Losses: train: 2.328176259994507, validate: 2.459861993789673\n",
      "Epoch 6/10. Iteration 4800/47167 Losses: train: 2.1376800537109375, validate: 2.4519309997558594\n",
      "Epoch 6/10. Iteration 4900/47167 Losses: train: 2.32147479057312, validate: 2.47149395942688\n",
      "Epoch 6/10. Iteration 5000/47167 Losses: train: 2.212218999862671, validate: 2.4538941383361816\n",
      "Epoch 6/10. Iteration 5100/47167 Losses: train: 2.4579055309295654, validate: 2.459371566772461\n",
      "Epoch 6/10. Iteration 5200/47167 Losses: train: 2.2411844730377197, validate: 2.4579503536224365\n",
      "Epoch 6/10. Iteration 5300/47167 Losses: train: 2.3219492435455322, validate: 2.4602890014648438\n",
      "Epoch 6/10. Iteration 5400/47167 Losses: train: 2.3650991916656494, validate: 2.4430019855499268\n",
      "Epoch 6/10. Iteration 5500/47167 Losses: train: 2.4304401874542236, validate: 2.448758125305176\n",
      "Epoch 6/10. Iteration 5600/47167 Losses: train: 2.2348382472991943, validate: 2.4513561725616455\n",
      "Epoch 6/10. Iteration 5700/47167 Losses: train: 2.4511048793792725, validate: 2.4621403217315674\n",
      "Epoch 6/10. Iteration 5800/47167 Losses: train: 2.2366271018981934, validate: 2.452791452407837\n",
      "Epoch 6/10. Iteration 5900/47167 Losses: train: 2.3329226970672607, validate: 2.446415424346924\n",
      "Epoch 6/10. Iteration 6000/47167 Losses: train: 2.286301612854004, validate: 2.4512507915496826\n",
      "Epoch 6/10. Iteration 6100/47167 Losses: train: 2.201017379760742, validate: 2.4507741928100586\n",
      "Epoch 6/10. Iteration 6200/47167 Losses: train: 2.264491558074951, validate: 2.4413185119628906\n",
      "Epoch 6/10. Iteration 6300/47167 Losses: train: 2.429769277572632, validate: 2.435037136077881\n",
      "Epoch 6/10. Iteration 6400/47167 Losses: train: 2.143524169921875, validate: 2.447105884552002\n",
      "Epoch 6/10. Iteration 6500/47167 Losses: train: 2.2307538986206055, validate: 2.4469153881073\n",
      "Epoch 6/10. Iteration 6600/47167 Losses: train: 2.332217216491699, validate: 2.443455457687378\n",
      "Epoch 6/10. Iteration 6700/47167 Losses: train: 2.214407444000244, validate: 2.44796085357666\n",
      "Epoch 6/10. Iteration 6800/47167 Losses: train: 2.4840662479400635, validate: 2.454979181289673\n",
      "Epoch 6/10. Iteration 6900/47167 Losses: train: 2.5301876068115234, validate: 2.446887254714966\n",
      "Epoch 6/10. Iteration 7000/47167 Losses: train: 2.382866382598877, validate: 2.4441051483154297\n",
      "Epoch 6/10. Iteration 7100/47167 Losses: train: 2.155196189880371, validate: 2.447550058364868\n",
      "Epoch 6/10. Iteration 7200/47167 Losses: train: 2.3666930198669434, validate: 2.4514803886413574\n",
      "Epoch 6/10. Iteration 7300/47167 Losses: train: 2.176316499710083, validate: 2.453894853591919\n",
      "Epoch 6/10. Iteration 7400/47167 Losses: train: 2.2231104373931885, validate: 2.4543001651763916\n",
      "Epoch 6/10. Iteration 7500/47167 Losses: train: 2.2315056324005127, validate: 2.458730459213257\n",
      "Epoch 6/10. Iteration 7600/47167 Losses: train: 2.3635103702545166, validate: 2.4598653316497803\n",
      "Epoch 6/10. Iteration 7700/47167 Losses: train: 2.2447304725646973, validate: 2.454007387161255\n",
      "Epoch 6/10. Iteration 7800/47167 Losses: train: 2.2815587520599365, validate: 2.456061601638794\n",
      "Epoch 6/10. Iteration 7900/47167 Losses: train: 2.2405080795288086, validate: 2.45395827293396\n",
      "Epoch 6/10. Iteration 8000/47167 Losses: train: 2.2671172618865967, validate: 2.4455416202545166\n",
      "Epoch 6/10. Iteration 8100/47167 Losses: train: 2.293013095855713, validate: 2.439136028289795\n",
      "Epoch 6/10. Iteration 8200/47167 Losses: train: 2.2002713680267334, validate: 2.4484360218048096\n",
      "Epoch 6/10. Iteration 8300/47167 Losses: train: 2.4088902473449707, validate: 2.444300889968872\n",
      "Epoch 6/10. Iteration 8400/47167 Losses: train: 2.2397239208221436, validate: 2.4487130641937256\n",
      "Epoch 6/10. Iteration 8500/47167 Losses: train: 2.2360169887542725, validate: 2.4417378902435303\n",
      "Epoch 6/10. Iteration 8600/47167 Losses: train: 2.386857509613037, validate: 2.454181432723999\n",
      "Epoch 6/10. Iteration 8700/47167 Losses: train: 2.1312363147735596, validate: 2.44628643989563\n",
      "Epoch 6/10. Iteration 8800/47167 Losses: train: 2.252668619155884, validate: 2.452768325805664\n",
      "Epoch 6/10. Iteration 8900/47167 Losses: train: 2.2172908782958984, validate: 2.451205253601074\n",
      "Epoch 6/10. Iteration 9000/47167 Losses: train: 2.277559280395508, validate: 2.4554173946380615\n",
      "Epoch 6/10. Iteration 9100/47167 Losses: train: 2.1376571655273438, validate: 2.4468257427215576\n",
      "Epoch 6/10. Iteration 9200/47167 Losses: train: 2.4434430599212646, validate: 2.450000286102295\n",
      "Epoch 6/10. Iteration 9300/47167 Losses: train: 2.2108170986175537, validate: 2.4545094966888428\n",
      "Epoch 6/10. Iteration 9400/47167 Losses: train: 2.227001905441284, validate: 2.44873309135437\n",
      "Epoch 6/10. Iteration 9500/47167 Losses: train: 2.401724338531494, validate: 2.458462715148926\n",
      "Epoch 6/10. Iteration 9600/47167 Losses: train: 2.2694191932678223, validate: 2.4458742141723633\n",
      "Epoch 6/10. Iteration 9700/47167 Losses: train: 2.1631031036376953, validate: 2.4606664180755615\n",
      "Epoch 6/10. Iteration 9800/47167 Losses: train: 2.112122058868408, validate: 2.441200017929077\n",
      "Epoch 6/10. Iteration 9900/47167 Losses: train: 2.3015248775482178, validate: 2.456697702407837\n",
      "Epoch 6/10. Iteration 10000/47167 Losses: train: 2.354996681213379, validate: 2.4489006996154785\n",
      "Epoch 6/10. Iteration 10100/47167 Losses: train: 2.3135321140289307, validate: 2.451162099838257\n",
      "Epoch 6/10. Iteration 10200/47167 Losses: train: 2.3680481910705566, validate: 2.4482526779174805\n",
      "Epoch 6/10. Iteration 10300/47167 Losses: train: 2.3736960887908936, validate: 2.450648069381714\n",
      "Epoch 6/10. Iteration 10400/47167 Losses: train: 2.3177175521850586, validate: 2.450936794281006\n",
      "Epoch 6/10. Iteration 10500/47167 Losses: train: 2.433831214904785, validate: 2.4461493492126465\n",
      "Epoch 6/10. Iteration 10600/47167 Losses: train: 2.408792018890381, validate: 2.4585657119750977\n",
      "Epoch 6/10. Iteration 10700/47167 Losses: train: 2.2025146484375, validate: 2.443359375\n",
      "Epoch 6/10. Iteration 10800/47167 Losses: train: 2.254424810409546, validate: 2.446058511734009\n",
      "Epoch 6/10. Iteration 10900/47167 Losses: train: 2.2205233573913574, validate: 2.4402248859405518\n",
      "Epoch 6/10. Iteration 11000/47167 Losses: train: 2.2605509757995605, validate: 2.45620059967041\n",
      "Epoch 6/10. Iteration 11100/47167 Losses: train: 2.1839394569396973, validate: 2.437835454940796\n",
      "Epoch 6/10. Iteration 11200/47167 Losses: train: 2.2879867553710938, validate: 2.4587624073028564\n",
      "Epoch 6/10. Iteration 11300/47167 Losses: train: 2.1910057067871094, validate: 2.4516184329986572\n",
      "Epoch 6/10. Iteration 11400/47167 Losses: train: 2.339137315750122, validate: 2.4366416931152344\n",
      "Epoch 6/10. Iteration 11500/47167 Losses: train: 2.282857894897461, validate: 2.4390041828155518\n",
      "Epoch 6/10. Iteration 11600/47167 Losses: train: 2.457223892211914, validate: 2.445577621459961\n",
      "Epoch 6/10. Iteration 11700/47167 Losses: train: 2.1761209964752197, validate: 2.4401586055755615\n",
      "Epoch 6/10. Iteration 11800/47167 Losses: train: 2.261836051940918, validate: 2.4382388591766357\n",
      "Epoch 6/10. Iteration 11900/47167 Losses: train: 2.1853153705596924, validate: 2.4410908222198486\n",
      "Epoch 6/10. Iteration 12000/47167 Losses: train: 2.3132407665252686, validate: 2.43843936920166\n",
      "Epoch 6/10. Iteration 12100/47167 Losses: train: 2.2774624824523926, validate: 2.4553041458129883\n",
      "Epoch 6/10. Iteration 12200/47167 Losses: train: 2.2315621376037598, validate: 2.4393608570098877\n",
      "Epoch 6/10. Iteration 12300/47167 Losses: train: 2.206742286682129, validate: 2.449939012527466\n",
      "Epoch 6/10. Iteration 12400/47167 Losses: train: 2.4418528079986572, validate: 2.4374492168426514\n",
      "Epoch 6/10. Iteration 12500/47167 Losses: train: 2.1758413314819336, validate: 2.446373224258423\n",
      "Epoch 6/10. Iteration 12600/47167 Losses: train: 2.2337327003479004, validate: 2.440586566925049\n",
      "Epoch 6/10. Iteration 12700/47167 Losses: train: 2.373776435852051, validate: 2.436410665512085\n",
      "Epoch 6/10. Iteration 12800/47167 Losses: train: 2.226593494415283, validate: 2.436354637145996\n",
      "Epoch 6/10. Iteration 12900/47167 Losses: train: 2.4586806297302246, validate: 2.4304628372192383\n",
      "Epoch 6/10. Iteration 13000/47167 Losses: train: 2.298219919204712, validate: 2.4600179195404053\n",
      "Epoch 6/10. Iteration 13100/47167 Losses: train: 2.299807071685791, validate: 2.4319655895233154\n",
      "Epoch 6/10. Iteration 13200/47167 Losses: train: 2.1541025638580322, validate: 2.44146466255188\n",
      "Epoch 6/10. Iteration 13300/47167 Losses: train: 2.275989532470703, validate: 2.4417972564697266\n",
      "Epoch 6/10. Iteration 13400/47167 Losses: train: 2.350532054901123, validate: 2.4283549785614014\n",
      "Epoch 6/10. Iteration 13500/47167 Losses: train: 2.25614070892334, validate: 2.4428184032440186\n",
      "Epoch 6/10. Iteration 13600/47167 Losses: train: 2.3484315872192383, validate: 2.435610771179199\n",
      "Epoch 6/10. Iteration 13700/47167 Losses: train: 2.219141960144043, validate: 2.433650255203247\n",
      "Epoch 6/10. Iteration 13800/47167 Losses: train: 2.3895773887634277, validate: 2.4432451725006104\n",
      "Epoch 6/10. Iteration 13900/47167 Losses: train: 2.2929959297180176, validate: 2.4462814331054688\n",
      "Epoch 6/10. Iteration 14000/47167 Losses: train: 2.3412859439849854, validate: 2.4371519088745117\n",
      "Epoch 6/10. Iteration 14100/47167 Losses: train: 2.3775768280029297, validate: 2.442713737487793\n",
      "Epoch 6/10. Iteration 14200/47167 Losses: train: 2.311488628387451, validate: 2.4508748054504395\n",
      "Epoch 6/10. Iteration 14300/47167 Losses: train: 2.1821305751800537, validate: 2.4413130283355713\n",
      "Epoch 6/10. Iteration 14400/47167 Losses: train: 2.573333263397217, validate: 2.443763017654419\n",
      "Epoch 6/10. Iteration 14500/47167 Losses: train: 2.6026952266693115, validate: 2.4458673000335693\n",
      "Epoch 6/10. Iteration 14600/47167 Losses: train: 2.313246726989746, validate: 2.445401191711426\n",
      "Epoch 6/10. Iteration 14700/47167 Losses: train: 2.436295986175537, validate: 2.447932004928589\n",
      "Epoch 6/10. Iteration 14800/47167 Losses: train: 2.4205899238586426, validate: 2.447002649307251\n",
      "Epoch 6/10. Iteration 14900/47167 Losses: train: 2.3932793140411377, validate: 2.441922426223755\n",
      "Epoch 6/10. Iteration 15000/47167 Losses: train: 2.6823856830596924, validate: 2.4500081539154053\n",
      "Epoch 6/10. Iteration 15100/47167 Losses: train: 2.1766374111175537, validate: 2.441213607788086\n",
      "Epoch 6/10. Iteration 15200/47167 Losses: train: 2.3764185905456543, validate: 2.4403200149536133\n",
      "Epoch 6/10. Iteration 15300/47167 Losses: train: 2.188671112060547, validate: 2.4464707374572754\n",
      "Epoch 6/10. Iteration 15400/47167 Losses: train: 2.2971487045288086, validate: 2.4437761306762695\n",
      "Epoch 6/10. Iteration 15500/47167 Losses: train: 2.308316230773926, validate: 2.4422600269317627\n",
      "Epoch 6/10. Iteration 15600/47167 Losses: train: 2.2086021900177, validate: 2.4364817142486572\n",
      "Epoch 6/10. Iteration 15700/47167 Losses: train: 2.2863850593566895, validate: 2.429900646209717\n",
      "Epoch 6/10. Iteration 15800/47167 Losses: train: 2.2798540592193604, validate: 2.4368765354156494\n",
      "Epoch 6/10. Iteration 15900/47167 Losses: train: 2.2469446659088135, validate: 2.4391000270843506\n",
      "Epoch 6/10. Iteration 16000/47167 Losses: train: 2.3035340309143066, validate: 2.4335732460021973\n",
      "Epoch 6/10. Iteration 16100/47167 Losses: train: 2.230246067047119, validate: 2.4454495906829834\n",
      "Epoch 6/10. Iteration 16200/47167 Losses: train: 2.269895553588867, validate: 2.431771993637085\n",
      "Epoch 6/10. Iteration 16300/47167 Losses: train: 2.0683507919311523, validate: 2.440343141555786\n",
      "Epoch 6/10. Iteration 16400/47167 Losses: train: 2.1235363483428955, validate: 2.445786237716675\n",
      "Epoch 6/10. Iteration 16500/47167 Losses: train: 2.1020150184631348, validate: 2.437448263168335\n",
      "Epoch 6/10. Iteration 16600/47167 Losses: train: 2.4003477096557617, validate: 2.4359323978424072\n",
      "Epoch 6/10. Iteration 16700/47167 Losses: train: 2.238389730453491, validate: 2.4430758953094482\n",
      "Epoch 6/10. Iteration 16800/47167 Losses: train: 2.393683671951294, validate: 2.452974796295166\n",
      "Epoch 6/10. Iteration 16900/47167 Losses: train: 2.288137674331665, validate: 2.439361810684204\n",
      "Epoch 6/10. Iteration 17000/47167 Losses: train: 2.283703327178955, validate: 2.4488179683685303\n",
      "Epoch 6/10. Iteration 17100/47167 Losses: train: 2.281642198562622, validate: 2.4435617923736572\n",
      "Epoch 6/10. Iteration 17200/47167 Losses: train: 2.302661657333374, validate: 2.452381134033203\n",
      "Epoch 6/10. Iteration 17300/47167 Losses: train: 2.2496697902679443, validate: 2.442429304122925\n",
      "Epoch 6/10. Iteration 17400/47167 Losses: train: 2.4672839641571045, validate: 2.4498348236083984\n",
      "Epoch 6/10. Iteration 17500/47167 Losses: train: 2.293461322784424, validate: 2.4382436275482178\n",
      "Epoch 6/10. Iteration 17600/47167 Losses: train: 2.2955193519592285, validate: 2.4449098110198975\n",
      "Epoch 6/10. Iteration 17700/47167 Losses: train: 2.3858609199523926, validate: 2.4354746341705322\n",
      "Epoch 6/10. Iteration 17800/47167 Losses: train: 2.1490509510040283, validate: 2.4446184635162354\n",
      "Epoch 6/10. Iteration 17900/47167 Losses: train: 2.2604737281799316, validate: 2.4354186058044434\n",
      "Epoch 6/10. Iteration 18000/47167 Losses: train: 2.1637611389160156, validate: 2.4490249156951904\n",
      "Epoch 6/10. Iteration 18100/47167 Losses: train: 2.122371196746826, validate: 2.448239326477051\n",
      "Epoch 6/10. Iteration 18200/47167 Losses: train: 2.149744749069214, validate: 2.442011594772339\n",
      "Epoch 6/10. Iteration 18300/47167 Losses: train: 2.31758451461792, validate: 2.4468801021575928\n",
      "Epoch 6/10. Iteration 18400/47167 Losses: train: 2.394587516784668, validate: 2.438016891479492\n",
      "Epoch 6/10. Iteration 18500/47167 Losses: train: 2.1628496646881104, validate: 2.4375689029693604\n",
      "Epoch 6/10. Iteration 18600/47167 Losses: train: 2.5892674922943115, validate: 2.44211483001709\n",
      "Epoch 6/10. Iteration 18700/47167 Losses: train: 2.162752389907837, validate: 2.441342830657959\n",
      "Epoch 6/10. Iteration 18800/47167 Losses: train: 2.20194411277771, validate: 2.4446468353271484\n",
      "Epoch 6/10. Iteration 18900/47167 Losses: train: 2.228609561920166, validate: 2.4365057945251465\n",
      "Epoch 6/10. Iteration 19000/47167 Losses: train: 2.262855052947998, validate: 2.4343338012695312\n",
      "Epoch 6/10. Iteration 19100/47167 Losses: train: 2.283949851989746, validate: 2.442967414855957\n",
      "Epoch 6/10. Iteration 19200/47167 Losses: train: 2.318480968475342, validate: 2.4454753398895264\n",
      "Epoch 6/10. Iteration 19300/47167 Losses: train: 2.157072067260742, validate: 2.436739683151245\n",
      "Epoch 6/10. Iteration 19400/47167 Losses: train: 2.2863516807556152, validate: 2.4329535961151123\n",
      "Epoch 6/10. Iteration 19500/47167 Losses: train: 2.154637575149536, validate: 2.4338202476501465\n",
      "Epoch 6/10. Iteration 19600/47167 Losses: train: 2.333935260772705, validate: 2.427792549133301\n",
      "Epoch 6/10. Iteration 19700/47167 Losses: train: 2.329195737838745, validate: 2.4442741870880127\n",
      "Epoch 6/10. Iteration 19800/47167 Losses: train: 2.2844693660736084, validate: 2.432434320449829\n",
      "Epoch 6/10. Iteration 19900/47167 Losses: train: 2.0189127922058105, validate: 2.4323296546936035\n",
      "Epoch 6/10. Iteration 20000/47167 Losses: train: 2.0321903228759766, validate: 2.4425625801086426\n",
      "Epoch 6/10. Iteration 20100/47167 Losses: train: 2.1652097702026367, validate: 2.441277503967285\n",
      "Epoch 6/10. Iteration 20200/47167 Losses: train: 2.2841920852661133, validate: 2.436863422393799\n",
      "Epoch 6/10. Iteration 20300/47167 Losses: train: 2.3517777919769287, validate: 2.4324724674224854\n",
      "Epoch 6/10. Iteration 20400/47167 Losses: train: 2.2303526401519775, validate: 2.4360389709472656\n",
      "Epoch 6/10. Iteration 20500/47167 Losses: train: 2.130300760269165, validate: 2.4445390701293945\n",
      "Epoch 6/10. Iteration 20600/47167 Losses: train: 2.0630156993865967, validate: 2.4324564933776855\n",
      "Epoch 6/10. Iteration 20700/47167 Losses: train: 2.361285448074341, validate: 2.4436464309692383\n",
      "Epoch 6/10. Iteration 20800/47167 Losses: train: 2.322273015975952, validate: 2.43869686126709\n",
      "Epoch 6/10. Iteration 20900/47167 Losses: train: 2.317431688308716, validate: 2.4477150440216064\n",
      "Epoch 6/10. Iteration 21000/47167 Losses: train: 2.2737226486206055, validate: 2.4329237937927246\n",
      "Epoch 6/10. Iteration 21100/47167 Losses: train: 2.3306801319122314, validate: 2.4397904872894287\n",
      "Epoch 6/10. Iteration 21200/47167 Losses: train: 2.2517247200012207, validate: 2.4532244205474854\n",
      "Epoch 6/10. Iteration 21300/47167 Losses: train: 2.295208692550659, validate: 2.4458789825439453\n",
      "Epoch 6/10. Iteration 21400/47167 Losses: train: 2.220623254776001, validate: 2.4410665035247803\n",
      "Epoch 6/10. Iteration 21500/47167 Losses: train: 2.4661078453063965, validate: 2.4374537467956543\n",
      "Epoch 6/10. Iteration 21600/47167 Losses: train: 2.2726762294769287, validate: 2.447100877761841\n",
      "Epoch 6/10. Iteration 21700/47167 Losses: train: 2.1125991344451904, validate: 2.450273275375366\n",
      "Epoch 6/10. Iteration 21800/47167 Losses: train: 2.299856185913086, validate: 2.439812660217285\n",
      "Epoch 6/10. Iteration 21900/47167 Losses: train: 2.23964786529541, validate: 2.450558662414551\n",
      "Epoch 6/10. Iteration 22000/47167 Losses: train: 2.3067433834075928, validate: 2.4471733570098877\n",
      "Epoch 6/10. Iteration 22100/47167 Losses: train: 2.2092456817626953, validate: 2.448809862136841\n",
      "Epoch 6/10. Iteration 22200/47167 Losses: train: 2.2546591758728027, validate: 2.4411470890045166\n",
      "Epoch 6/10. Iteration 22300/47167 Losses: train: 2.242093563079834, validate: 2.4472861289978027\n",
      "Epoch 6/10. Iteration 22400/47167 Losses: train: 2.115478038787842, validate: 2.439530849456787\n",
      "Epoch 6/10. Iteration 22500/47167 Losses: train: 2.254821300506592, validate: 2.4306540489196777\n",
      "Epoch 6/10. Iteration 22600/47167 Losses: train: 2.3983683586120605, validate: 2.451082944869995\n",
      "Epoch 6/10. Iteration 22700/47167 Losses: train: 2.212040662765503, validate: 2.450253486633301\n",
      "Epoch 6/10. Iteration 22800/47167 Losses: train: 2.32175350189209, validate: 2.4442217350006104\n",
      "Epoch 6/10. Iteration 22900/47167 Losses: train: 2.352895498275757, validate: 2.4425253868103027\n",
      "Epoch 6/10. Iteration 23000/47167 Losses: train: 2.2171576023101807, validate: 2.4341788291931152\n",
      "Epoch 6/10. Iteration 23100/47167 Losses: train: 2.2814078330993652, validate: 2.428112506866455\n",
      "Epoch 6/10. Iteration 23200/47167 Losses: train: 2.281371593475342, validate: 2.433234214782715\n",
      "Epoch 6/10. Iteration 23300/47167 Losses: train: 2.309037208557129, validate: 2.440373420715332\n",
      "Epoch 6/10. Iteration 23400/47167 Losses: train: 2.3131916522979736, validate: 2.432959794998169\n",
      "Epoch 6/10. Iteration 23500/47167 Losses: train: 2.2055869102478027, validate: 2.438446044921875\n",
      "Epoch 6/10. Iteration 23600/47167 Losses: train: 2.394075393676758, validate: 2.4392473697662354\n",
      "Epoch 6/10. Iteration 23700/47167 Losses: train: 2.1897337436676025, validate: 2.449789047241211\n",
      "Epoch 6/10. Iteration 23800/47167 Losses: train: 2.345242500305176, validate: 2.442185163497925\n",
      "Epoch 6/10. Iteration 23900/47167 Losses: train: 2.1523256301879883, validate: 2.434321880340576\n",
      "Epoch 6/10. Iteration 24000/47167 Losses: train: 2.393942356109619, validate: 2.426720142364502\n",
      "Epoch 6/10. Iteration 24100/47167 Losses: train: 2.3505866527557373, validate: 2.445366382598877\n",
      "Epoch 6/10. Iteration 24200/47167 Losses: train: 2.0317811965942383, validate: 2.4288899898529053\n",
      "Epoch 6/10. Iteration 24300/47167 Losses: train: 2.29963755607605, validate: 2.4337124824523926\n",
      "Epoch 6/10. Iteration 24400/47167 Losses: train: 2.159261703491211, validate: 2.430410146713257\n",
      "Epoch 6/10. Iteration 24500/47167 Losses: train: 2.112194299697876, validate: 2.4278950691223145\n",
      "Epoch 6/10. Iteration 24600/47167 Losses: train: 2.2269461154937744, validate: 2.4426932334899902\n",
      "Epoch 6/10. Iteration 24700/47167 Losses: train: 2.3098089694976807, validate: 2.437760353088379\n",
      "Epoch 6/10. Iteration 24800/47167 Losses: train: 2.3666200637817383, validate: 2.4336156845092773\n",
      "Epoch 6/10. Iteration 24900/47167 Losses: train: 2.107808828353882, validate: 2.439525842666626\n",
      "Epoch 6/10. Iteration 25000/47167 Losses: train: 2.4042482376098633, validate: 2.433972120285034\n",
      "Epoch 6/10. Iteration 25100/47167 Losses: train: 2.323626756668091, validate: 2.4325358867645264\n",
      "Epoch 6/10. Iteration 25200/47167 Losses: train: 2.395925998687744, validate: 2.4259345531463623\n",
      "Epoch 6/10. Iteration 25300/47167 Losses: train: 2.428210496902466, validate: 2.4371414184570312\n",
      "Epoch 6/10. Iteration 25400/47167 Losses: train: 2.372202157974243, validate: 2.4317500591278076\n",
      "Epoch 6/10. Iteration 25500/47167 Losses: train: 2.122084617614746, validate: 2.43929386138916\n",
      "Epoch 6/10. Iteration 25600/47167 Losses: train: 2.2945404052734375, validate: 2.4245176315307617\n",
      "Epoch 6/10. Iteration 25700/47167 Losses: train: 2.284369468688965, validate: 2.4336647987365723\n",
      "Epoch 6/10. Iteration 25800/47167 Losses: train: 2.3251092433929443, validate: 2.433767318725586\n",
      "Epoch 6/10. Iteration 25900/47167 Losses: train: 2.2618091106414795, validate: 2.439580202102661\n",
      "Epoch 6/10. Iteration 26000/47167 Losses: train: 2.184419631958008, validate: 2.430354595184326\n",
      "Epoch 6/10. Iteration 26100/47167 Losses: train: 2.3655612468719482, validate: 2.436764717102051\n",
      "Epoch 6/10. Iteration 26200/47167 Losses: train: 2.2349512577056885, validate: 2.4291224479675293\n",
      "Epoch 6/10. Iteration 26300/47167 Losses: train: 2.2240381240844727, validate: 2.438814640045166\n",
      "Epoch 6/10. Iteration 26400/47167 Losses: train: 2.3336422443389893, validate: 2.440777540206909\n",
      "Epoch 6/10. Iteration 26500/47167 Losses: train: 2.3357439041137695, validate: 2.438725471496582\n",
      "Epoch 6/10. Iteration 26600/47167 Losses: train: 2.2840051651000977, validate: 2.4268715381622314\n",
      "Epoch 6/10. Iteration 26700/47167 Losses: train: 2.313417911529541, validate: 2.433887243270874\n",
      "Epoch 6/10. Iteration 26800/47167 Losses: train: 2.2212796211242676, validate: 2.427607774734497\n",
      "Epoch 6/10. Iteration 26900/47167 Losses: train: 2.3459205627441406, validate: 2.429948568344116\n",
      "Epoch 6/10. Iteration 27000/47167 Losses: train: 2.3271324634552, validate: 2.437725782394409\n",
      "Epoch 6/10. Iteration 27100/47167 Losses: train: 2.4072349071502686, validate: 2.441093683242798\n",
      "Epoch 6/10. Iteration 27200/47167 Losses: train: 2.2621700763702393, validate: 2.4223556518554688\n",
      "Epoch 6/10. Iteration 27300/47167 Losses: train: 2.2413477897644043, validate: 2.437680959701538\n",
      "Epoch 6/10. Iteration 27400/47167 Losses: train: 2.2261528968811035, validate: 2.437859535217285\n",
      "Epoch 6/10. Iteration 27500/47167 Losses: train: 2.180799961090088, validate: 2.445472002029419\n",
      "Epoch 6/10. Iteration 27600/47167 Losses: train: 2.2079966068267822, validate: 2.4325368404388428\n",
      "Epoch 6/10. Iteration 27700/47167 Losses: train: 2.406705617904663, validate: 2.4440152645111084\n",
      "Epoch 6/10. Iteration 27800/47167 Losses: train: 2.0799808502197266, validate: 2.4485509395599365\n",
      "Epoch 6/10. Iteration 27900/47167 Losses: train: 2.1245288848876953, validate: 2.437253475189209\n",
      "Epoch 6/10. Iteration 28000/47167 Losses: train: 2.248755931854248, validate: 2.44327974319458\n",
      "Epoch 6/10. Iteration 28100/47167 Losses: train: 2.4721596240997314, validate: 2.438838005065918\n",
      "Epoch 6/10. Iteration 28200/47167 Losses: train: 2.293828248977661, validate: 2.438683271408081\n",
      "Epoch 6/10. Iteration 28300/47167 Losses: train: 2.2563936710357666, validate: 2.4433648586273193\n",
      "Epoch 6/10. Iteration 28400/47167 Losses: train: 2.3452401161193848, validate: 2.43803334236145\n",
      "Epoch 6/10. Iteration 28500/47167 Losses: train: 2.3717923164367676, validate: 2.4350881576538086\n",
      "Epoch 6/10. Iteration 28600/47167 Losses: train: 2.224604845046997, validate: 2.4456284046173096\n",
      "Epoch 6/10. Iteration 28700/47167 Losses: train: 2.363048553466797, validate: 2.4393253326416016\n",
      "Epoch 6/10. Iteration 28800/47167 Losses: train: 2.185161828994751, validate: 2.444578170776367\n",
      "Epoch 6/10. Iteration 28900/47167 Losses: train: 2.192763090133667, validate: 2.4438881874084473\n",
      "Epoch 6/10. Iteration 29000/47167 Losses: train: 2.1944329738616943, validate: 2.438236951828003\n",
      "Epoch 6/10. Iteration 29100/47167 Losses: train: 2.281243085861206, validate: 2.434718608856201\n",
      "Epoch 6/10. Iteration 29200/47167 Losses: train: 2.314815044403076, validate: 2.4451215267181396\n",
      "Epoch 6/10. Iteration 29300/47167 Losses: train: 2.4288430213928223, validate: 2.4388649463653564\n",
      "Epoch 6/10. Iteration 29400/47167 Losses: train: 2.112643003463745, validate: 2.4385764598846436\n",
      "Epoch 6/10. Iteration 29500/47167 Losses: train: 2.362804889678955, validate: 2.4340267181396484\n",
      "Epoch 6/10. Iteration 29600/47167 Losses: train: 2.176752805709839, validate: 2.434018611907959\n",
      "Epoch 6/10. Iteration 29700/47167 Losses: train: 2.442068576812744, validate: 2.4374797344207764\n",
      "Epoch 6/10. Iteration 29800/47167 Losses: train: 2.263347625732422, validate: 2.441033363342285\n",
      "Epoch 6/10. Iteration 29900/47167 Losses: train: 2.5350210666656494, validate: 2.437401056289673\n",
      "Epoch 6/10. Iteration 30000/47167 Losses: train: 2.041330099105835, validate: 2.430233955383301\n",
      "Epoch 6/10. Iteration 30100/47167 Losses: train: 2.2179131507873535, validate: 2.4315202236175537\n",
      "Epoch 6/10. Iteration 30200/47167 Losses: train: 2.259875774383545, validate: 2.4288320541381836\n",
      "Epoch 6/10. Iteration 30300/47167 Losses: train: 2.273998737335205, validate: 2.4279167652130127\n",
      "Epoch 6/10. Iteration 30400/47167 Losses: train: 2.3409337997436523, validate: 2.4373488426208496\n",
      "Epoch 6/10. Iteration 30500/47167 Losses: train: 2.3948516845703125, validate: 2.4291276931762695\n",
      "Epoch 6/10. Iteration 30600/47167 Losses: train: 2.2589528560638428, validate: 2.4373111724853516\n",
      "Epoch 6/10. Iteration 30700/47167 Losses: train: 2.064596652984619, validate: 2.436074733734131\n",
      "Epoch 6/10. Iteration 30800/47167 Losses: train: 2.268003463745117, validate: 2.426715612411499\n",
      "Epoch 6/10. Iteration 30900/47167 Losses: train: 2.508279800415039, validate: 2.4327688217163086\n",
      "Epoch 6/10. Iteration 31000/47167 Losses: train: 2.258277654647827, validate: 2.43153977394104\n",
      "Epoch 6/10. Iteration 31100/47167 Losses: train: 2.2231297492980957, validate: 2.4302306175231934\n",
      "Epoch 6/10. Iteration 31200/47167 Losses: train: 2.2437963485717773, validate: 2.4349727630615234\n",
      "Epoch 6/10. Iteration 31300/47167 Losses: train: 2.2135932445526123, validate: 2.4279134273529053\n",
      "Epoch 6/10. Iteration 31400/47167 Losses: train: 2.355844497680664, validate: 2.436612129211426\n",
      "Epoch 6/10. Iteration 31500/47167 Losses: train: 2.2736804485321045, validate: 2.4307234287261963\n",
      "Epoch 6/10. Iteration 31600/47167 Losses: train: 2.177717685699463, validate: 2.4263057708740234\n",
      "Epoch 6/10. Iteration 31700/47167 Losses: train: 2.3773159980773926, validate: 2.433966875076294\n",
      "Epoch 6/10. Iteration 31800/47167 Losses: train: 2.29506254196167, validate: 2.43402361869812\n",
      "Epoch 6/10. Iteration 31900/47167 Losses: train: 2.3560216426849365, validate: 2.4355998039245605\n",
      "Epoch 6/10. Iteration 32000/47167 Losses: train: 2.5484893321990967, validate: 2.426131248474121\n",
      "Epoch 6/10. Iteration 32100/47167 Losses: train: 2.270838499069214, validate: 2.427924394607544\n",
      "Epoch 6/10. Iteration 32200/47167 Losses: train: 2.1833715438842773, validate: 2.4271318912506104\n",
      "Epoch 6/10. Iteration 32300/47167 Losses: train: 2.279129981994629, validate: 2.4320576190948486\n",
      "Epoch 6/10. Iteration 32400/47167 Losses: train: 2.1243245601654053, validate: 2.429157257080078\n",
      "Epoch 6/10. Iteration 32500/47167 Losses: train: 2.272015333175659, validate: 2.4325413703918457\n",
      "Epoch 6/10. Iteration 32600/47167 Losses: train: 2.1563074588775635, validate: 2.4342360496520996\n",
      "Epoch 6/10. Iteration 32700/47167 Losses: train: 2.396090507507324, validate: 2.4431064128875732\n",
      "Epoch 6/10. Iteration 32800/47167 Losses: train: 2.413592576980591, validate: 2.4333953857421875\n",
      "Epoch 6/10. Iteration 32900/47167 Losses: train: 2.3696353435516357, validate: 2.4291832447052\n",
      "Epoch 6/10. Iteration 33000/47167 Losses: train: 2.2883918285369873, validate: 2.431753635406494\n",
      "Epoch 6/10. Iteration 33100/47167 Losses: train: 2.218874931335449, validate: 2.4299914836883545\n",
      "Epoch 6/10. Iteration 33200/47167 Losses: train: 2.240781307220459, validate: 2.429938554763794\n",
      "Epoch 6/10. Iteration 33300/47167 Losses: train: 2.452040433883667, validate: 2.429105520248413\n",
      "Epoch 6/10. Iteration 33400/47167 Losses: train: 2.275355815887451, validate: 2.4206950664520264\n",
      "Epoch 6/10. Iteration 33500/47167 Losses: train: 2.309483289718628, validate: 2.4300224781036377\n",
      "Epoch 6/10. Iteration 33600/47167 Losses: train: 2.394512176513672, validate: 2.425424575805664\n",
      "Epoch 6/10. Iteration 33700/47167 Losses: train: 2.237971544265747, validate: 2.4309897422790527\n",
      "Epoch 6/10. Iteration 33800/47167 Losses: train: 2.367609977722168, validate: 2.432192325592041\n",
      "Epoch 6/10. Iteration 33900/47167 Losses: train: 2.2820067405700684, validate: 2.4364583492279053\n",
      "Epoch 6/10. Iteration 34000/47167 Losses: train: 2.198542594909668, validate: 2.4265692234039307\n",
      "Epoch 6/10. Iteration 34100/47167 Losses: train: 2.344618797302246, validate: 2.4379465579986572\n",
      "Epoch 6/10. Iteration 34200/47167 Losses: train: 2.27191162109375, validate: 2.4278993606567383\n",
      "Epoch 6/10. Iteration 34300/47167 Losses: train: 2.3881540298461914, validate: 2.427936553955078\n",
      "Epoch 6/10. Iteration 34400/47167 Losses: train: 2.1591567993164062, validate: 2.4289026260375977\n",
      "Epoch 6/10. Iteration 34500/47167 Losses: train: 2.151878595352173, validate: 2.4228649139404297\n",
      "Epoch 6/10. Iteration 34600/47167 Losses: train: 2.305752754211426, validate: 2.435746669769287\n",
      "Epoch 6/10. Iteration 34700/47167 Losses: train: 2.3366737365722656, validate: 2.4338316917419434\n",
      "Epoch 6/10. Iteration 34800/47167 Losses: train: 2.5151495933532715, validate: 2.4234983921051025\n",
      "Epoch 6/10. Iteration 34900/47167 Losses: train: 2.0705583095550537, validate: 2.428208351135254\n",
      "Epoch 6/10. Iteration 35000/47167 Losses: train: 2.3702454566955566, validate: 2.435114622116089\n",
      "Epoch 6/10. Iteration 35100/47167 Losses: train: 2.197613477706909, validate: 2.4258227348327637\n",
      "Epoch 6/10. Iteration 35200/47167 Losses: train: 2.398857593536377, validate: 2.425579309463501\n",
      "Epoch 6/10. Iteration 35300/47167 Losses: train: 2.328036069869995, validate: 2.4192771911621094\n",
      "Epoch 6/10. Iteration 35400/47167 Losses: train: 2.176248073577881, validate: 2.4211697578430176\n",
      "Epoch 6/10. Iteration 35500/47167 Losses: train: 2.3825528621673584, validate: 2.4309732913970947\n",
      "Epoch 6/10. Iteration 35600/47167 Losses: train: 2.239631414413452, validate: 2.4373607635498047\n",
      "Epoch 6/10. Iteration 35700/47167 Losses: train: 2.3179433345794678, validate: 2.426248073577881\n",
      "Epoch 6/10. Iteration 35800/47167 Losses: train: 2.243269443511963, validate: 2.4331374168395996\n",
      "Epoch 6/10. Iteration 35900/47167 Losses: train: 2.2852072715759277, validate: 2.4337234497070312\n",
      "Epoch 6/10. Iteration 36000/47167 Losses: train: 2.2559947967529297, validate: 2.4339494705200195\n",
      "Epoch 6/10. Iteration 36100/47167 Losses: train: 2.1838724613189697, validate: 2.424433469772339\n",
      "Epoch 6/10. Iteration 36200/47167 Losses: train: 2.270331621170044, validate: 2.4307899475097656\n",
      "Epoch 6/10. Iteration 36300/47167 Losses: train: 2.324747085571289, validate: 2.447385311126709\n",
      "Epoch 6/10. Iteration 36400/47167 Losses: train: 2.2611939907073975, validate: 2.421384811401367\n",
      "Epoch 6/10. Iteration 36500/47167 Losses: train: 2.3313493728637695, validate: 2.4318313598632812\n",
      "Epoch 6/10. Iteration 36600/47167 Losses: train: 2.220841646194458, validate: 2.4309048652648926\n",
      "Epoch 6/10. Iteration 36700/47167 Losses: train: 2.475436210632324, validate: 2.4354395866394043\n",
      "Epoch 6/10. Iteration 36800/47167 Losses: train: 2.468233108520508, validate: 2.419191598892212\n",
      "Epoch 6/10. Iteration 36900/47167 Losses: train: 2.296736240386963, validate: 2.420614719390869\n",
      "Epoch 6/10. Iteration 37000/47167 Losses: train: 2.1807587146759033, validate: 2.4277217388153076\n",
      "Epoch 6/10. Iteration 37100/47167 Losses: train: 2.224086046218872, validate: 2.421124219894409\n",
      "Epoch 6/10. Iteration 37200/47167 Losses: train: 2.550267219543457, validate: 2.418238639831543\n",
      "Epoch 6/10. Iteration 37300/47167 Losses: train: 2.3876535892486572, validate: 2.421886920928955\n",
      "Epoch 6/10. Iteration 37400/47167 Losses: train: 2.178661823272705, validate: 2.4158036708831787\n",
      "Epoch 6/10. Iteration 37500/47167 Losses: train: 2.2644243240356445, validate: 2.425316572189331\n",
      "Epoch 6/10. Iteration 37600/47167 Losses: train: 2.113718271255493, validate: 2.4196224212646484\n",
      "Epoch 6/10. Iteration 37700/47167 Losses: train: 2.09737491607666, validate: 2.4234414100646973\n",
      "Epoch 6/10. Iteration 37800/47167 Losses: train: 2.254528284072876, validate: 2.418476104736328\n",
      "Epoch 6/10. Iteration 37900/47167 Losses: train: 2.3236236572265625, validate: 2.4183685779571533\n",
      "Epoch 6/10. Iteration 38000/47167 Losses: train: 2.133486747741699, validate: 2.419945240020752\n",
      "Epoch 6/10. Iteration 38100/47167 Losses: train: 2.4135286808013916, validate: 2.4174134731292725\n",
      "Epoch 6/10. Iteration 38200/47167 Losses: train: 2.1714329719543457, validate: 2.4261257648468018\n",
      "Epoch 6/10. Iteration 38300/47167 Losses: train: 2.309433937072754, validate: 2.42901873588562\n",
      "Epoch 6/10. Iteration 38400/47167 Losses: train: 2.38700270652771, validate: 2.4214296340942383\n",
      "Epoch 6/10. Iteration 38500/47167 Losses: train: 2.303067684173584, validate: 2.4171929359436035\n",
      "Epoch 6/10. Iteration 38600/47167 Losses: train: 2.287907838821411, validate: 2.4232466220855713\n",
      "Epoch 6/10. Iteration 38700/47167 Losses: train: 2.020873546600342, validate: 2.4110569953918457\n",
      "Epoch 6/10. Iteration 38800/47167 Losses: train: 2.1631033420562744, validate: 2.4248898029327393\n",
      "Epoch 6/10. Iteration 38900/47167 Losses: train: 2.2854268550872803, validate: 2.420717716217041\n",
      "Epoch 6/10. Iteration 39000/47167 Losses: train: 2.4530699253082275, validate: 2.425750970840454\n",
      "Epoch 6/10. Iteration 39100/47167 Losses: train: 2.1158645153045654, validate: 2.4095892906188965\n",
      "Epoch 6/10. Iteration 39200/47167 Losses: train: 2.2707109451293945, validate: 2.418488025665283\n",
      "Epoch 6/10. Iteration 39300/47167 Losses: train: 2.3550500869750977, validate: 2.416970729827881\n",
      "Epoch 6/10. Iteration 39400/47167 Losses: train: 2.1599974632263184, validate: 2.419987916946411\n",
      "Epoch 6/10. Iteration 39500/47167 Losses: train: 2.3688466548919678, validate: 2.4183101654052734\n",
      "Epoch 6/10. Iteration 39600/47167 Losses: train: 2.1565234661102295, validate: 2.42349910736084\n",
      "Epoch 6/10. Iteration 39700/47167 Losses: train: 2.3764607906341553, validate: 2.404386520385742\n",
      "Epoch 6/10. Iteration 39800/47167 Losses: train: 2.0518856048583984, validate: 2.412975311279297\n",
      "Epoch 6/10. Iteration 39900/47167 Losses: train: 2.318936824798584, validate: 2.408400774002075\n",
      "Epoch 6/10. Iteration 40000/47167 Losses: train: 2.1701462268829346, validate: 2.413482904434204\n",
      "Epoch 6/10. Iteration 40100/47167 Losses: train: 2.3320815563201904, validate: 2.4197707176208496\n",
      "Epoch 6/10. Iteration 40200/47167 Losses: train: 2.2334206104278564, validate: 2.413850784301758\n",
      "Epoch 6/10. Iteration 40300/47167 Losses: train: 2.4257559776306152, validate: 2.42006778717041\n",
      "Epoch 6/10. Iteration 40400/47167 Losses: train: 2.3282101154327393, validate: 2.4192519187927246\n",
      "Epoch 6/10. Iteration 40500/47167 Losses: train: 2.2561185359954834, validate: 2.424104928970337\n",
      "Epoch 6/10. Iteration 40600/47167 Losses: train: 2.3713011741638184, validate: 2.4224233627319336\n",
      "Epoch 6/10. Iteration 40700/47167 Losses: train: 2.3235061168670654, validate: 2.421645402908325\n",
      "Epoch 6/10. Iteration 40800/47167 Losses: train: 2.2695562839508057, validate: 2.423994541168213\n",
      "Epoch 6/10. Iteration 40900/47167 Losses: train: 2.296447515487671, validate: 2.4247219562530518\n",
      "Epoch 6/10. Iteration 41000/47167 Losses: train: 2.223405599594116, validate: 2.429285764694214\n",
      "Epoch 6/10. Iteration 41100/47167 Losses: train: 2.1350977420806885, validate: 2.4354586601257324\n",
      "Epoch 6/10. Iteration 41200/47167 Losses: train: 2.2149457931518555, validate: 2.430285930633545\n",
      "Epoch 6/10. Iteration 41300/47167 Losses: train: 2.241572141647339, validate: 2.4235141277313232\n",
      "Epoch 6/10. Iteration 41400/47167 Losses: train: 2.4187963008880615, validate: 2.421367883682251\n",
      "Epoch 6/10. Iteration 41500/47167 Losses: train: 2.2933335304260254, validate: 2.440218925476074\n",
      "Epoch 6/10. Iteration 41600/47167 Losses: train: 2.403481960296631, validate: 2.4321632385253906\n",
      "Epoch 6/10. Iteration 41700/47167 Losses: train: 2.3337461948394775, validate: 2.426511287689209\n",
      "Epoch 6/10. Iteration 41800/47167 Losses: train: 2.377861976623535, validate: 2.431025505065918\n",
      "Epoch 6/10. Iteration 41900/47167 Losses: train: 2.4384729862213135, validate: 2.4380311965942383\n",
      "Epoch 6/10. Iteration 42000/47167 Losses: train: 2.1476364135742188, validate: 2.4288065433502197\n",
      "Epoch 6/10. Iteration 42100/47167 Losses: train: 2.3367526531219482, validate: 2.4263033866882324\n",
      "Epoch 6/10. Iteration 42200/47167 Losses: train: 2.451159715652466, validate: 2.4263153076171875\n",
      "Epoch 6/10. Iteration 42300/47167 Losses: train: 2.3015549182891846, validate: 2.424041986465454\n",
      "Epoch 6/10. Iteration 42400/47167 Losses: train: 2.2861557006835938, validate: 2.412182092666626\n",
      "Epoch 6/10. Iteration 42500/47167 Losses: train: 2.2688510417938232, validate: 2.423910140991211\n",
      "Epoch 6/10. Iteration 42600/47167 Losses: train: 2.1156716346740723, validate: 2.4276816844940186\n",
      "Epoch 6/10. Iteration 42700/47167 Losses: train: 2.3615078926086426, validate: 2.424888849258423\n",
      "Epoch 6/10. Iteration 42800/47167 Losses: train: 2.2161993980407715, validate: 2.4296512603759766\n",
      "Epoch 6/10. Iteration 42900/47167 Losses: train: 2.363872766494751, validate: 2.4203624725341797\n",
      "Epoch 6/10. Iteration 43000/47167 Losses: train: 2.068635940551758, validate: 2.4130115509033203\n",
      "Epoch 6/10. Iteration 43100/47167 Losses: train: 2.342643976211548, validate: 2.418062448501587\n",
      "Epoch 6/10. Iteration 43200/47167 Losses: train: 2.2495856285095215, validate: 2.428314685821533\n",
      "Epoch 6/10. Iteration 43300/47167 Losses: train: 2.3615269660949707, validate: 2.4284634590148926\n",
      "Epoch 6/10. Iteration 43400/47167 Losses: train: 2.458280563354492, validate: 2.428077220916748\n",
      "Epoch 6/10. Iteration 43500/47167 Losses: train: 2.389105796813965, validate: 2.4357078075408936\n",
      "Epoch 6/10. Iteration 43600/47167 Losses: train: 2.4455175399780273, validate: 2.4235777854919434\n",
      "Epoch 6/10. Iteration 43700/47167 Losses: train: 2.2840957641601562, validate: 2.4269859790802\n",
      "Epoch 6/10. Iteration 43800/47167 Losses: train: 2.422663450241089, validate: 2.4279587268829346\n",
      "Epoch 6/10. Iteration 43900/47167 Losses: train: 2.373030424118042, validate: 2.4336254596710205\n",
      "Epoch 6/10. Iteration 44000/47167 Losses: train: 2.1812551021575928, validate: 2.4366414546966553\n",
      "Epoch 6/10. Iteration 44100/47167 Losses: train: 2.184891700744629, validate: 2.4298391342163086\n",
      "Epoch 6/10. Iteration 44200/47167 Losses: train: 2.3132543563842773, validate: 2.439256429672241\n",
      "Epoch 6/10. Iteration 44300/47167 Losses: train: 2.3506782054901123, validate: 2.424349784851074\n",
      "Epoch 6/10. Iteration 44400/47167 Losses: train: 2.2825427055358887, validate: 2.4334897994995117\n",
      "Epoch 6/10. Iteration 44500/47167 Losses: train: 2.331057548522949, validate: 2.423086166381836\n",
      "Epoch 6/10. Iteration 44600/47167 Losses: train: 2.2519166469573975, validate: 2.4176228046417236\n",
      "Epoch 6/10. Iteration 44700/47167 Losses: train: 2.1697998046875, validate: 2.4261937141418457\n",
      "Epoch 6/10. Iteration 44800/47167 Losses: train: 2.324728012084961, validate: 2.4222888946533203\n",
      "Epoch 6/10. Iteration 44900/47167 Losses: train: 2.346176862716675, validate: 2.433445930480957\n",
      "Epoch 6/10. Iteration 45000/47167 Losses: train: 2.1843533515930176, validate: 2.4221434593200684\n",
      "Epoch 6/10. Iteration 45100/47167 Losses: train: 2.1173455715179443, validate: 2.4141063690185547\n",
      "Epoch 6/10. Iteration 45200/47167 Losses: train: 1.9818034172058105, validate: 2.423826217651367\n",
      "Epoch 6/10. Iteration 45300/47167 Losses: train: 2.3224036693573, validate: 2.4213366508483887\n",
      "Epoch 6/10. Iteration 45400/47167 Losses: train: 2.2627711296081543, validate: 2.434603452682495\n",
      "Epoch 6/10. Iteration 45500/47167 Losses: train: 2.2116081714630127, validate: 2.4365336894989014\n",
      "Epoch 6/10. Iteration 45600/47167 Losses: train: 2.2937827110290527, validate: 2.430434226989746\n",
      "Epoch 6/10. Iteration 45700/47167 Losses: train: 2.368701457977295, validate: 2.421818494796753\n",
      "Epoch 6/10. Iteration 45800/47167 Losses: train: 2.2106337547302246, validate: 2.4132068157196045\n",
      "Epoch 6/10. Iteration 45900/47167 Losses: train: 2.1781201362609863, validate: 2.4204907417297363\n",
      "Epoch 6/10. Iteration 46000/47167 Losses: train: 2.3442718982696533, validate: 2.4124562740325928\n",
      "Epoch 6/10. Iteration 46100/47167 Losses: train: 2.2782251834869385, validate: 2.4239768981933594\n",
      "Epoch 6/10. Iteration 46200/47167 Losses: train: 2.2129201889038086, validate: 2.4226560592651367\n",
      "Epoch 6/10. Iteration 46300/47167 Losses: train: 2.3333494663238525, validate: 2.4145617485046387\n",
      "Epoch 6/10. Iteration 46400/47167 Losses: train: 2.2894184589385986, validate: 2.418901205062866\n",
      "Epoch 6/10. Iteration 46500/47167 Losses: train: 2.408547878265381, validate: 2.4232215881347656\n",
      "Epoch 6/10. Iteration 46600/47167 Losses: train: 2.164804220199585, validate: 2.4155211448669434\n",
      "Epoch 6/10. Iteration 46700/47167 Losses: train: 2.375164270401001, validate: 2.4219439029693604\n",
      "Epoch 6/10. Iteration 46800/47167 Losses: train: 2.3587918281555176, validate: 2.4138123989105225\n",
      "Epoch 6/10. Iteration 46900/47167 Losses: train: 2.134878396987915, validate: 2.4208316802978516\n",
      "Epoch 6/10. Iteration 47000/47167 Losses: train: 2.131861686706543, validate: 2.4188756942749023\n",
      "Epoch 6/10. Iteration 47100/47167 Losses: train: 2.387996196746826, validate: 2.414358139038086\n",
      "Epoch 7/10. Iteration 100/47167 Losses: train: 2.0701956748962402, validate: 2.422175407409668\n",
      "Epoch 7/10. Iteration 200/47167 Losses: train: 2.165886878967285, validate: 2.4242568016052246\n",
      "Epoch 7/10. Iteration 300/47167 Losses: train: 2.248743772506714, validate: 2.4244704246520996\n",
      "Epoch 7/10. Iteration 400/47167 Losses: train: 2.3008646965026855, validate: 2.4137415885925293\n",
      "Epoch 7/10. Iteration 500/47167 Losses: train: 2.0333168506622314, validate: 2.4137492179870605\n",
      "Epoch 7/10. Iteration 600/47167 Losses: train: 2.268057346343994, validate: 2.4114632606506348\n",
      "Epoch 7/10. Iteration 700/47167 Losses: train: 2.2244832515716553, validate: 2.4129726886749268\n",
      "Epoch 7/10. Iteration 800/47167 Losses: train: 2.4180004596710205, validate: 2.4174859523773193\n",
      "Epoch 7/10. Iteration 900/47167 Losses: train: 2.1859612464904785, validate: 2.414315700531006\n",
      "Epoch 7/10. Iteration 1000/47167 Losses: train: 2.2400362491607666, validate: 2.4158451557159424\n",
      "Epoch 7/10. Iteration 1100/47167 Losses: train: 2.247581958770752, validate: 2.4168763160705566\n",
      "Epoch 7/10. Iteration 1200/47167 Losses: train: 2.166034460067749, validate: 2.418591022491455\n",
      "Epoch 7/10. Iteration 1300/47167 Losses: train: 2.2660019397735596, validate: 2.4145212173461914\n",
      "Epoch 7/10. Iteration 1400/47167 Losses: train: 2.0572965145111084, validate: 2.4159090518951416\n",
      "Epoch 7/10. Iteration 1500/47167 Losses: train: 2.1575751304626465, validate: 2.4215381145477295\n",
      "Epoch 7/10. Iteration 1600/47167 Losses: train: 2.1829440593719482, validate: 2.41937518119812\n",
      "Epoch 7/10. Iteration 1700/47167 Losses: train: 2.3931403160095215, validate: 2.4194724559783936\n",
      "Epoch 7/10. Iteration 1800/47167 Losses: train: 2.3817172050476074, validate: 2.4195384979248047\n",
      "Epoch 7/10. Iteration 1900/47167 Losses: train: 2.382658004760742, validate: 2.426151990890503\n",
      "Epoch 7/10. Iteration 2000/47167 Losses: train: 2.2010207176208496, validate: 2.422150135040283\n",
      "Epoch 7/10. Iteration 2100/47167 Losses: train: 1.9734889268875122, validate: 2.422335147857666\n",
      "Epoch 7/10. Iteration 2200/47167 Losses: train: 2.2288339138031006, validate: 2.4226982593536377\n",
      "Epoch 7/10. Iteration 2300/47167 Losses: train: 2.314741849899292, validate: 2.433090925216675\n",
      "Epoch 7/10. Iteration 2400/47167 Losses: train: 2.101060628890991, validate: 2.411186695098877\n",
      "Epoch 7/10. Iteration 2500/47167 Losses: train: 2.297694444656372, validate: 2.4210078716278076\n",
      "Epoch 7/10. Iteration 2600/47167 Losses: train: 2.0975234508514404, validate: 2.4272329807281494\n",
      "Epoch 7/10. Iteration 2700/47167 Losses: train: 2.384169578552246, validate: 2.4248549938201904\n",
      "Epoch 7/10. Iteration 2800/47167 Losses: train: 2.1373801231384277, validate: 2.4113216400146484\n",
      "Epoch 7/10. Iteration 2900/47167 Losses: train: 2.165354013442993, validate: 2.41033673286438\n",
      "Epoch 7/10. Iteration 3000/47167 Losses: train: 2.311923027038574, validate: 2.408053159713745\n",
      "Epoch 7/10. Iteration 3100/47167 Losses: train: 2.3402864933013916, validate: 2.4125826358795166\n",
      "Epoch 7/10. Iteration 3200/47167 Losses: train: 2.3093161582946777, validate: 2.4171416759490967\n",
      "Epoch 7/10. Iteration 3300/47167 Losses: train: 2.322901964187622, validate: 2.4198663234710693\n",
      "Epoch 7/10. Iteration 3400/47167 Losses: train: 2.496060609817505, validate: 2.4185638427734375\n",
      "Epoch 7/10. Iteration 3500/47167 Losses: train: 2.2968502044677734, validate: 2.4133031368255615\n",
      "Epoch 7/10. Iteration 3600/47167 Losses: train: 2.2903270721435547, validate: 2.4259259700775146\n",
      "Epoch 7/10. Iteration 3700/47167 Losses: train: 2.3588831424713135, validate: 2.423194408416748\n",
      "Epoch 7/10. Iteration 3800/47167 Losses: train: 2.0927555561065674, validate: 2.404801607131958\n",
      "Epoch 7/10. Iteration 3900/47167 Losses: train: 2.302114248275757, validate: 2.4122164249420166\n",
      "Epoch 7/10. Iteration 4000/47167 Losses: train: 2.2105698585510254, validate: 2.4136836528778076\n",
      "Epoch 7/10. Iteration 4100/47167 Losses: train: 2.17881441116333, validate: 2.414956569671631\n",
      "Epoch 7/10. Iteration 4200/47167 Losses: train: 2.1920697689056396, validate: 2.4160425662994385\n",
      "Epoch 7/10. Iteration 4300/47167 Losses: train: 2.165153741836548, validate: 2.4221408367156982\n",
      "Epoch 7/10. Iteration 4400/47167 Losses: train: 2.4416322708129883, validate: 2.4096767902374268\n",
      "Epoch 7/10. Iteration 4500/47167 Losses: train: 2.1956241130828857, validate: 2.4212300777435303\n",
      "Epoch 7/10. Iteration 4600/47167 Losses: train: 2.2227461338043213, validate: 2.419793128967285\n",
      "Epoch 7/10. Iteration 4700/47167 Losses: train: 2.2608838081359863, validate: 2.42008900642395\n",
      "Epoch 7/10. Iteration 4800/47167 Losses: train: 2.4490091800689697, validate: 2.4186277389526367\n",
      "Epoch 7/10. Iteration 4900/47167 Losses: train: 2.185210943222046, validate: 2.424577236175537\n",
      "Epoch 7/10. Iteration 5000/47167 Losses: train: 2.3362045288085938, validate: 2.420224905014038\n",
      "Epoch 7/10. Iteration 5100/47167 Losses: train: 2.110536813735962, validate: 2.414292812347412\n",
      "Epoch 7/10. Iteration 5200/47167 Losses: train: 2.354763984680176, validate: 2.4234848022460938\n",
      "Epoch 7/10. Iteration 5300/47167 Losses: train: 2.3718221187591553, validate: 2.41212797164917\n",
      "Epoch 7/10. Iteration 5400/47167 Losses: train: 2.090327262878418, validate: 2.403836727142334\n",
      "Epoch 7/10. Iteration 5500/47167 Losses: train: 2.3162765502929688, validate: 2.4189202785491943\n",
      "Epoch 7/10. Iteration 5600/47167 Losses: train: 2.374152898788452, validate: 2.419114589691162\n",
      "Epoch 7/10. Iteration 5700/47167 Losses: train: 2.0899782180786133, validate: 2.4050045013427734\n",
      "Epoch 7/10. Iteration 5800/47167 Losses: train: 2.3634045124053955, validate: 2.4170408248901367\n",
      "Epoch 7/10. Iteration 5900/47167 Losses: train: 2.2326955795288086, validate: 2.410562038421631\n",
      "Epoch 7/10. Iteration 6000/47167 Losses: train: 2.0941720008850098, validate: 2.41170072555542\n",
      "Epoch 7/10. Iteration 6100/47167 Losses: train: 2.2484686374664307, validate: 2.411165952682495\n",
      "Epoch 7/10. Iteration 6200/47167 Losses: train: 2.1523594856262207, validate: 2.401231288909912\n",
      "Epoch 7/10. Iteration 6300/47167 Losses: train: 2.227485418319702, validate: 2.39271879196167\n",
      "Epoch 7/10. Iteration 6400/47167 Losses: train: 2.1710004806518555, validate: 2.402456283569336\n",
      "Epoch 7/10. Iteration 6500/47167 Losses: train: 2.3391506671905518, validate: 2.3958287239074707\n",
      "Epoch 7/10. Iteration 6600/47167 Losses: train: 2.254025459289551, validate: 2.4155783653259277\n",
      "Epoch 7/10. Iteration 6700/47167 Losses: train: 2.1324715614318848, validate: 2.4003753662109375\n",
      "Epoch 7/10. Iteration 6800/47167 Losses: train: 2.2870211601257324, validate: 2.409550666809082\n",
      "Epoch 7/10. Iteration 6900/47167 Losses: train: 2.2437617778778076, validate: 2.413325786590576\n",
      "Epoch 7/10. Iteration 7000/47167 Losses: train: 2.1230762004852295, validate: 2.417691230773926\n",
      "Epoch 7/10. Iteration 7100/47167 Losses: train: 2.251983642578125, validate: 2.4088101387023926\n",
      "Epoch 7/10. Iteration 7200/47167 Losses: train: 2.3272817134857178, validate: 2.401679515838623\n",
      "Epoch 7/10. Iteration 7300/47167 Losses: train: 2.3310487270355225, validate: 2.418419122695923\n",
      "Epoch 7/10. Iteration 7400/47167 Losses: train: 2.2251763343811035, validate: 2.415055751800537\n",
      "Epoch 7/10. Iteration 7500/47167 Losses: train: 2.211167812347412, validate: 2.412055730819702\n",
      "Epoch 7/10. Iteration 7600/47167 Losses: train: 2.332760810852051, validate: 2.414872646331787\n",
      "Epoch 7/10. Iteration 7700/47167 Losses: train: 2.3515710830688477, validate: 2.4105563163757324\n",
      "Epoch 7/10. Iteration 7800/47167 Losses: train: 2.1894068717956543, validate: 2.413187026977539\n",
      "Epoch 7/10. Iteration 7900/47167 Losses: train: 2.2085580825805664, validate: 2.4170265197753906\n",
      "Epoch 7/10. Iteration 8000/47167 Losses: train: 2.3021597862243652, validate: 2.4167730808258057\n",
      "Epoch 7/10. Iteration 8100/47167 Losses: train: 2.078458786010742, validate: 2.408834457397461\n",
      "Epoch 7/10. Iteration 8200/47167 Losses: train: 2.176950216293335, validate: 2.4083282947540283\n",
      "Epoch 7/10. Iteration 8300/47167 Losses: train: 2.1526501178741455, validate: 2.4102137088775635\n",
      "Epoch 7/10. Iteration 8400/47167 Losses: train: 2.3210675716400146, validate: 2.416163206100464\n",
      "Epoch 7/10. Iteration 8500/47167 Losses: train: 2.190868854522705, validate: 2.3993237018585205\n",
      "Epoch 7/10. Iteration 8600/47167 Losses: train: 2.1537044048309326, validate: 2.40211820602417\n",
      "Epoch 7/10. Iteration 8700/47167 Losses: train: 2.2075371742248535, validate: 2.40248441696167\n",
      "Epoch 7/10. Iteration 8800/47167 Losses: train: 2.3253417015075684, validate: 2.4167745113372803\n",
      "Epoch 7/10. Iteration 8900/47167 Losses: train: 2.1292457580566406, validate: 2.4146780967712402\n",
      "Epoch 7/10. Iteration 9000/47167 Losses: train: 2.2268757820129395, validate: 2.4198477268218994\n",
      "Epoch 7/10. Iteration 9100/47167 Losses: train: 2.266148805618286, validate: 2.4127414226531982\n",
      "Epoch 7/10. Iteration 9200/47167 Losses: train: 2.2561299800872803, validate: 2.410403251647949\n",
      "Epoch 7/10. Iteration 9300/47167 Losses: train: 2.3036892414093018, validate: 2.4041340351104736\n",
      "Epoch 7/10. Iteration 9400/47167 Losses: train: 2.4334847927093506, validate: 2.40647029876709\n",
      "Epoch 7/10. Iteration 9500/47167 Losses: train: 2.2468810081481934, validate: 2.404195785522461\n",
      "Epoch 7/10. Iteration 9600/47167 Losses: train: 2.186739206314087, validate: 2.4125559329986572\n",
      "Epoch 7/10. Iteration 9700/47167 Losses: train: 2.289233446121216, validate: 2.411663055419922\n",
      "Epoch 7/10. Iteration 9800/47167 Losses: train: 2.168738603591919, validate: 2.4047300815582275\n",
      "Epoch 7/10. Iteration 9900/47167 Losses: train: 2.329439163208008, validate: 2.396899461746216\n",
      "Epoch 7/10. Iteration 10000/47167 Losses: train: 2.363990306854248, validate: 2.413222551345825\n",
      "Epoch 7/10. Iteration 10100/47167 Losses: train: 2.267432928085327, validate: 2.4069387912750244\n",
      "Epoch 7/10. Iteration 10200/47167 Losses: train: 2.2167792320251465, validate: 2.406994104385376\n",
      "Epoch 7/10. Iteration 10300/47167 Losses: train: 2.080202102661133, validate: 2.404067277908325\n",
      "Epoch 7/10. Iteration 10400/47167 Losses: train: 2.2773547172546387, validate: 2.397890567779541\n",
      "Epoch 7/10. Iteration 10500/47167 Losses: train: 2.224116086959839, validate: 2.411653518676758\n",
      "Epoch 7/10. Iteration 10600/47167 Losses: train: 2.1225697994232178, validate: 2.409126043319702\n",
      "Epoch 7/10. Iteration 10700/47167 Losses: train: 2.2279934883117676, validate: 2.4014225006103516\n",
      "Epoch 7/10. Iteration 10800/47167 Losses: train: 2.402219772338867, validate: 2.4067211151123047\n",
      "Epoch 7/10. Iteration 10900/47167 Losses: train: 2.105522632598877, validate: 2.409456968307495\n",
      "Epoch 7/10. Iteration 11000/47167 Losses: train: 2.3154754638671875, validate: 2.4087836742401123\n",
      "Epoch 7/10. Iteration 11100/47167 Losses: train: 2.267378807067871, validate: 2.400373697280884\n",
      "Epoch 7/10. Iteration 11200/47167 Losses: train: 2.250056743621826, validate: 2.4107656478881836\n",
      "Epoch 7/10. Iteration 11300/47167 Losses: train: 2.379685878753662, validate: 2.401860237121582\n",
      "Epoch 7/10. Iteration 11400/47167 Losses: train: 2.2565548419952393, validate: 2.415107011795044\n",
      "Epoch 7/10. Iteration 11500/47167 Losses: train: 2.2149200439453125, validate: 2.402841091156006\n",
      "Epoch 7/10. Iteration 11600/47167 Losses: train: 2.4025001525878906, validate: 2.4089410305023193\n",
      "Epoch 7/10. Iteration 11700/47167 Losses: train: 2.0751707553863525, validate: 2.396712303161621\n",
      "Epoch 7/10. Iteration 11800/47167 Losses: train: 2.299485445022583, validate: 2.4072694778442383\n",
      "Epoch 7/10. Iteration 11900/47167 Losses: train: 2.3358733654022217, validate: 2.4103853702545166\n",
      "Epoch 7/10. Iteration 12000/47167 Losses: train: 2.273664712905884, validate: 2.3994715213775635\n",
      "Epoch 7/10. Iteration 12100/47167 Losses: train: 2.2508881092071533, validate: 2.411241292953491\n",
      "Epoch 7/10. Iteration 12200/47167 Losses: train: 2.3449900150299072, validate: 2.4034903049468994\n",
      "Epoch 7/10. Iteration 12300/47167 Losses: train: 2.209522247314453, validate: 2.402836799621582\n",
      "Epoch 7/10. Iteration 12400/47167 Losses: train: 2.1970183849334717, validate: 2.401075839996338\n",
      "Epoch 7/10. Iteration 12500/47167 Losses: train: 2.4170279502868652, validate: 2.40118670463562\n",
      "Epoch 7/10. Iteration 12600/47167 Losses: train: 2.323291063308716, validate: 2.4037253856658936\n",
      "Epoch 7/10. Iteration 12700/47167 Losses: train: 2.2627182006835938, validate: 2.4054160118103027\n",
      "Epoch 7/10. Iteration 12800/47167 Losses: train: 2.2454090118408203, validate: 2.4008281230926514\n",
      "Epoch 7/10. Iteration 12900/47167 Losses: train: 2.2228569984436035, validate: 2.4105641841888428\n",
      "Epoch 7/10. Iteration 13000/47167 Losses: train: 2.227736234664917, validate: 2.398479461669922\n",
      "Epoch 7/10. Iteration 13100/47167 Losses: train: 2.060070753097534, validate: 2.4085135459899902\n",
      "Epoch 7/10. Iteration 13200/47167 Losses: train: 2.3713648319244385, validate: 2.396266460418701\n",
      "Epoch 7/10. Iteration 13300/47167 Losses: train: 2.4017696380615234, validate: 2.4104466438293457\n",
      "Epoch 7/10. Iteration 13400/47167 Losses: train: 2.1337389945983887, validate: 2.4043078422546387\n",
      "Epoch 7/10. Iteration 13500/47167 Losses: train: 2.1894378662109375, validate: 2.403618812561035\n",
      "Epoch 7/10. Iteration 13600/47167 Losses: train: 2.2443530559539795, validate: 2.394923686981201\n",
      "Epoch 7/10. Iteration 13700/47167 Losses: train: 2.2314445972442627, validate: 2.401470184326172\n",
      "Epoch 7/10. Iteration 13800/47167 Losses: train: 2.351024866104126, validate: 2.4056878089904785\n",
      "Epoch 7/10. Iteration 13900/47167 Losses: train: 2.2477099895477295, validate: 2.4002411365509033\n",
      "Epoch 7/10. Iteration 14000/47167 Losses: train: 2.234361410140991, validate: 2.41695499420166\n",
      "Epoch 7/10. Iteration 14100/47167 Losses: train: 2.335341453552246, validate: 2.4125378131866455\n",
      "Epoch 7/10. Iteration 14200/47167 Losses: train: 2.1854782104492188, validate: 2.418158531188965\n",
      "Epoch 7/10. Iteration 14300/47167 Losses: train: 2.1041154861450195, validate: 2.4089760780334473\n",
      "Epoch 7/10. Iteration 14400/47167 Losses: train: 2.262064218521118, validate: 2.4094786643981934\n",
      "Epoch 7/10. Iteration 14500/47167 Losses: train: 2.365499258041382, validate: 2.4051291942596436\n",
      "Epoch 7/10. Iteration 14600/47167 Losses: train: 2.0772411823272705, validate: 2.410968065261841\n",
      "Epoch 7/10. Iteration 14700/47167 Losses: train: 2.051131248474121, validate: 2.4071836471557617\n",
      "Epoch 7/10. Iteration 14800/47167 Losses: train: 2.2415378093719482, validate: 2.4144866466522217\n",
      "Epoch 7/10. Iteration 14900/47167 Losses: train: 2.296050548553467, validate: 2.4005138874053955\n",
      "Epoch 7/10. Iteration 15000/47167 Losses: train: 2.324155807495117, validate: 2.4059298038482666\n",
      "Epoch 7/10. Iteration 15100/47167 Losses: train: 2.2195050716400146, validate: 2.418515920639038\n",
      "Epoch 7/10. Iteration 15200/47167 Losses: train: 2.162245512008667, validate: 2.3986735343933105\n",
      "Epoch 7/10. Iteration 15300/47167 Losses: train: 2.3075218200683594, validate: 2.4184563159942627\n",
      "Epoch 7/10. Iteration 15400/47167 Losses: train: 2.1280620098114014, validate: 2.4072911739349365\n",
      "Epoch 7/10. Iteration 15500/47167 Losses: train: 2.186220645904541, validate: 2.419276237487793\n",
      "Epoch 7/10. Iteration 15600/47167 Losses: train: 2.214149236679077, validate: 2.414008855819702\n",
      "Epoch 7/10. Iteration 15700/47167 Losses: train: 2.3346943855285645, validate: 2.4108657836914062\n",
      "Epoch 7/10. Iteration 15800/47167 Losses: train: 2.2450318336486816, validate: 2.4083728790283203\n",
      "Epoch 7/10. Iteration 15900/47167 Losses: train: 2.3047378063201904, validate: 2.4280214309692383\n",
      "Epoch 7/10. Iteration 16000/47167 Losses: train: 2.2686874866485596, validate: 2.4012763500213623\n",
      "Epoch 7/10. Iteration 16100/47167 Losses: train: 2.272167682647705, validate: 2.4177005290985107\n",
      "Epoch 7/10. Iteration 16200/47167 Losses: train: 2.419579029083252, validate: 2.4050700664520264\n",
      "Epoch 7/10. Iteration 16300/47167 Losses: train: 2.1599555015563965, validate: 2.410557746887207\n",
      "Epoch 7/10. Iteration 16400/47167 Losses: train: 2.224886178970337, validate: 2.4147450923919678\n",
      "Epoch 7/10. Iteration 16500/47167 Losses: train: 2.16713809967041, validate: 2.4171059131622314\n",
      "Epoch 7/10. Iteration 16600/47167 Losses: train: 2.41882061958313, validate: 2.4100544452667236\n",
      "Epoch 7/10. Iteration 16700/47167 Losses: train: 2.4111335277557373, validate: 2.41227388381958\n",
      "Epoch 7/10. Iteration 16800/47167 Losses: train: 2.2146873474121094, validate: 2.406635046005249\n",
      "Epoch 7/10. Iteration 16900/47167 Losses: train: 2.3333444595336914, validate: 2.4167211055755615\n",
      "Epoch 7/10. Iteration 17000/47167 Losses: train: 2.3087148666381836, validate: 2.412935495376587\n",
      "Epoch 7/10. Iteration 17100/47167 Losses: train: 2.288762092590332, validate: 2.4242517948150635\n",
      "Epoch 7/10. Iteration 17200/47167 Losses: train: 2.1781394481658936, validate: 2.4171247482299805\n",
      "Epoch 7/10. Iteration 17300/47167 Losses: train: 2.0856337547302246, validate: 2.4131736755371094\n",
      "Epoch 7/10. Iteration 17400/47167 Losses: train: 2.1557469367980957, validate: 2.408581495285034\n",
      "Epoch 7/10. Iteration 17500/47167 Losses: train: 2.2370731830596924, validate: 2.404730796813965\n",
      "Epoch 7/10. Iteration 17600/47167 Losses: train: 2.2446131706237793, validate: 2.4099652767181396\n",
      "Epoch 7/10. Iteration 17700/47167 Losses: train: 2.3269996643066406, validate: 2.410808563232422\n",
      "Epoch 7/10. Iteration 17800/47167 Losses: train: 2.1873886585235596, validate: 2.4112625122070312\n",
      "Epoch 7/10. Iteration 17900/47167 Losses: train: 2.093148708343506, validate: 2.41375470161438\n",
      "Epoch 7/10. Iteration 18000/47167 Losses: train: 2.3320610523223877, validate: 2.409632921218872\n",
      "Epoch 7/10. Iteration 18100/47167 Losses: train: 2.1007466316223145, validate: 2.402238130569458\n",
      "Epoch 7/10. Iteration 18200/47167 Losses: train: 2.1112937927246094, validate: 2.4141335487365723\n",
      "Epoch 7/10. Iteration 18300/47167 Losses: train: 2.2380669116973877, validate: 2.4095687866210938\n",
      "Epoch 7/10. Iteration 18400/47167 Losses: train: 2.192518949508667, validate: 2.4032175540924072\n",
      "Epoch 7/10. Iteration 18500/47167 Losses: train: 2.140455961227417, validate: 2.4034581184387207\n",
      "Epoch 7/10. Iteration 18600/47167 Losses: train: 2.2571256160736084, validate: 2.415731191635132\n",
      "Epoch 7/10. Iteration 18700/47167 Losses: train: 2.4193227291107178, validate: 2.4033355712890625\n",
      "Epoch 7/10. Iteration 18800/47167 Losses: train: 2.244752883911133, validate: 2.412066698074341\n",
      "Epoch 7/10. Iteration 18900/47167 Losses: train: 2.1273748874664307, validate: 2.4020450115203857\n",
      "Epoch 7/10. Iteration 19000/47167 Losses: train: 2.1899971961975098, validate: 2.392483711242676\n",
      "Epoch 7/10. Iteration 19100/47167 Losses: train: 2.1856536865234375, validate: 2.415933609008789\n",
      "Epoch 7/10. Iteration 19200/47167 Losses: train: 2.0780599117279053, validate: 2.4127776622772217\n",
      "Epoch 7/10. Iteration 19300/47167 Losses: train: 2.0005249977111816, validate: 2.411435842514038\n",
      "Epoch 7/10. Iteration 19400/47167 Losses: train: 2.3314476013183594, validate: 2.402360200881958\n",
      "Epoch 7/10. Iteration 19500/47167 Losses: train: 2.173276662826538, validate: 2.405155658721924\n",
      "Epoch 7/10. Iteration 19600/47167 Losses: train: 2.207237720489502, validate: 2.4084365367889404\n",
      "Epoch 7/10. Iteration 19700/47167 Losses: train: 2.1160664558410645, validate: 2.395685911178589\n",
      "Epoch 7/10. Iteration 19800/47167 Losses: train: 2.0700724124908447, validate: 2.396672248840332\n",
      "Epoch 7/10. Iteration 19900/47167 Losses: train: 2.2741239070892334, validate: 2.4090194702148438\n",
      "Epoch 7/10. Iteration 20000/47167 Losses: train: 2.206937789916992, validate: 2.4007680416107178\n",
      "Epoch 7/10. Iteration 20100/47167 Losses: train: 2.3284757137298584, validate: 2.3899478912353516\n",
      "Epoch 7/10. Iteration 20200/47167 Losses: train: 2.1665544509887695, validate: 2.398991823196411\n",
      "Epoch 7/10. Iteration 20300/47167 Losses: train: 2.1645193099975586, validate: 2.399909257888794\n",
      "Epoch 7/10. Iteration 20400/47167 Losses: train: 2.379878520965576, validate: 2.4098711013793945\n",
      "Epoch 7/10. Iteration 20500/47167 Losses: train: 2.3201894760131836, validate: 2.4056270122528076\n",
      "Epoch 7/10. Iteration 20600/47167 Losses: train: 2.017117500305176, validate: 2.40297269821167\n",
      "Epoch 7/10. Iteration 20700/47167 Losses: train: 2.2656893730163574, validate: 2.3932979106903076\n",
      "Epoch 7/10. Iteration 20800/47167 Losses: train: 2.286104917526245, validate: 2.398325204849243\n",
      "Epoch 7/10. Iteration 20900/47167 Losses: train: 2.117033004760742, validate: 2.399047374725342\n",
      "Epoch 7/10. Iteration 21000/47167 Losses: train: 2.025956392288208, validate: 2.4015517234802246\n",
      "Epoch 7/10. Iteration 21100/47167 Losses: train: 2.3328158855438232, validate: 2.397754430770874\n",
      "Epoch 7/10. Iteration 21200/47167 Losses: train: 2.053262948989868, validate: 2.402501344680786\n",
      "Epoch 7/10. Iteration 21300/47167 Losses: train: 2.234543800354004, validate: 2.397036552429199\n",
      "Epoch 7/10. Iteration 21400/47167 Losses: train: 2.047044038772583, validate: 2.408742666244507\n",
      "Epoch 7/10. Iteration 21500/47167 Losses: train: 2.148254156112671, validate: 2.396805763244629\n",
      "Epoch 7/10. Iteration 21600/47167 Losses: train: 2.2686078548431396, validate: 2.406222343444824\n",
      "Epoch 7/10. Iteration 21700/47167 Losses: train: 2.217999219894409, validate: 2.3997600078582764\n",
      "Epoch 7/10. Iteration 21800/47167 Losses: train: 2.2406375408172607, validate: 2.3934555053710938\n",
      "Epoch 7/10. Iteration 21900/47167 Losses: train: 2.3939199447631836, validate: 2.4199278354644775\n",
      "Epoch 7/10. Iteration 22000/47167 Losses: train: 2.135063886642456, validate: 2.401721715927124\n",
      "Epoch 7/10. Iteration 22100/47167 Losses: train: 2.108898401260376, validate: 2.4076802730560303\n",
      "Epoch 7/10. Iteration 22200/47167 Losses: train: 2.2112488746643066, validate: 2.401143789291382\n",
      "Epoch 7/10. Iteration 22300/47167 Losses: train: 2.1282660961151123, validate: 2.3972771167755127\n",
      "Epoch 7/10. Iteration 22400/47167 Losses: train: 2.4100139141082764, validate: 2.3965141773223877\n",
      "Epoch 7/10. Iteration 22500/47167 Losses: train: 2.1630218029022217, validate: 2.396775722503662\n",
      "Epoch 7/10. Iteration 22600/47167 Losses: train: 2.1826717853546143, validate: 2.4047417640686035\n",
      "Epoch 7/10. Iteration 22700/47167 Losses: train: 2.1066527366638184, validate: 2.393827438354492\n",
      "Epoch 7/10. Iteration 22800/47167 Losses: train: 2.289693832397461, validate: 2.391317367553711\n",
      "Epoch 7/10. Iteration 22900/47167 Losses: train: 2.243725061416626, validate: 2.3868560791015625\n",
      "Epoch 7/10. Iteration 23000/47167 Losses: train: 2.2505385875701904, validate: 2.39274525642395\n",
      "Epoch 7/10. Iteration 23100/47167 Losses: train: 2.2493984699249268, validate: 2.395948886871338\n",
      "Epoch 7/10. Iteration 23200/47167 Losses: train: 2.1838035583496094, validate: 2.394651412963867\n",
      "Epoch 7/10. Iteration 23300/47167 Losses: train: 2.1752164363861084, validate: 2.4054911136627197\n",
      "Epoch 7/10. Iteration 23400/47167 Losses: train: 2.334440231323242, validate: 2.392094612121582\n",
      "Epoch 7/10. Iteration 23500/47167 Losses: train: 2.3180830478668213, validate: 2.403775930404663\n",
      "Epoch 7/10. Iteration 23600/47167 Losses: train: 2.155174493789673, validate: 2.404059410095215\n",
      "Epoch 7/10. Iteration 23700/47167 Losses: train: 2.4329981803894043, validate: 2.398796319961548\n",
      "Epoch 7/10. Iteration 23800/47167 Losses: train: 2.3334240913391113, validate: 2.4085209369659424\n",
      "Epoch 7/10. Iteration 23900/47167 Losses: train: 2.2527811527252197, validate: 2.3978233337402344\n",
      "Epoch 7/10. Iteration 24000/47167 Losses: train: 2.263824462890625, validate: 2.4081172943115234\n",
      "Epoch 7/10. Iteration 24100/47167 Losses: train: 2.4453158378601074, validate: 2.405531167984009\n",
      "Epoch 7/10. Iteration 24200/47167 Losses: train: 2.2230753898620605, validate: 2.39581561088562\n",
      "Epoch 7/10. Iteration 24300/47167 Losses: train: 2.0907082557678223, validate: 2.408590316772461\n",
      "Epoch 7/10. Iteration 24400/47167 Losses: train: 2.338561534881592, validate: 2.4010491371154785\n",
      "Epoch 7/10. Iteration 24500/47167 Losses: train: 2.3917293548583984, validate: 2.3981964588165283\n",
      "Epoch 7/10. Iteration 24600/47167 Losses: train: 2.4235517978668213, validate: 2.404956340789795\n",
      "Epoch 7/10. Iteration 24700/47167 Losses: train: 2.113464593887329, validate: 2.3928332328796387\n",
      "Epoch 7/10. Iteration 24800/47167 Losses: train: 2.2642199993133545, validate: 2.415647029876709\n",
      "Epoch 7/10. Iteration 24900/47167 Losses: train: 2.121154308319092, validate: 2.3985586166381836\n",
      "Epoch 7/10. Iteration 25000/47167 Losses: train: 2.2083311080932617, validate: 2.3958654403686523\n",
      "Epoch 7/10. Iteration 25100/47167 Losses: train: 2.1843907833099365, validate: 2.396289587020874\n",
      "Epoch 7/10. Iteration 25200/47167 Losses: train: 2.1982507705688477, validate: 2.3984673023223877\n",
      "Epoch 7/10. Iteration 25300/47167 Losses: train: 2.1860837936401367, validate: 2.4161500930786133\n",
      "Epoch 7/10. Iteration 25400/47167 Losses: train: 2.1898269653320312, validate: 2.407421350479126\n",
      "Epoch 7/10. Iteration 25500/47167 Losses: train: 2.242825984954834, validate: 2.402475357055664\n",
      "Epoch 7/10. Iteration 25600/47167 Losses: train: 2.1241466999053955, validate: 2.406083583831787\n",
      "Epoch 7/10. Iteration 25700/47167 Losses: train: 2.2052156925201416, validate: 2.4002528190612793\n",
      "Epoch 7/10. Iteration 25800/47167 Losses: train: 2.2934906482696533, validate: 2.4035146236419678\n",
      "Epoch 7/10. Iteration 25900/47167 Losses: train: 2.2327916622161865, validate: 2.398836851119995\n",
      "Epoch 7/10. Iteration 26000/47167 Losses: train: 2.045020818710327, validate: 2.3927059173583984\n",
      "Epoch 7/10. Iteration 26100/47167 Losses: train: 2.3090343475341797, validate: 2.415151596069336\n",
      "Epoch 7/10. Iteration 26200/47167 Losses: train: 2.047240734100342, validate: 2.39754319190979\n",
      "Epoch 7/10. Iteration 26300/47167 Losses: train: 2.123440980911255, validate: 2.3930504322052\n",
      "Epoch 7/10. Iteration 26400/47167 Losses: train: 2.3545331954956055, validate: 2.3900108337402344\n",
      "Epoch 7/10. Iteration 26500/47167 Losses: train: 2.327803373336792, validate: 2.3995914459228516\n",
      "Epoch 7/10. Iteration 26600/47167 Losses: train: 2.2287890911102295, validate: 2.3875601291656494\n",
      "Epoch 7/10. Iteration 26700/47167 Losses: train: 2.3240549564361572, validate: 2.4042437076568604\n",
      "Epoch 7/10. Iteration 26800/47167 Losses: train: 2.456195116043091, validate: 2.400768756866455\n",
      "Epoch 7/10. Iteration 26900/47167 Losses: train: 2.2159504890441895, validate: 2.399782657623291\n",
      "Epoch 7/10. Iteration 27000/47167 Losses: train: 2.181293487548828, validate: 2.3942503929138184\n",
      "Epoch 7/10. Iteration 27100/47167 Losses: train: 2.1291449069976807, validate: 2.401656150817871\n",
      "Epoch 7/10. Iteration 27200/47167 Losses: train: 2.0313756465911865, validate: 2.396328926086426\n",
      "Epoch 7/10. Iteration 27300/47167 Losses: train: 2.2883412837982178, validate: 2.4032795429229736\n",
      "Epoch 7/10. Iteration 27400/47167 Losses: train: 2.1067357063293457, validate: 2.4021809101104736\n",
      "Epoch 7/10. Iteration 27500/47167 Losses: train: 2.304337978363037, validate: 2.404096841812134\n",
      "Epoch 7/10. Iteration 27600/47167 Losses: train: 2.26214337348938, validate: 2.402484893798828\n",
      "Epoch 7/10. Iteration 27700/47167 Losses: train: 2.111701011657715, validate: 2.3943817615509033\n",
      "Epoch 7/10. Iteration 27800/47167 Losses: train: 2.153202772140503, validate: 2.394930839538574\n",
      "Epoch 7/10. Iteration 27900/47167 Losses: train: 2.2266337871551514, validate: 2.3956706523895264\n",
      "Epoch 7/10. Iteration 28000/47167 Losses: train: 2.2830235958099365, validate: 2.3945696353912354\n",
      "Epoch 7/10. Iteration 28100/47167 Losses: train: 2.155376434326172, validate: 2.3937411308288574\n",
      "Epoch 7/10. Iteration 28200/47167 Losses: train: 2.344513416290283, validate: 2.392876148223877\n",
      "Epoch 7/10. Iteration 28300/47167 Losses: train: 2.2310595512390137, validate: 2.3986809253692627\n",
      "Epoch 7/10. Iteration 28400/47167 Losses: train: 2.1993520259857178, validate: 2.3872487545013428\n",
      "Epoch 7/10. Iteration 28500/47167 Losses: train: 2.109344244003296, validate: 2.388104200363159\n",
      "Epoch 7/10. Iteration 28600/47167 Losses: train: 2.2864315509796143, validate: 2.3848347663879395\n",
      "Epoch 7/10. Iteration 28700/47167 Losses: train: 2.366098165512085, validate: 2.39454984664917\n",
      "Epoch 7/10. Iteration 28800/47167 Losses: train: 2.3570425510406494, validate: 2.3909335136413574\n",
      "Epoch 7/10. Iteration 28900/47167 Losses: train: 2.3048293590545654, validate: 2.39520525932312\n",
      "Epoch 7/10. Iteration 29000/47167 Losses: train: 2.247166872024536, validate: 2.383816719055176\n",
      "Epoch 7/10. Iteration 29100/47167 Losses: train: 2.3838448524475098, validate: 2.384011745452881\n",
      "Epoch 7/10. Iteration 29200/47167 Losses: train: 2.264531135559082, validate: 2.3877322673797607\n",
      "Epoch 7/10. Iteration 29300/47167 Losses: train: 2.1977171897888184, validate: 2.393040418624878\n",
      "Epoch 7/10. Iteration 29400/47167 Losses: train: 2.1386783123016357, validate: 2.3961021900177\n",
      "Epoch 7/10. Iteration 29500/47167 Losses: train: 2.3145763874053955, validate: 2.391082763671875\n",
      "Epoch 7/10. Iteration 29600/47167 Losses: train: 2.3708155155181885, validate: 2.404642105102539\n",
      "Epoch 7/10. Iteration 29700/47167 Losses: train: 2.235158920288086, validate: 2.3970203399658203\n",
      "Epoch 7/10. Iteration 29800/47167 Losses: train: 2.1615397930145264, validate: 2.4025378227233887\n",
      "Epoch 7/10. Iteration 29900/47167 Losses: train: 2.118705987930298, validate: 2.405703067779541\n",
      "Epoch 7/10. Iteration 30000/47167 Losses: train: 2.5105369091033936, validate: 2.3972973823547363\n",
      "Epoch 7/10. Iteration 30100/47167 Losses: train: 2.155533790588379, validate: 2.404571771621704\n",
      "Epoch 7/10. Iteration 30200/47167 Losses: train: 2.4127275943756104, validate: 2.3880422115325928\n",
      "Epoch 7/10. Iteration 30300/47167 Losses: train: 2.287519931793213, validate: 2.402698278427124\n",
      "Epoch 7/10. Iteration 30400/47167 Losses: train: 2.3006744384765625, validate: 2.398320198059082\n",
      "Epoch 7/10. Iteration 30500/47167 Losses: train: 2.294849395751953, validate: 2.3936545848846436\n",
      "Epoch 7/10. Iteration 30600/47167 Losses: train: 2.2084357738494873, validate: 2.4020020961761475\n",
      "Epoch 7/10. Iteration 30700/47167 Losses: train: 2.2292966842651367, validate: 2.397557258605957\n",
      "Epoch 7/10. Iteration 30800/47167 Losses: train: 2.0679702758789062, validate: 2.3929927349090576\n",
      "Epoch 7/10. Iteration 30900/47167 Losses: train: 2.308032989501953, validate: 2.397861957550049\n",
      "Epoch 7/10. Iteration 31000/47167 Losses: train: 2.276824712753296, validate: 2.3873045444488525\n",
      "Epoch 7/10. Iteration 31100/47167 Losses: train: 2.4706177711486816, validate: 2.3896143436431885\n",
      "Epoch 7/10. Iteration 31200/47167 Losses: train: 2.111900806427002, validate: 2.39152193069458\n",
      "Epoch 7/10. Iteration 31300/47167 Losses: train: 2.1798839569091797, validate: 2.386706829071045\n",
      "Epoch 7/10. Iteration 31400/47167 Losses: train: 2.144798755645752, validate: 2.3983564376831055\n",
      "Epoch 7/10. Iteration 31500/47167 Losses: train: 2.4202985763549805, validate: 2.386939525604248\n",
      "Epoch 7/10. Iteration 31600/47167 Losses: train: 2.1352968215942383, validate: 2.3963747024536133\n",
      "Epoch 7/10. Iteration 31700/47167 Losses: train: 2.3506078720092773, validate: 2.3777501583099365\n",
      "Epoch 7/10. Iteration 31800/47167 Losses: train: 2.175262928009033, validate: 2.372939348220825\n",
      "Epoch 7/10. Iteration 31900/47167 Losses: train: 2.446612596511841, validate: 2.3928885459899902\n",
      "Epoch 7/10. Iteration 32000/47167 Losses: train: 2.1243631839752197, validate: 2.3879878520965576\n",
      "Epoch 7/10. Iteration 32100/47167 Losses: train: 2.188089370727539, validate: 2.390157461166382\n",
      "Epoch 7/10. Iteration 32200/47167 Losses: train: 2.1878249645233154, validate: 2.3876898288726807\n",
      "Epoch 7/10. Iteration 32300/47167 Losses: train: 2.1826658248901367, validate: 2.3880085945129395\n",
      "Epoch 7/10. Iteration 32400/47167 Losses: train: 2.2139625549316406, validate: 2.389187812805176\n",
      "Epoch 7/10. Iteration 32500/47167 Losses: train: 2.155252695083618, validate: 2.3904857635498047\n",
      "Epoch 7/10. Iteration 32600/47167 Losses: train: 2.4788949489593506, validate: 2.3805723190307617\n",
      "Epoch 7/10. Iteration 32700/47167 Losses: train: 2.175184488296509, validate: 2.3764126300811768\n",
      "Epoch 7/10. Iteration 32800/47167 Losses: train: 2.355015993118286, validate: 2.3838255405426025\n",
      "Epoch 7/10. Iteration 32900/47167 Losses: train: 2.185845136642456, validate: 2.389730453491211\n",
      "Epoch 7/10. Iteration 33000/47167 Losses: train: 2.450040340423584, validate: 2.3843348026275635\n",
      "Epoch 7/10. Iteration 33100/47167 Losses: train: 2.302715539932251, validate: 2.386922836303711\n",
      "Epoch 7/10. Iteration 33200/47167 Losses: train: 2.298104763031006, validate: 2.390826463699341\n",
      "Epoch 7/10. Iteration 33300/47167 Losses: train: 2.2639667987823486, validate: 2.385524272918701\n",
      "Epoch 7/10. Iteration 33400/47167 Losses: train: 2.139127731323242, validate: 2.391667127609253\n",
      "Epoch 7/10. Iteration 33500/47167 Losses: train: 2.1872406005859375, validate: 2.3836324214935303\n",
      "Epoch 7/10. Iteration 33600/47167 Losses: train: 2.2365779876708984, validate: 2.381862163543701\n",
      "Epoch 7/10. Iteration 33700/47167 Losses: train: 1.9852466583251953, validate: 2.380542039871216\n",
      "Epoch 7/10. Iteration 33800/47167 Losses: train: 2.291905641555786, validate: 2.393758535385132\n",
      "Epoch 7/10. Iteration 33900/47167 Losses: train: 2.1725990772247314, validate: 2.3824679851531982\n",
      "Epoch 7/10. Iteration 34000/47167 Losses: train: 2.4775285720825195, validate: 2.3728466033935547\n",
      "Epoch 7/10. Iteration 34100/47167 Losses: train: 2.4430556297302246, validate: 2.378608226776123\n",
      "Epoch 7/10. Iteration 34200/47167 Losses: train: 2.3308308124542236, validate: 2.384404420852661\n",
      "Epoch 7/10. Iteration 34300/47167 Losses: train: 2.226092576980591, validate: 2.385374069213867\n",
      "Epoch 7/10. Iteration 34400/47167 Losses: train: 1.9961490631103516, validate: 2.3822319507598877\n",
      "Epoch 7/10. Iteration 34500/47167 Losses: train: 2.2074086666107178, validate: 2.3820345401763916\n",
      "Epoch 7/10. Iteration 34600/47167 Losses: train: 2.190434455871582, validate: 2.3941640853881836\n",
      "Epoch 7/10. Iteration 34700/47167 Losses: train: 2.1633646488189697, validate: 2.3920607566833496\n",
      "Epoch 7/10. Iteration 34800/47167 Losses: train: 2.1695873737335205, validate: 2.3866121768951416\n",
      "Epoch 7/10. Iteration 34900/47167 Losses: train: 2.2063441276550293, validate: 2.396145820617676\n",
      "Epoch 7/10. Iteration 35000/47167 Losses: train: 2.22656512260437, validate: 2.3820302486419678\n",
      "Epoch 7/10. Iteration 35100/47167 Losses: train: 2.093862771987915, validate: 2.389575719833374\n",
      "Epoch 7/10. Iteration 35200/47167 Losses: train: 2.1784257888793945, validate: 2.3780593872070312\n",
      "Epoch 7/10. Iteration 35300/47167 Losses: train: 2.271657943725586, validate: 2.382280111312866\n",
      "Epoch 7/10. Iteration 35400/47167 Losses: train: 2.215862512588501, validate: 2.388244867324829\n",
      "Epoch 7/10. Iteration 35500/47167 Losses: train: 2.4220011234283447, validate: 2.375542640686035\n",
      "Epoch 7/10. Iteration 35600/47167 Losses: train: 2.2488083839416504, validate: 2.379789113998413\n",
      "Epoch 7/10. Iteration 35700/47167 Losses: train: 2.289945125579834, validate: 2.388829231262207\n",
      "Epoch 7/10. Iteration 35800/47167 Losses: train: 2.244274377822876, validate: 2.3793444633483887\n",
      "Epoch 7/10. Iteration 35900/47167 Losses: train: 2.3153743743896484, validate: 2.387120246887207\n",
      "Epoch 7/10. Iteration 36000/47167 Losses: train: 2.151676654815674, validate: 2.3899312019348145\n",
      "Epoch 7/10. Iteration 36100/47167 Losses: train: 2.1899685859680176, validate: 2.385344982147217\n",
      "Epoch 7/10. Iteration 36200/47167 Losses: train: 2.206498622894287, validate: 2.3770382404327393\n",
      "Epoch 7/10. Iteration 36300/47167 Losses: train: 2.2011008262634277, validate: 2.385815382003784\n",
      "Epoch 7/10. Iteration 36400/47167 Losses: train: 2.362060785293579, validate: 2.3899550437927246\n",
      "Epoch 7/10. Iteration 36500/47167 Losses: train: 2.364180088043213, validate: 2.3911595344543457\n",
      "Epoch 7/10. Iteration 36600/47167 Losses: train: 2.147305488586426, validate: 2.3826260566711426\n",
      "Epoch 7/10. Iteration 36700/47167 Losses: train: 2.0814452171325684, validate: 2.3756911754608154\n",
      "Epoch 7/10. Iteration 36800/47167 Losses: train: 2.235550880432129, validate: 2.379147529602051\n",
      "Epoch 7/10. Iteration 36900/47167 Losses: train: 2.106705665588379, validate: 2.37846040725708\n",
      "Epoch 7/10. Iteration 37000/47167 Losses: train: 2.1113858222961426, validate: 2.379530668258667\n",
      "Epoch 7/10. Iteration 37100/47167 Losses: train: 2.1534321308135986, validate: 2.390265703201294\n",
      "Epoch 7/10. Iteration 37200/47167 Losses: train: 2.410154104232788, validate: 2.3844804763793945\n",
      "Epoch 7/10. Iteration 37300/47167 Losses: train: 2.311798095703125, validate: 2.4001169204711914\n",
      "Epoch 7/10. Iteration 37400/47167 Losses: train: 2.1895291805267334, validate: 2.3926587104797363\n",
      "Epoch 7/10. Iteration 37500/47167 Losses: train: 2.147130250930786, validate: 2.377326726913452\n",
      "Epoch 7/10. Iteration 37600/47167 Losses: train: 2.2029871940612793, validate: 2.388425827026367\n",
      "Epoch 7/10. Iteration 37700/47167 Losses: train: 2.2758970260620117, validate: 2.3917222023010254\n",
      "Epoch 7/10. Iteration 37800/47167 Losses: train: 2.166053295135498, validate: 2.3799943923950195\n",
      "Epoch 7/10. Iteration 37900/47167 Losses: train: 2.2205491065979004, validate: 2.3783535957336426\n",
      "Epoch 7/10. Iteration 38000/47167 Losses: train: 2.3504016399383545, validate: 2.3820905685424805\n",
      "Epoch 7/10. Iteration 38100/47167 Losses: train: 2.290137767791748, validate: 2.386298418045044\n",
      "Epoch 7/10. Iteration 38200/47167 Losses: train: 2.1666665077209473, validate: 2.385056972503662\n",
      "Epoch 7/10. Iteration 38300/47167 Losses: train: 2.294450521469116, validate: 2.3883819580078125\n",
      "Epoch 7/10. Iteration 38400/47167 Losses: train: 2.262284994125366, validate: 2.3915016651153564\n",
      "Epoch 7/10. Iteration 38500/47167 Losses: train: 2.2118639945983887, validate: 2.3931691646575928\n",
      "Epoch 7/10. Iteration 38600/47167 Losses: train: 2.0706887245178223, validate: 2.387686252593994\n",
      "Epoch 7/10. Iteration 38700/47167 Losses: train: 2.201523542404175, validate: 2.3877222537994385\n",
      "Epoch 7/10. Iteration 38800/47167 Losses: train: 2.342865467071533, validate: 2.3948543071746826\n",
      "Epoch 7/10. Iteration 38900/47167 Losses: train: 2.2950704097747803, validate: 2.383592128753662\n",
      "Epoch 7/10. Iteration 39000/47167 Losses: train: 2.25022029876709, validate: 2.3844492435455322\n",
      "Epoch 7/10. Iteration 39100/47167 Losses: train: 2.2542836666107178, validate: 2.379181146621704\n",
      "Epoch 7/10. Iteration 39200/47167 Losses: train: 2.4020330905914307, validate: 2.3836543560028076\n",
      "Epoch 7/10. Iteration 39300/47167 Losses: train: 2.3403539657592773, validate: 2.3909413814544678\n",
      "Epoch 7/10. Iteration 39400/47167 Losses: train: 2.05656361579895, validate: 2.3867878913879395\n",
      "Epoch 7/10. Iteration 39500/47167 Losses: train: 2.185715436935425, validate: 2.3882410526275635\n",
      "Epoch 7/10. Iteration 39600/47167 Losses: train: 2.2776572704315186, validate: 2.3711583614349365\n",
      "Epoch 7/10. Iteration 39700/47167 Losses: train: 2.309046983718872, validate: 2.380908727645874\n",
      "Epoch 7/10. Iteration 39800/47167 Losses: train: 1.9850616455078125, validate: 2.378296136856079\n",
      "Epoch 7/10. Iteration 39900/47167 Losses: train: 2.1656689643859863, validate: 2.383758783340454\n",
      "Epoch 7/10. Iteration 40000/47167 Losses: train: 2.2174952030181885, validate: 2.3748831748962402\n",
      "Epoch 7/10. Iteration 40100/47167 Losses: train: 2.157346487045288, validate: 2.3882176876068115\n",
      "Epoch 7/10. Iteration 40200/47167 Losses: train: 2.1087496280670166, validate: 2.378418207168579\n",
      "Epoch 7/10. Iteration 40300/47167 Losses: train: 2.1590330600738525, validate: 2.3915343284606934\n",
      "Epoch 7/10. Iteration 40400/47167 Losses: train: 2.1805758476257324, validate: 2.384356737136841\n",
      "Epoch 7/10. Iteration 40500/47167 Losses: train: 2.325084686279297, validate: 2.3917062282562256\n",
      "Epoch 7/10. Iteration 40600/47167 Losses: train: 2.096522092819214, validate: 2.3779637813568115\n",
      "Epoch 7/10. Iteration 40700/47167 Losses: train: 2.2548704147338867, validate: 2.3945493698120117\n",
      "Epoch 7/10. Iteration 40800/47167 Losses: train: 2.100935220718384, validate: 2.3774917125701904\n",
      "Epoch 7/10. Iteration 40900/47167 Losses: train: 2.2954654693603516, validate: 2.3889963626861572\n",
      "Epoch 7/10. Iteration 41000/47167 Losses: train: 2.2107462882995605, validate: 2.386678695678711\n",
      "Epoch 7/10. Iteration 41100/47167 Losses: train: 2.1495025157928467, validate: 2.3811817169189453\n",
      "Epoch 7/10. Iteration 41200/47167 Losses: train: 2.354842185974121, validate: 2.3838913440704346\n",
      "Epoch 7/10. Iteration 41300/47167 Losses: train: 2.22727370262146, validate: 2.392320156097412\n",
      "Epoch 7/10. Iteration 41400/47167 Losses: train: 2.1819021701812744, validate: 2.3769261837005615\n",
      "Epoch 7/10. Iteration 41500/47167 Losses: train: 2.33120059967041, validate: 2.39169979095459\n",
      "Epoch 7/10. Iteration 41600/47167 Losses: train: 2.2558517456054688, validate: 2.395911931991577\n",
      "Epoch 7/10. Iteration 41700/47167 Losses: train: 2.239194631576538, validate: 2.376323699951172\n",
      "Epoch 7/10. Iteration 41800/47167 Losses: train: 2.1073107719421387, validate: 2.379457950592041\n",
      "Epoch 7/10. Iteration 41900/47167 Losses: train: 2.469151735305786, validate: 2.381960391998291\n",
      "Epoch 7/10. Iteration 42000/47167 Losses: train: 2.2396857738494873, validate: 2.38565731048584\n",
      "Epoch 7/10. Iteration 42100/47167 Losses: train: 2.1485793590545654, validate: 2.3979151248931885\n",
      "Epoch 7/10. Iteration 42200/47167 Losses: train: 2.2864575386047363, validate: 2.3838040828704834\n",
      "Epoch 7/10. Iteration 42300/47167 Losses: train: 2.3555991649627686, validate: 2.3905632495880127\n",
      "Epoch 7/10. Iteration 42400/47167 Losses: train: 2.2451164722442627, validate: 2.3821773529052734\n",
      "Epoch 7/10. Iteration 42500/47167 Losses: train: 2.1288702487945557, validate: 2.388822078704834\n",
      "Epoch 7/10. Iteration 42600/47167 Losses: train: 2.287729024887085, validate: 2.394113063812256\n",
      "Epoch 7/10. Iteration 42700/47167 Losses: train: 2.084366798400879, validate: 2.3839526176452637\n",
      "Epoch 7/10. Iteration 42800/47167 Losses: train: 2.2558822631835938, validate: 2.392806053161621\n",
      "Epoch 7/10. Iteration 42900/47167 Losses: train: 2.260340929031372, validate: 2.3916242122650146\n",
      "Epoch 7/10. Iteration 43000/47167 Losses: train: 2.204418659210205, validate: 2.3963706493377686\n",
      "Epoch 7/10. Iteration 43100/47167 Losses: train: 2.2515645027160645, validate: 2.3834033012390137\n",
      "Epoch 7/10. Iteration 43200/47167 Losses: train: 2.2951622009277344, validate: 2.393686532974243\n",
      "Epoch 7/10. Iteration 43300/47167 Losses: train: 2.3958754539489746, validate: 2.3884897232055664\n",
      "Epoch 7/10. Iteration 43400/47167 Losses: train: 2.338646173477173, validate: 2.400454521179199\n",
      "Epoch 7/10. Iteration 43500/47167 Losses: train: 2.4368479251861572, validate: 2.402996063232422\n",
      "Epoch 7/10. Iteration 43600/47167 Losses: train: 2.3410561084747314, validate: 2.399610757827759\n",
      "Epoch 7/10. Iteration 43700/47167 Losses: train: 2.3267524242401123, validate: 2.388200283050537\n",
      "Epoch 7/10. Iteration 43800/47167 Losses: train: 2.15012526512146, validate: 2.388334035873413\n",
      "Epoch 7/10. Iteration 43900/47167 Losses: train: 2.0842790603637695, validate: 2.3827178478240967\n",
      "Epoch 7/10. Iteration 44000/47167 Losses: train: 2.370739459991455, validate: 2.384007453918457\n",
      "Epoch 7/10. Iteration 44100/47167 Losses: train: 2.3751351833343506, validate: 2.3892123699188232\n",
      "Epoch 7/10. Iteration 44200/47167 Losses: train: 2.1889829635620117, validate: 2.392221450805664\n",
      "Epoch 7/10. Iteration 44300/47167 Losses: train: 2.27944278717041, validate: 2.382366180419922\n",
      "Epoch 7/10. Iteration 44400/47167 Losses: train: 2.233609914779663, validate: 2.3812968730926514\n",
      "Epoch 7/10. Iteration 44500/47167 Losses: train: 2.07295298576355, validate: 2.3847243785858154\n",
      "Epoch 7/10. Iteration 44600/47167 Losses: train: 2.1260738372802734, validate: 2.3959810733795166\n",
      "Epoch 7/10. Iteration 44700/47167 Losses: train: 2.337116241455078, validate: 2.3844356536865234\n",
      "Epoch 7/10. Iteration 44800/47167 Losses: train: 2.1557865142822266, validate: 2.38556170463562\n",
      "Epoch 7/10. Iteration 44900/47167 Losses: train: 2.1485681533813477, validate: 2.397239923477173\n",
      "Epoch 7/10. Iteration 45000/47167 Losses: train: 2.180131196975708, validate: 2.392058849334717\n",
      "Epoch 7/10. Iteration 45100/47167 Losses: train: 2.407484769821167, validate: 2.3841655254364014\n",
      "Epoch 7/10. Iteration 45200/47167 Losses: train: 2.2193310260772705, validate: 2.3837077617645264\n",
      "Epoch 7/10. Iteration 45300/47167 Losses: train: 2.2640459537506104, validate: 2.393862247467041\n",
      "Epoch 7/10. Iteration 45400/47167 Losses: train: 2.1008687019348145, validate: 2.3760087490081787\n",
      "Epoch 7/10. Iteration 45500/47167 Losses: train: 2.35117769241333, validate: 2.3820455074310303\n",
      "Epoch 7/10. Iteration 45600/47167 Losses: train: 2.0651097297668457, validate: 2.390615940093994\n",
      "Epoch 7/10. Iteration 45700/47167 Losses: train: 2.2075610160827637, validate: 2.3748817443847656\n",
      "Epoch 7/10. Iteration 45800/47167 Losses: train: 2.2647218704223633, validate: 2.38043475151062\n",
      "Epoch 7/10. Iteration 45900/47167 Losses: train: 2.176539421081543, validate: 2.386683702468872\n",
      "Epoch 7/10. Iteration 46000/47167 Losses: train: 2.3794806003570557, validate: 2.38854718208313\n",
      "Epoch 7/10. Iteration 46100/47167 Losses: train: 2.302954912185669, validate: 2.3838329315185547\n",
      "Epoch 7/10. Iteration 46200/47167 Losses: train: 2.1855664253234863, validate: 2.389219284057617\n",
      "Epoch 7/10. Iteration 46300/47167 Losses: train: 2.529341697692871, validate: 2.3808562755584717\n",
      "Epoch 7/10. Iteration 46400/47167 Losses: train: 2.2100839614868164, validate: 2.389176368713379\n",
      "Epoch 7/10. Iteration 46500/47167 Losses: train: 2.2485482692718506, validate: 2.388681650161743\n",
      "Epoch 7/10. Iteration 46600/47167 Losses: train: 2.174137830734253, validate: 2.387410879135132\n",
      "Epoch 7/10. Iteration 46700/47167 Losses: train: 2.3624191284179688, validate: 2.3776988983154297\n",
      "Epoch 7/10. Iteration 46800/47167 Losses: train: 2.000183343887329, validate: 2.3829710483551025\n",
      "Epoch 7/10. Iteration 46900/47167 Losses: train: 2.2398288249969482, validate: 2.386277675628662\n",
      "Epoch 7/10. Iteration 47000/47167 Losses: train: 2.260402202606201, validate: 2.3842172622680664\n",
      "Epoch 7/10. Iteration 47100/47167 Losses: train: 2.0961151123046875, validate: 2.3992576599121094\n",
      "Epoch 8/10. Iteration 100/47167 Losses: train: 2.1365787982940674, validate: 2.391272783279419\n",
      "Epoch 8/10. Iteration 200/47167 Losses: train: 2.0546445846557617, validate: 2.3839125633239746\n",
      "Epoch 8/10. Iteration 300/47167 Losses: train: 2.0461013317108154, validate: 2.3945205211639404\n",
      "Epoch 8/10. Iteration 400/47167 Losses: train: 2.2192800045013428, validate: 2.3936092853546143\n",
      "Epoch 8/10. Iteration 500/47167 Losses: train: 2.147204637527466, validate: 2.397637367248535\n",
      "Epoch 8/10. Iteration 600/47167 Losses: train: 2.2032222747802734, validate: 2.3907766342163086\n",
      "Epoch 8/10. Iteration 700/47167 Losses: train: 2.0771234035491943, validate: 2.384782314300537\n",
      "Epoch 8/10. Iteration 800/47167 Losses: train: 2.1699070930480957, validate: 2.4026949405670166\n",
      "Epoch 8/10. Iteration 900/47167 Losses: train: 2.2280309200286865, validate: 2.3865294456481934\n",
      "Epoch 8/10. Iteration 1000/47167 Losses: train: 2.1867525577545166, validate: 2.38972806930542\n",
      "Epoch 8/10. Iteration 1100/47167 Losses: train: 2.159381866455078, validate: 2.384495735168457\n",
      "Epoch 8/10. Iteration 1200/47167 Losses: train: 2.275561809539795, validate: 2.38649582862854\n",
      "Epoch 8/10. Iteration 1300/47167 Losses: train: 2.2172293663024902, validate: 2.3879592418670654\n",
      "Epoch 8/10. Iteration 1400/47167 Losses: train: 2.1008331775665283, validate: 2.3875274658203125\n",
      "Epoch 8/10. Iteration 1500/47167 Losses: train: 2.158916473388672, validate: 2.3934638500213623\n",
      "Epoch 8/10. Iteration 1600/47167 Losses: train: 2.3055264949798584, validate: 2.3960559368133545\n",
      "Epoch 8/10. Iteration 1700/47167 Losses: train: 2.128502368927002, validate: 2.383002281188965\n",
      "Epoch 8/10. Iteration 1800/47167 Losses: train: 1.900305986404419, validate: 2.3813135623931885\n",
      "Epoch 8/10. Iteration 1900/47167 Losses: train: 2.101653575897217, validate: 2.3855791091918945\n",
      "Epoch 8/10. Iteration 2000/47167 Losses: train: 2.2234842777252197, validate: 2.378253698348999\n",
      "Epoch 8/10. Iteration 2100/47167 Losses: train: 2.228924512863159, validate: 2.3818814754486084\n",
      "Epoch 8/10. Iteration 2200/47167 Losses: train: 2.117133140563965, validate: 2.3872921466827393\n",
      "Epoch 8/10. Iteration 2300/47167 Losses: train: 2.2012760639190674, validate: 2.3970096111297607\n",
      "Epoch 8/10. Iteration 2400/47167 Losses: train: 2.2500288486480713, validate: 2.3929367065429688\n",
      "Epoch 8/10. Iteration 2500/47167 Losses: train: 2.045217752456665, validate: 2.3844354152679443\n",
      "Epoch 8/10. Iteration 2600/47167 Losses: train: 2.144169807434082, validate: 2.3848185539245605\n",
      "Epoch 8/10. Iteration 2700/47167 Losses: train: 2.264069080352783, validate: 2.3964972496032715\n",
      "Epoch 8/10. Iteration 2800/47167 Losses: train: 2.2768430709838867, validate: 2.3914191722869873\n",
      "Epoch 8/10. Iteration 2900/47167 Losses: train: 2.1013846397399902, validate: 2.392184019088745\n",
      "Epoch 8/10. Iteration 3000/47167 Losses: train: 2.313694715499878, validate: 2.395779848098755\n",
      "Epoch 8/10. Iteration 3100/47167 Losses: train: 2.1285006999969482, validate: 2.3886687755584717\n",
      "Epoch 8/10. Iteration 3200/47167 Losses: train: 2.1971466541290283, validate: 2.3917062282562256\n",
      "Epoch 8/10. Iteration 3300/47167 Losses: train: 2.272108793258667, validate: 2.3884150981903076\n",
      "Epoch 8/10. Iteration 3400/47167 Losses: train: 2.0132601261138916, validate: 2.3835043907165527\n",
      "Epoch 8/10. Iteration 3500/47167 Losses: train: 2.1471495628356934, validate: 2.3864235877990723\n",
      "Epoch 8/10. Iteration 3600/47167 Losses: train: 2.0811667442321777, validate: 2.3951029777526855\n",
      "Epoch 8/10. Iteration 3700/47167 Losses: train: 2.1879780292510986, validate: 2.3884871006011963\n",
      "Epoch 8/10. Iteration 3800/47167 Losses: train: 2.2946135997772217, validate: 2.397653102874756\n",
      "Epoch 8/10. Iteration 3900/47167 Losses: train: 2.1524176597595215, validate: 2.384220838546753\n",
      "Epoch 8/10. Iteration 4000/47167 Losses: train: 2.326521158218384, validate: 2.3917407989501953\n",
      "Epoch 8/10. Iteration 4100/47167 Losses: train: 2.149427890777588, validate: 2.3829567432403564\n",
      "Epoch 8/10. Iteration 4200/47167 Losses: train: 2.0533671379089355, validate: 2.3798041343688965\n",
      "Epoch 8/10. Iteration 4300/47167 Losses: train: 2.3302018642425537, validate: 2.3838090896606445\n",
      "Epoch 8/10. Iteration 4400/47167 Losses: train: 2.2153642177581787, validate: 2.39180588722229\n",
      "Epoch 8/10. Iteration 4500/47167 Losses: train: 2.1560494899749756, validate: 2.3875434398651123\n",
      "Epoch 8/10. Iteration 4600/47167 Losses: train: 2.1470470428466797, validate: 2.3864455223083496\n",
      "Epoch 8/10. Iteration 4700/47167 Losses: train: 2.113748550415039, validate: 2.3891539573669434\n",
      "Epoch 8/10. Iteration 4800/47167 Losses: train: 2.2655932903289795, validate: 2.3939695358276367\n",
      "Epoch 8/10. Iteration 4900/47167 Losses: train: 2.0765368938446045, validate: 2.389002799987793\n",
      "Epoch 8/10. Iteration 5000/47167 Losses: train: 2.1431922912597656, validate: 2.3894784450531006\n",
      "Epoch 8/10. Iteration 5100/47167 Losses: train: 2.1895358562469482, validate: 2.3762693405151367\n",
      "Epoch 8/10. Iteration 5200/47167 Losses: train: 2.021517276763916, validate: 2.387852191925049\n",
      "Epoch 8/10. Iteration 5300/47167 Losses: train: 2.2424545288085938, validate: 2.3915045261383057\n",
      "Epoch 8/10. Iteration 5400/47167 Losses: train: 2.1695921421051025, validate: 2.389373779296875\n",
      "Epoch 8/10. Iteration 5500/47167 Losses: train: 2.142557144165039, validate: 2.3823697566986084\n",
      "Epoch 8/10. Iteration 5600/47167 Losses: train: 2.152893304824829, validate: 2.3842039108276367\n",
      "Epoch 8/10. Iteration 5700/47167 Losses: train: 2.1133010387420654, validate: 2.3916015625\n",
      "Epoch 8/10. Iteration 5800/47167 Losses: train: 2.1084494590759277, validate: 2.385427236557007\n",
      "Epoch 8/10. Iteration 5900/47167 Losses: train: 2.1014909744262695, validate: 2.382890224456787\n",
      "Epoch 8/10. Iteration 6000/47167 Losses: train: 2.3022561073303223, validate: 2.382216691970825\n",
      "Epoch 8/10. Iteration 6100/47167 Losses: train: 2.0102438926696777, validate: 2.379424810409546\n",
      "Epoch 8/10. Iteration 6200/47167 Losses: train: 2.096745014190674, validate: 2.3844668865203857\n",
      "Epoch 8/10. Iteration 6300/47167 Losses: train: 2.2210233211517334, validate: 2.3804988861083984\n",
      "Epoch 8/10. Iteration 6400/47167 Losses: train: 2.160829782485962, validate: 2.3861136436462402\n",
      "Epoch 8/10. Iteration 6500/47167 Losses: train: 2.1714460849761963, validate: 2.380267381668091\n",
      "Epoch 8/10. Iteration 6600/47167 Losses: train: 2.235560894012451, validate: 2.378559112548828\n",
      "Epoch 8/10. Iteration 6700/47167 Losses: train: 2.268725633621216, validate: 2.384359359741211\n",
      "Epoch 8/10. Iteration 6800/47167 Losses: train: 2.1299283504486084, validate: 2.380082130432129\n",
      "Epoch 8/10. Iteration 6900/47167 Losses: train: 2.0419397354125977, validate: 2.380540609359741\n",
      "Epoch 8/10. Iteration 7000/47167 Losses: train: 2.1516916751861572, validate: 2.3801233768463135\n",
      "Epoch 8/10. Iteration 7100/47167 Losses: train: 2.204881191253662, validate: 2.383586883544922\n",
      "Epoch 8/10. Iteration 7200/47167 Losses: train: 2.3579812049865723, validate: 2.3839807510375977\n",
      "Epoch 8/10. Iteration 7300/47167 Losses: train: 2.1116645336151123, validate: 2.378527879714966\n",
      "Epoch 8/10. Iteration 7400/47167 Losses: train: 2.116377115249634, validate: 2.388868570327759\n",
      "Epoch 8/10. Iteration 7500/47167 Losses: train: 2.2459676265716553, validate: 2.394427537918091\n",
      "Epoch 8/10. Iteration 7600/47167 Losses: train: 2.157864809036255, validate: 2.4001338481903076\n",
      "Epoch 8/10. Iteration 7700/47167 Losses: train: 2.157559871673584, validate: 2.3934249877929688\n",
      "Epoch 8/10. Iteration 7800/47167 Losses: train: 2.2224652767181396, validate: 2.382272481918335\n",
      "Epoch 8/10. Iteration 7900/47167 Losses: train: 2.186743974685669, validate: 2.3898513317108154\n",
      "Epoch 8/10. Iteration 8000/47167 Losses: train: 2.2639358043670654, validate: 2.3867688179016113\n",
      "Epoch 8/10. Iteration 8100/47167 Losses: train: 2.0007317066192627, validate: 2.3815999031066895\n",
      "Epoch 8/10. Iteration 8200/47167 Losses: train: 2.3689115047454834, validate: 2.371105194091797\n",
      "Epoch 8/10. Iteration 8300/47167 Losses: train: 2.3632254600524902, validate: 2.387840986251831\n",
      "Epoch 8/10. Iteration 8400/47167 Losses: train: 2.244378089904785, validate: 2.3907430171966553\n",
      "Epoch 8/10. Iteration 8500/47167 Losses: train: 2.2122418880462646, validate: 2.3800785541534424\n",
      "Epoch 8/10. Iteration 8600/47167 Losses: train: 2.3077073097229004, validate: 2.3911187648773193\n",
      "Epoch 8/10. Iteration 8700/47167 Losses: train: 2.3263702392578125, validate: 2.38307785987854\n",
      "Epoch 8/10. Iteration 8800/47167 Losses: train: 2.196465015411377, validate: 2.3815934658050537\n",
      "Epoch 8/10. Iteration 8900/47167 Losses: train: 2.1215357780456543, validate: 2.3762519359588623\n",
      "Epoch 8/10. Iteration 9000/47167 Losses: train: 2.162630796432495, validate: 2.3838820457458496\n",
      "Epoch 8/10. Iteration 9100/47167 Losses: train: 2.207833766937256, validate: 2.398146390914917\n",
      "Epoch 8/10. Iteration 9200/47167 Losses: train: 2.236698627471924, validate: 2.3764262199401855\n",
      "Epoch 8/10. Iteration 9300/47167 Losses: train: 2.275393486022949, validate: 2.3727169036865234\n",
      "Epoch 8/10. Iteration 9400/47167 Losses: train: 2.1761438846588135, validate: 2.3814244270324707\n",
      "Epoch 8/10. Iteration 9500/47167 Losses: train: 2.0905511379241943, validate: 2.38257098197937\n",
      "Epoch 8/10. Iteration 9600/47167 Losses: train: 2.161797285079956, validate: 2.384294033050537\n",
      "Epoch 8/10. Iteration 9700/47167 Losses: train: 2.209925413131714, validate: 2.396641731262207\n",
      "Epoch 8/10. Iteration 9800/47167 Losses: train: 2.253523349761963, validate: 2.374030351638794\n",
      "Epoch 8/10. Iteration 9900/47167 Losses: train: 2.254897117614746, validate: 2.3880608081817627\n",
      "Epoch 8/10. Iteration 10000/47167 Losses: train: 2.0636115074157715, validate: 2.3892550468444824\n",
      "Epoch 8/10. Iteration 10100/47167 Losses: train: 2.285356283187866, validate: 2.387657642364502\n",
      "Epoch 8/10. Iteration 10200/47167 Losses: train: 2.31893253326416, validate: 2.3868191242218018\n",
      "Epoch 8/10. Iteration 10300/47167 Losses: train: 2.107020378112793, validate: 2.3901352882385254\n",
      "Epoch 8/10. Iteration 10400/47167 Losses: train: 2.180420398712158, validate: 2.3782830238342285\n",
      "Epoch 8/10. Iteration 10500/47167 Losses: train: 2.3741567134857178, validate: 2.383903741836548\n",
      "Epoch 8/10. Iteration 10600/47167 Losses: train: 2.1534643173217773, validate: 2.3759701251983643\n",
      "Epoch 8/10. Iteration 10700/47167 Losses: train: 2.188159704208374, validate: 2.3651366233825684\n",
      "Epoch 8/10. Iteration 10800/47167 Losses: train: 2.161592960357666, validate: 2.385830879211426\n",
      "Epoch 8/10. Iteration 10900/47167 Losses: train: 2.1970534324645996, validate: 2.3826606273651123\n",
      "Epoch 8/10. Iteration 11000/47167 Losses: train: 2.1146254539489746, validate: 2.3911235332489014\n",
      "Epoch 8/10. Iteration 11100/47167 Losses: train: 2.2839577198028564, validate: 2.3768160343170166\n",
      "Epoch 8/10. Iteration 11200/47167 Losses: train: 2.091193199157715, validate: 2.3713817596435547\n",
      "Epoch 8/10. Iteration 11300/47167 Losses: train: 2.023526906967163, validate: 2.374713659286499\n",
      "Epoch 8/10. Iteration 11400/47167 Losses: train: 2.093256711959839, validate: 2.3841938972473145\n",
      "Epoch 8/10. Iteration 11500/47167 Losses: train: 2.4036099910736084, validate: 2.3826398849487305\n",
      "Epoch 8/10. Iteration 11600/47167 Losses: train: 2.0839452743530273, validate: 2.3806564807891846\n",
      "Epoch 8/10. Iteration 11700/47167 Losses: train: 2.1423959732055664, validate: 2.379913330078125\n",
      "Epoch 8/10. Iteration 11800/47167 Losses: train: 2.1657633781433105, validate: 2.3879616260528564\n",
      "Epoch 8/10. Iteration 11900/47167 Losses: train: 2.1327123641967773, validate: 2.36771297454834\n",
      "Epoch 8/10. Iteration 12000/47167 Losses: train: 2.3124375343322754, validate: 2.3786537647247314\n",
      "Epoch 8/10. Iteration 12100/47167 Losses: train: 2.2943785190582275, validate: 2.378150463104248\n",
      "Epoch 8/10. Iteration 12200/47167 Losses: train: 2.0947892665863037, validate: 2.382932662963867\n",
      "Epoch 8/10. Iteration 12300/47167 Losses: train: 2.1217949390411377, validate: 2.368373394012451\n",
      "Epoch 8/10. Iteration 12400/47167 Losses: train: 2.121542453765869, validate: 2.380800485610962\n",
      "Epoch 8/10. Iteration 12500/47167 Losses: train: 2.1988871097564697, validate: 2.395434617996216\n",
      "Epoch 8/10. Iteration 12600/47167 Losses: train: 2.169828176498413, validate: 2.382438898086548\n",
      "Epoch 8/10. Iteration 12700/47167 Losses: train: 2.227950096130371, validate: 2.3753163814544678\n",
      "Epoch 8/10. Iteration 12800/47167 Losses: train: 2.3911049365997314, validate: 2.3804309368133545\n",
      "Epoch 8/10. Iteration 12900/47167 Losses: train: 2.2185471057891846, validate: 2.3744311332702637\n",
      "Epoch 8/10. Iteration 13000/47167 Losses: train: 2.296165943145752, validate: 2.3841021060943604\n",
      "Epoch 8/10. Iteration 13100/47167 Losses: train: 2.2327723503112793, validate: 2.3740198612213135\n",
      "Epoch 8/10. Iteration 13200/47167 Losses: train: 2.204468011856079, validate: 2.379155158996582\n",
      "Epoch 8/10. Iteration 13300/47167 Losses: train: 2.150360345840454, validate: 2.3700029850006104\n",
      "Epoch 8/10. Iteration 13400/47167 Losses: train: 2.4115381240844727, validate: 2.370145797729492\n",
      "Epoch 8/10. Iteration 13500/47167 Losses: train: 2.18048095703125, validate: 2.370314359664917\n",
      "Epoch 8/10. Iteration 13600/47167 Losses: train: 2.216851234436035, validate: 2.389202356338501\n",
      "Epoch 8/10. Iteration 13700/47167 Losses: train: 2.164363145828247, validate: 2.371403694152832\n",
      "Epoch 8/10. Iteration 13800/47167 Losses: train: 2.1311087608337402, validate: 2.3794631958007812\n",
      "Epoch 8/10. Iteration 13900/47167 Losses: train: 2.2205722332000732, validate: 2.381155014038086\n",
      "Epoch 8/10. Iteration 14000/47167 Losses: train: 2.3525655269622803, validate: 2.3741884231567383\n",
      "Epoch 8/10. Iteration 14100/47167 Losses: train: 2.0229573249816895, validate: 2.376375675201416\n",
      "Epoch 8/10. Iteration 14200/47167 Losses: train: 2.272226572036743, validate: 2.3811705112457275\n",
      "Epoch 8/10. Iteration 14300/47167 Losses: train: 2.1325035095214844, validate: 2.383967161178589\n",
      "Epoch 8/10. Iteration 14400/47167 Losses: train: 2.389742136001587, validate: 2.3680036067962646\n",
      "Epoch 8/10. Iteration 14500/47167 Losses: train: 2.3153817653656006, validate: 2.3814806938171387\n",
      "Epoch 8/10. Iteration 14600/47167 Losses: train: 1.9650245904922485, validate: 2.371171712875366\n",
      "Epoch 8/10. Iteration 14700/47167 Losses: train: 2.267212152481079, validate: 2.3894574642181396\n",
      "Epoch 8/10. Iteration 14800/47167 Losses: train: 2.2860796451568604, validate: 2.3792459964752197\n",
      "Epoch 8/10. Iteration 14900/47167 Losses: train: 2.3960325717926025, validate: 2.381770133972168\n",
      "Epoch 8/10. Iteration 15000/47167 Losses: train: 2.2217354774475098, validate: 2.3811581134796143\n",
      "Epoch 8/10. Iteration 15100/47167 Losses: train: 2.2078018188476562, validate: 2.3805675506591797\n",
      "Epoch 8/10. Iteration 15200/47167 Losses: train: 2.3479931354522705, validate: 2.377441167831421\n",
      "Epoch 8/10. Iteration 15300/47167 Losses: train: 2.1256721019744873, validate: 2.3831443786621094\n",
      "Epoch 8/10. Iteration 15400/47167 Losses: train: 2.1963589191436768, validate: 2.369600534439087\n",
      "Epoch 8/10. Iteration 15500/47167 Losses: train: 2.295330047607422, validate: 2.3801653385162354\n",
      "Epoch 8/10. Iteration 15600/47167 Losses: train: 2.237035036087036, validate: 2.3770110607147217\n",
      "Epoch 8/10. Iteration 15700/47167 Losses: train: 2.2152445316314697, validate: 2.388848066329956\n",
      "Epoch 8/10. Iteration 15800/47167 Losses: train: 2.1473090648651123, validate: 2.3912134170532227\n",
      "Epoch 8/10. Iteration 15900/47167 Losses: train: 2.1158382892608643, validate: 2.3753621578216553\n",
      "Epoch 8/10. Iteration 16000/47167 Losses: train: 2.1147329807281494, validate: 2.393004894256592\n",
      "Epoch 8/10. Iteration 16100/47167 Losses: train: 2.345520496368408, validate: 2.382716178894043\n",
      "Epoch 8/10. Iteration 16200/47167 Losses: train: 2.1225314140319824, validate: 2.3792645931243896\n",
      "Epoch 8/10. Iteration 16300/47167 Losses: train: 2.278275489807129, validate: 2.380248785018921\n",
      "Epoch 8/10. Iteration 16400/47167 Losses: train: 2.238293409347534, validate: 2.3767788410186768\n",
      "Epoch 8/10. Iteration 16500/47167 Losses: train: 2.167738437652588, validate: 2.3836233615875244\n",
      "Epoch 8/10. Iteration 16600/47167 Losses: train: 2.257200241088867, validate: 2.3742363452911377\n",
      "Epoch 8/10. Iteration 16700/47167 Losses: train: 2.197249412536621, validate: 2.3908464908599854\n",
      "Epoch 8/10. Iteration 16800/47167 Losses: train: 2.1484484672546387, validate: 2.381735324859619\n",
      "Epoch 8/10. Iteration 16900/47167 Losses: train: 2.1296143531799316, validate: 2.3775181770324707\n",
      "Epoch 8/10. Iteration 17000/47167 Losses: train: 2.207753896713257, validate: 2.377228021621704\n",
      "Epoch 8/10. Iteration 17100/47167 Losses: train: 2.204477071762085, validate: 2.3763086795806885\n",
      "Epoch 8/10. Iteration 17200/47167 Losses: train: 2.009732723236084, validate: 2.3814632892608643\n",
      "Epoch 8/10. Iteration 17300/47167 Losses: train: 2.2440662384033203, validate: 2.383885145187378\n",
      "Epoch 8/10. Iteration 17400/47167 Losses: train: 2.0562398433685303, validate: 2.3704543113708496\n",
      "Epoch 8/10. Iteration 17500/47167 Losses: train: 2.2606542110443115, validate: 2.376509666442871\n",
      "Epoch 8/10. Iteration 17600/47167 Losses: train: 2.184572696685791, validate: 2.381193161010742\n",
      "Epoch 8/10. Iteration 17700/47167 Losses: train: 2.1649835109710693, validate: 2.383667469024658\n",
      "Epoch 8/10. Iteration 17800/47167 Losses: train: 2.0519022941589355, validate: 2.3728973865509033\n",
      "Epoch 8/10. Iteration 17900/47167 Losses: train: 2.2205355167388916, validate: 2.3799331188201904\n",
      "Epoch 8/10. Iteration 18000/47167 Losses: train: 2.197969436645508, validate: 2.367138147354126\n",
      "Epoch 8/10. Iteration 18100/47167 Losses: train: 2.0357818603515625, validate: 2.3699567317962646\n",
      "Epoch 8/10. Iteration 18200/47167 Losses: train: 2.255587577819824, validate: 2.3808648586273193\n",
      "Epoch 8/10. Iteration 18300/47167 Losses: train: 2.105733871459961, validate: 2.3824667930603027\n",
      "Epoch 8/10. Iteration 18400/47167 Losses: train: 2.331857442855835, validate: 2.374852418899536\n",
      "Epoch 8/10. Iteration 18500/47167 Losses: train: 2.0127408504486084, validate: 2.3829588890075684\n",
      "Epoch 8/10. Iteration 18600/47167 Losses: train: 2.2480368614196777, validate: 2.384714126586914\n",
      "Epoch 8/10. Iteration 18700/47167 Losses: train: 2.1158342361450195, validate: 2.3803632259368896\n",
      "Epoch 8/10. Iteration 18800/47167 Losses: train: 2.1812539100646973, validate: 2.3668715953826904\n",
      "Epoch 8/10. Iteration 18900/47167 Losses: train: 2.111846685409546, validate: 2.3948585987091064\n",
      "Epoch 8/10. Iteration 19000/47167 Losses: train: 2.3409640789031982, validate: 2.3769137859344482\n",
      "Epoch 8/10. Iteration 19100/47167 Losses: train: 1.9765019416809082, validate: 2.373812675476074\n",
      "Epoch 8/10. Iteration 19200/47167 Losses: train: 2.158087968826294, validate: 2.3831610679626465\n",
      "Epoch 8/10. Iteration 19300/47167 Losses: train: 1.945350170135498, validate: 2.3690621852874756\n",
      "Epoch 8/10. Iteration 19400/47167 Losses: train: 2.127936601638794, validate: 2.3737757205963135\n",
      "Epoch 8/10. Iteration 19500/47167 Losses: train: 2.149909496307373, validate: 2.3705925941467285\n",
      "Epoch 8/10. Iteration 19600/47167 Losses: train: 2.14133620262146, validate: 2.380019426345825\n",
      "Epoch 8/10. Iteration 19700/47167 Losses: train: 2.1630074977874756, validate: 2.3761043548583984\n",
      "Epoch 8/10. Iteration 19800/47167 Losses: train: 2.1788861751556396, validate: 2.3705556392669678\n",
      "Epoch 8/10. Iteration 19900/47167 Losses: train: 2.3714938163757324, validate: 2.367016553878784\n",
      "Epoch 8/10. Iteration 20000/47167 Losses: train: 2.2935471534729004, validate: 2.3668317794799805\n",
      "Epoch 8/10. Iteration 20100/47167 Losses: train: 2.0398306846618652, validate: 2.366302490234375\n",
      "Epoch 8/10. Iteration 20200/47167 Losses: train: 2.3922102451324463, validate: 2.370739698410034\n",
      "Epoch 8/10. Iteration 20300/47167 Losses: train: 2.1492767333984375, validate: 2.3836631774902344\n",
      "Epoch 8/10. Iteration 20400/47167 Losses: train: 2.178262948989868, validate: 2.377000093460083\n",
      "Epoch 8/10. Iteration 20500/47167 Losses: train: 2.1624996662139893, validate: 2.3709657192230225\n",
      "Epoch 8/10. Iteration 20600/47167 Losses: train: 2.3536322116851807, validate: 2.374668836593628\n",
      "Epoch 8/10. Iteration 20700/47167 Losses: train: 2.2661523818969727, validate: 2.371860980987549\n",
      "Epoch 8/10. Iteration 20800/47167 Losses: train: 2.0714223384857178, validate: 2.385986566543579\n",
      "Epoch 8/10. Iteration 20900/47167 Losses: train: 2.256092071533203, validate: 2.359985589981079\n",
      "Epoch 8/10. Iteration 21000/47167 Losses: train: 2.2655251026153564, validate: 2.377702236175537\n",
      "Epoch 8/10. Iteration 21100/47167 Losses: train: 2.220325469970703, validate: 2.3732247352600098\n",
      "Epoch 8/10. Iteration 21200/47167 Losses: train: 2.0973238945007324, validate: 2.3749754428863525\n",
      "Epoch 8/10. Iteration 21300/47167 Losses: train: 2.1996209621429443, validate: 2.3665785789489746\n",
      "Epoch 8/10. Iteration 21400/47167 Losses: train: 2.063826084136963, validate: 2.368732213973999\n",
      "Epoch 8/10. Iteration 21500/47167 Losses: train: 2.1807188987731934, validate: 2.3745925426483154\n",
      "Epoch 8/10. Iteration 21600/47167 Losses: train: 2.1692397594451904, validate: 2.369757652282715\n",
      "Epoch 8/10. Iteration 21700/47167 Losses: train: 2.2426819801330566, validate: 2.3787717819213867\n",
      "Epoch 8/10. Iteration 21800/47167 Losses: train: 2.161191463470459, validate: 2.369706392288208\n",
      "Epoch 8/10. Iteration 21900/47167 Losses: train: 2.202310800552368, validate: 2.375372886657715\n",
      "Epoch 8/10. Iteration 22000/47167 Losses: train: 2.0609240531921387, validate: 2.367105722427368\n",
      "Epoch 8/10. Iteration 22100/47167 Losses: train: 2.1784722805023193, validate: 2.3783020973205566\n",
      "Epoch 8/10. Iteration 22200/47167 Losses: train: 2.2038040161132812, validate: 2.3857076168060303\n",
      "Epoch 8/10. Iteration 22300/47167 Losses: train: 2.15596866607666, validate: 2.3750557899475098\n",
      "Epoch 8/10. Iteration 22400/47167 Losses: train: 2.0549817085266113, validate: 2.383390188217163\n",
      "Epoch 8/10. Iteration 22500/47167 Losses: train: 2.077803373336792, validate: 2.378209352493286\n",
      "Epoch 8/10. Iteration 22600/47167 Losses: train: 2.0221009254455566, validate: 2.384009838104248\n",
      "Epoch 8/10. Iteration 22700/47167 Losses: train: 2.1775803565979004, validate: 2.3762879371643066\n",
      "Epoch 8/10. Iteration 22800/47167 Losses: train: 2.2901101112365723, validate: 2.3807547092437744\n",
      "Epoch 8/10. Iteration 22900/47167 Losses: train: 2.0076851844787598, validate: 2.3851048946380615\n",
      "Epoch 8/10. Iteration 23000/47167 Losses: train: 2.3314239978790283, validate: 2.383810043334961\n",
      "Epoch 8/10. Iteration 23100/47167 Losses: train: 2.1550452709198, validate: 2.384810447692871\n",
      "Epoch 8/10. Iteration 23200/47167 Losses: train: 2.224884033203125, validate: 2.3792519569396973\n",
      "Epoch 8/10. Iteration 23300/47167 Losses: train: 2.150520086288452, validate: 2.377861499786377\n",
      "Epoch 8/10. Iteration 23400/47167 Losses: train: 2.1327669620513916, validate: 2.385571241378784\n",
      "Epoch 8/10. Iteration 23500/47167 Losses: train: 2.3382246494293213, validate: 2.3843817710876465\n",
      "Epoch 8/10. Iteration 23600/47167 Losses: train: 2.2674877643585205, validate: 2.380413770675659\n",
      "Epoch 8/10. Iteration 23700/47167 Losses: train: 2.266071081161499, validate: 2.3738138675689697\n",
      "Epoch 8/10. Iteration 23800/47167 Losses: train: 2.1862120628356934, validate: 2.372068405151367\n",
      "Epoch 8/10. Iteration 23900/47167 Losses: train: 2.4084527492523193, validate: 2.3769755363464355\n",
      "Epoch 8/10. Iteration 24000/47167 Losses: train: 2.1764914989471436, validate: 2.386101007461548\n",
      "Epoch 8/10. Iteration 24100/47167 Losses: train: 2.1858973503112793, validate: 2.378338575363159\n",
      "Epoch 8/10. Iteration 24200/47167 Losses: train: 2.2500386238098145, validate: 2.3842217922210693\n",
      "Epoch 8/10. Iteration 24300/47167 Losses: train: 2.15492582321167, validate: 2.3682265281677246\n",
      "Epoch 8/10. Iteration 24400/47167 Losses: train: 2.2321717739105225, validate: 2.3772644996643066\n",
      "Epoch 8/10. Iteration 24500/47167 Losses: train: 2.3100504875183105, validate: 2.3907711505889893\n",
      "Epoch 8/10. Iteration 24600/47167 Losses: train: 2.2581264972686768, validate: 2.385864496231079\n",
      "Epoch 8/10. Iteration 24700/47167 Losses: train: 2.2119204998016357, validate: 2.381423234939575\n",
      "Epoch 8/10. Iteration 24800/47167 Losses: train: 2.2531697750091553, validate: 2.3860158920288086\n",
      "Epoch 8/10. Iteration 24900/47167 Losses: train: 2.274937391281128, validate: 2.3932204246520996\n",
      "Epoch 8/10. Iteration 25000/47167 Losses: train: 2.0456247329711914, validate: 2.3875906467437744\n",
      "Epoch 8/10. Iteration 25100/47167 Losses: train: 2.23380970954895, validate: 2.3806402683258057\n",
      "Epoch 8/10. Iteration 25200/47167 Losses: train: 2.2299604415893555, validate: 2.3778393268585205\n",
      "Epoch 8/10. Iteration 25300/47167 Losses: train: 2.1875765323638916, validate: 2.381831645965576\n",
      "Epoch 8/10. Iteration 25400/47167 Losses: train: 2.1324667930603027, validate: 2.3838493824005127\n",
      "Epoch 8/10. Iteration 25500/47167 Losses: train: 2.2695837020874023, validate: 2.380525588989258\n",
      "Epoch 8/10. Iteration 25600/47167 Losses: train: 2.2012200355529785, validate: 2.374260425567627\n",
      "Epoch 8/10. Iteration 25700/47167 Losses: train: 2.2307021617889404, validate: 2.3762307167053223\n",
      "Epoch 8/10. Iteration 25800/47167 Losses: train: 2.1172361373901367, validate: 2.3881900310516357\n",
      "Epoch 8/10. Iteration 25900/47167 Losses: train: 2.3317668437957764, validate: 2.3824825286865234\n",
      "Epoch 8/10. Iteration 26000/47167 Losses: train: 2.301711082458496, validate: 2.3882555961608887\n",
      "Epoch 8/10. Iteration 26100/47167 Losses: train: 2.1171646118164062, validate: 2.382910966873169\n",
      "Epoch 8/10. Iteration 26200/47167 Losses: train: 2.1306443214416504, validate: 2.3768198490142822\n",
      "Epoch 8/10. Iteration 26300/47167 Losses: train: 2.280785322189331, validate: 2.380035877227783\n",
      "Epoch 8/10. Iteration 26400/47167 Losses: train: 2.090223789215088, validate: 2.384376287460327\n",
      "Epoch 8/10. Iteration 26500/47167 Losses: train: 2.0451512336730957, validate: 2.382899761199951\n",
      "Epoch 8/10. Iteration 26600/47167 Losses: train: 2.3505430221557617, validate: 2.3788716793060303\n",
      "Epoch 8/10. Iteration 26700/47167 Losses: train: 2.396156072616577, validate: 2.372434139251709\n",
      "Epoch 8/10. Iteration 26800/47167 Losses: train: 2.2257587909698486, validate: 2.3747339248657227\n",
      "Epoch 8/10. Iteration 26900/47167 Losses: train: 2.2431869506835938, validate: 2.373061418533325\n",
      "Epoch 8/10. Iteration 27000/47167 Losses: train: 2.227879524230957, validate: 2.3774430751800537\n",
      "Epoch 8/10. Iteration 27100/47167 Losses: train: 2.2905490398406982, validate: 2.3673410415649414\n",
      "Epoch 8/10. Iteration 27200/47167 Losses: train: 2.154268264770508, validate: 2.362403631210327\n",
      "Epoch 8/10. Iteration 27300/47167 Losses: train: 2.113283395767212, validate: 2.361046314239502\n",
      "Epoch 8/10. Iteration 27400/47167 Losses: train: 2.2881555557250977, validate: 2.3775782585144043\n",
      "Epoch 8/10. Iteration 27500/47167 Losses: train: 2.2330195903778076, validate: 2.3669466972351074\n",
      "Epoch 8/10. Iteration 27600/47167 Losses: train: 2.1725292205810547, validate: 2.369478702545166\n",
      "Epoch 8/10. Iteration 27700/47167 Losses: train: 2.098588228225708, validate: 2.374350070953369\n",
      "Epoch 8/10. Iteration 27800/47167 Losses: train: 2.2121989727020264, validate: 2.366335868835449\n",
      "Epoch 8/10. Iteration 27900/47167 Losses: train: 2.0459115505218506, validate: 2.3699986934661865\n",
      "Epoch 8/10. Iteration 28000/47167 Losses: train: 2.151047945022583, validate: 2.362347364425659\n",
      "Epoch 8/10. Iteration 28100/47167 Losses: train: 2.1033005714416504, validate: 2.371170997619629\n",
      "Epoch 8/10. Iteration 28200/47167 Losses: train: 2.172673225402832, validate: 2.367133140563965\n",
      "Epoch 8/10. Iteration 28300/47167 Losses: train: 2.43937087059021, validate: 2.3769657611846924\n",
      "Epoch 8/10. Iteration 28400/47167 Losses: train: 2.1359598636627197, validate: 2.3725905418395996\n",
      "Epoch 8/10. Iteration 28500/47167 Losses: train: 2.1660051345825195, validate: 2.3651888370513916\n",
      "Epoch 8/10. Iteration 28600/47167 Losses: train: 2.1687872409820557, validate: 2.367326021194458\n",
      "Epoch 8/10. Iteration 28700/47167 Losses: train: 2.23545503616333, validate: 2.3646299839019775\n",
      "Epoch 8/10. Iteration 28800/47167 Losses: train: 2.186539649963379, validate: 2.3636715412139893\n",
      "Epoch 8/10. Iteration 28900/47167 Losses: train: 2.265232801437378, validate: 2.371164560317993\n",
      "Epoch 8/10. Iteration 29000/47167 Losses: train: 2.107187271118164, validate: 2.3662800788879395\n",
      "Epoch 8/10. Iteration 29100/47167 Losses: train: 2.2662763595581055, validate: 2.3689255714416504\n",
      "Epoch 8/10. Iteration 29200/47167 Losses: train: 2.1798183917999268, validate: 2.362340211868286\n",
      "Epoch 8/10. Iteration 29300/47167 Losses: train: 2.1961522102355957, validate: 2.368549108505249\n",
      "Epoch 8/10. Iteration 29400/47167 Losses: train: 2.3329169750213623, validate: 2.3666913509368896\n",
      "Epoch 8/10. Iteration 29500/47167 Losses: train: 2.2932467460632324, validate: 2.373438596725464\n",
      "Epoch 8/10. Iteration 29600/47167 Losses: train: 2.2054545879364014, validate: 2.367058515548706\n",
      "Epoch 8/10. Iteration 29700/47167 Losses: train: 2.169450044631958, validate: 2.3797240257263184\n",
      "Epoch 8/10. Iteration 29800/47167 Losses: train: 2.169343948364258, validate: 2.3669958114624023\n",
      "Epoch 8/10. Iteration 29900/47167 Losses: train: 2.2627668380737305, validate: 2.369389295578003\n",
      "Epoch 8/10. Iteration 30000/47167 Losses: train: 2.313814640045166, validate: 2.3589375019073486\n",
      "Epoch 8/10. Iteration 30100/47167 Losses: train: 2.0345757007598877, validate: 2.3629379272460938\n",
      "Epoch 8/10. Iteration 30200/47167 Losses: train: 2.146458864212036, validate: 2.3634579181671143\n",
      "Epoch 8/10. Iteration 30300/47167 Losses: train: 2.2040202617645264, validate: 2.361964464187622\n",
      "Epoch 8/10. Iteration 30400/47167 Losses: train: 2.1572611331939697, validate: 2.375572919845581\n",
      "Epoch 8/10. Iteration 30500/47167 Losses: train: 2.230280637741089, validate: 2.377931594848633\n",
      "Epoch 8/10. Iteration 30600/47167 Losses: train: 2.315316915512085, validate: 2.3774380683898926\n",
      "Epoch 8/10. Iteration 30700/47167 Losses: train: 2.2941532135009766, validate: 2.370905637741089\n",
      "Epoch 8/10. Iteration 30800/47167 Losses: train: 2.153522491455078, validate: 2.383000135421753\n",
      "Epoch 8/10. Iteration 30900/47167 Losses: train: 2.2967934608459473, validate: 2.3712000846862793\n",
      "Epoch 8/10. Iteration 31000/47167 Losses: train: 2.2843523025512695, validate: 2.3726751804351807\n",
      "Epoch 8/10. Iteration 31100/47167 Losses: train: 2.2305514812469482, validate: 2.3673880100250244\n",
      "Epoch 8/10. Iteration 31200/47167 Losses: train: 2.149789333343506, validate: 2.3745124340057373\n",
      "Epoch 8/10. Iteration 31300/47167 Losses: train: 2.116948127746582, validate: 2.3815174102783203\n",
      "Epoch 8/10. Iteration 31400/47167 Losses: train: 2.095412015914917, validate: 2.3595046997070312\n",
      "Epoch 8/10. Iteration 31500/47167 Losses: train: 2.197488784790039, validate: 2.3828508853912354\n",
      "Epoch 8/10. Iteration 31600/47167 Losses: train: 2.171477794647217, validate: 2.380683660507202\n",
      "Epoch 8/10. Iteration 31700/47167 Losses: train: 2.1849567890167236, validate: 2.3764631748199463\n",
      "Epoch 8/10. Iteration 31800/47167 Losses: train: 2.1004438400268555, validate: 2.3620669841766357\n",
      "Epoch 8/10. Iteration 31900/47167 Losses: train: 2.314685344696045, validate: 2.379199504852295\n",
      "Epoch 8/10. Iteration 32000/47167 Losses: train: 2.2918508052825928, validate: 2.36824631690979\n",
      "Epoch 8/10. Iteration 32100/47167 Losses: train: 2.0474658012390137, validate: 2.368312120437622\n",
      "Epoch 8/10. Iteration 32200/47167 Losses: train: 2.2781243324279785, validate: 2.3728551864624023\n",
      "Epoch 8/10. Iteration 32300/47167 Losses: train: 2.114112138748169, validate: 2.3687520027160645\n",
      "Epoch 8/10. Iteration 32400/47167 Losses: train: 2.3408255577087402, validate: 2.385091543197632\n",
      "Epoch 8/10. Iteration 32500/47167 Losses: train: 2.2039730548858643, validate: 2.3664565086364746\n",
      "Epoch 8/10. Iteration 32600/47167 Losses: train: 1.9719792604446411, validate: 2.3724822998046875\n",
      "Epoch 8/10. Iteration 32700/47167 Losses: train: 2.2900876998901367, validate: 2.3680660724639893\n",
      "Epoch 8/10. Iteration 32800/47167 Losses: train: 2.3549129962921143, validate: 2.368454694747925\n",
      "Epoch 8/10. Iteration 32900/47167 Losses: train: 2.199500799179077, validate: 2.3722167015075684\n",
      "Epoch 8/10. Iteration 33000/47167 Losses: train: 2.145235300064087, validate: 2.368215560913086\n",
      "Epoch 8/10. Iteration 33100/47167 Losses: train: 2.152635097503662, validate: 2.3686721324920654\n",
      "Epoch 8/10. Iteration 33200/47167 Losses: train: 2.1802268028259277, validate: 2.3623669147491455\n",
      "Epoch 8/10. Iteration 33300/47167 Losses: train: 2.0830705165863037, validate: 2.3717403411865234\n",
      "Epoch 8/10. Iteration 33400/47167 Losses: train: 2.1381146907806396, validate: 2.369760274887085\n",
      "Epoch 8/10. Iteration 33500/47167 Losses: train: 2.3832383155822754, validate: 2.3748867511749268\n",
      "Epoch 8/10. Iteration 33600/47167 Losses: train: 2.3176257610321045, validate: 2.3778505325317383\n",
      "Epoch 8/10. Iteration 33700/47167 Losses: train: 2.1823649406433105, validate: 2.378757953643799\n",
      "Epoch 8/10. Iteration 33800/47167 Losses: train: 2.230559825897217, validate: 2.360236883163452\n",
      "Epoch 8/10. Iteration 33900/47167 Losses: train: 2.183279037475586, validate: 2.3722457885742188\n",
      "Epoch 8/10. Iteration 34000/47167 Losses: train: 2.2571604251861572, validate: 2.3807733058929443\n",
      "Epoch 8/10. Iteration 34100/47167 Losses: train: 2.40271258354187, validate: 2.3691020011901855\n",
      "Epoch 8/10. Iteration 34200/47167 Losses: train: 2.345425605773926, validate: 2.3655741214752197\n",
      "Epoch 8/10. Iteration 34300/47167 Losses: train: 2.2041308879852295, validate: 2.362501621246338\n",
      "Epoch 8/10. Iteration 34400/47167 Losses: train: 2.14328932762146, validate: 2.369946241378784\n",
      "Epoch 8/10. Iteration 34500/47167 Losses: train: 2.1899051666259766, validate: 2.3662607669830322\n",
      "Epoch 8/10. Iteration 34600/47167 Losses: train: 2.0080535411834717, validate: 2.373980760574341\n",
      "Epoch 8/10. Iteration 34700/47167 Losses: train: 2.0185632705688477, validate: 2.3636560440063477\n",
      "Epoch 8/10. Iteration 34800/47167 Losses: train: 2.3304126262664795, validate: 2.3795387744903564\n",
      "Epoch 8/10. Iteration 34900/47167 Losses: train: 2.141766309738159, validate: 2.3686234951019287\n",
      "Epoch 8/10. Iteration 35000/47167 Losses: train: 2.3438146114349365, validate: 2.371891975402832\n",
      "Epoch 8/10. Iteration 35100/47167 Losses: train: 2.182788133621216, validate: 2.364236354827881\n",
      "Epoch 8/10. Iteration 35200/47167 Losses: train: 2.230276107788086, validate: 2.371154308319092\n",
      "Epoch 8/10. Iteration 35300/47167 Losses: train: 2.2452821731567383, validate: 2.36246657371521\n",
      "Epoch 8/10. Iteration 35400/47167 Losses: train: 2.0172085762023926, validate: 2.3711962699890137\n",
      "Epoch 8/10. Iteration 35500/47167 Losses: train: 2.1907551288604736, validate: 2.373722553253174\n",
      "Epoch 8/10. Iteration 35600/47167 Losses: train: 1.999049425125122, validate: 2.376939535140991\n",
      "Epoch 8/10. Iteration 35700/47167 Losses: train: 2.2525625228881836, validate: 2.3661203384399414\n",
      "Epoch 8/10. Iteration 35800/47167 Losses: train: 2.1448473930358887, validate: 2.3766286373138428\n",
      "Epoch 8/10. Iteration 35900/47167 Losses: train: 2.0841660499572754, validate: 2.3718700408935547\n",
      "Epoch 8/10. Iteration 36000/47167 Losses: train: 2.2493879795074463, validate: 2.3697030544281006\n",
      "Epoch 8/10. Iteration 36100/47167 Losses: train: 2.130463123321533, validate: 2.3774807453155518\n",
      "Epoch 8/10. Iteration 36200/47167 Losses: train: 2.3929734230041504, validate: 2.3809170722961426\n",
      "Epoch 8/10. Iteration 36300/47167 Losses: train: 2.364649534225464, validate: 2.374314069747925\n",
      "Epoch 8/10. Iteration 36400/47167 Losses: train: 2.1629254817962646, validate: 2.3675341606140137\n",
      "Epoch 8/10. Iteration 36500/47167 Losses: train: 2.3375465869903564, validate: 2.3625547885894775\n",
      "Epoch 8/10. Iteration 36600/47167 Losses: train: 2.2533788681030273, validate: 2.373412847518921\n",
      "Epoch 8/10. Iteration 36700/47167 Losses: train: 2.13281512260437, validate: 2.360928535461426\n",
      "Epoch 8/10. Iteration 36800/47167 Losses: train: 2.251908540725708, validate: 2.36588191986084\n",
      "Epoch 8/10. Iteration 36900/47167 Losses: train: 2.300684690475464, validate: 2.3742809295654297\n",
      "Epoch 8/10. Iteration 37000/47167 Losses: train: 2.217323064804077, validate: 2.3751111030578613\n",
      "Epoch 8/10. Iteration 37100/47167 Losses: train: 2.2715861797332764, validate: 2.36897611618042\n",
      "Epoch 8/10. Iteration 37200/47167 Losses: train: 2.1730077266693115, validate: 2.3600687980651855\n",
      "Epoch 8/10. Iteration 37300/47167 Losses: train: 2.245250701904297, validate: 2.365537405014038\n",
      "Epoch 8/10. Iteration 37400/47167 Losses: train: 2.2414591312408447, validate: 2.3518567085266113\n",
      "Epoch 8/10. Iteration 37500/47167 Losses: train: 2.375202178955078, validate: 2.3658487796783447\n",
      "Epoch 8/10. Iteration 37600/47167 Losses: train: 2.229189395904541, validate: 2.3568737506866455\n",
      "Epoch 8/10. Iteration 37700/47167 Losses: train: 2.197791814804077, validate: 2.3509445190429688\n",
      "Epoch 8/10. Iteration 37800/47167 Losses: train: 2.3438644409179688, validate: 2.354887008666992\n",
      "Epoch 8/10. Iteration 37900/47167 Losses: train: 2.170078992843628, validate: 2.358569622039795\n",
      "Epoch 8/10. Iteration 38000/47167 Losses: train: 2.154984712600708, validate: 2.361470937728882\n",
      "Epoch 8/10. Iteration 38100/47167 Losses: train: 2.257141351699829, validate: 2.3685598373413086\n",
      "Epoch 8/10. Iteration 38200/47167 Losses: train: 2.273252248764038, validate: 2.367602825164795\n",
      "Epoch 8/10. Iteration 38300/47167 Losses: train: 2.3008880615234375, validate: 2.366267442703247\n",
      "Epoch 8/10. Iteration 38400/47167 Losses: train: 2.200714349746704, validate: 2.3642282485961914\n",
      "Epoch 8/10. Iteration 38500/47167 Losses: train: 2.0835342407226562, validate: 2.370741367340088\n",
      "Epoch 8/10. Iteration 38600/47167 Losses: train: 2.102222442626953, validate: 2.3742356300354004\n",
      "Epoch 8/10. Iteration 38700/47167 Losses: train: 2.1314804553985596, validate: 2.368403196334839\n",
      "Epoch 8/10. Iteration 38800/47167 Losses: train: 2.2654385566711426, validate: 2.359562873840332\n",
      "Epoch 8/10. Iteration 38900/47167 Losses: train: 2.1962838172912598, validate: 2.3585853576660156\n",
      "Epoch 8/10. Iteration 39000/47167 Losses: train: 2.0469508171081543, validate: 2.355431079864502\n",
      "Epoch 8/10. Iteration 39100/47167 Losses: train: 2.309467315673828, validate: 2.360731363296509\n",
      "Epoch 8/10. Iteration 39200/47167 Losses: train: 2.2866194248199463, validate: 2.360530376434326\n",
      "Epoch 8/10. Iteration 39300/47167 Losses: train: 2.302546977996826, validate: 2.3674654960632324\n",
      "Epoch 8/10. Iteration 39400/47167 Losses: train: 2.150259494781494, validate: 2.368659734725952\n",
      "Epoch 8/10. Iteration 39500/47167 Losses: train: 2.276243209838867, validate: 2.3725764751434326\n",
      "Epoch 8/10. Iteration 39600/47167 Losses: train: 2.180206298828125, validate: 2.369673728942871\n",
      "Epoch 8/10. Iteration 39700/47167 Losses: train: 2.2064247131347656, validate: 2.373425245285034\n",
      "Epoch 8/10. Iteration 39800/47167 Losses: train: 2.26068115234375, validate: 2.353973627090454\n",
      "Epoch 8/10. Iteration 39900/47167 Losses: train: 2.050549030303955, validate: 2.3633718490600586\n",
      "Epoch 8/10. Iteration 40000/47167 Losses: train: 2.214059829711914, validate: 2.3644859790802\n",
      "Epoch 8/10. Iteration 40100/47167 Losses: train: 2.180840492248535, validate: 2.361522912979126\n",
      "Epoch 8/10. Iteration 40200/47167 Losses: train: 2.1278533935546875, validate: 2.3585407733917236\n",
      "Epoch 8/10. Iteration 40300/47167 Losses: train: 2.013090133666992, validate: 2.3622286319732666\n",
      "Epoch 8/10. Iteration 40400/47167 Losses: train: 2.1524136066436768, validate: 2.3650403022766113\n",
      "Epoch 8/10. Iteration 40500/47167 Losses: train: 2.275707483291626, validate: 2.353003740310669\n",
      "Epoch 8/10. Iteration 40600/47167 Losses: train: 2.0643744468688965, validate: 2.356571674346924\n",
      "Epoch 8/10. Iteration 40700/47167 Losses: train: 2.255070209503174, validate: 2.363105058670044\n",
      "Epoch 8/10. Iteration 40800/47167 Losses: train: 2.2222394943237305, validate: 2.365365743637085\n",
      "Epoch 8/10. Iteration 40900/47167 Losses: train: 2.3635342121124268, validate: 2.3639767169952393\n",
      "Epoch 8/10. Iteration 41000/47167 Losses: train: 2.1903576850891113, validate: 2.3688743114471436\n",
      "Epoch 8/10. Iteration 41100/47167 Losses: train: 2.3326022624969482, validate: 2.355983257293701\n",
      "Epoch 8/10. Iteration 41200/47167 Losses: train: 2.46113657951355, validate: 2.3699042797088623\n",
      "Epoch 8/10. Iteration 41300/47167 Losses: train: 2.274181365966797, validate: 2.353808879852295\n",
      "Epoch 8/10. Iteration 41400/47167 Losses: train: 2.106247663497925, validate: 2.3663134574890137\n",
      "Epoch 8/10. Iteration 41500/47167 Losses: train: 2.1667957305908203, validate: 2.35732364654541\n",
      "Epoch 8/10. Iteration 41600/47167 Losses: train: 2.162440061569214, validate: 2.3517675399780273\n",
      "Epoch 8/10. Iteration 41700/47167 Losses: train: 2.1906344890594482, validate: 2.3525655269622803\n",
      "Epoch 8/10. Iteration 41800/47167 Losses: train: 2.3489181995391846, validate: 2.356395721435547\n",
      "Epoch 8/10. Iteration 41900/47167 Losses: train: 2.1103086471557617, validate: 2.3571999073028564\n",
      "Epoch 8/10. Iteration 42000/47167 Losses: train: 2.328216552734375, validate: 2.361480951309204\n",
      "Epoch 8/10. Iteration 42100/47167 Losses: train: 2.04890775680542, validate: 2.355116367340088\n",
      "Epoch 8/10. Iteration 42200/47167 Losses: train: 2.171118974685669, validate: 2.3573358058929443\n",
      "Epoch 8/10. Iteration 42300/47167 Losses: train: 2.291403293609619, validate: 2.3480727672576904\n",
      "Epoch 8/10. Iteration 42400/47167 Losses: train: 2.208970069885254, validate: 2.362521171569824\n",
      "Epoch 8/10. Iteration 42500/47167 Losses: train: 2.060645341873169, validate: 2.3584368228912354\n",
      "Epoch 8/10. Iteration 42600/47167 Losses: train: 2.3859434127807617, validate: 2.36238431930542\n",
      "Epoch 8/10. Iteration 42700/47167 Losses: train: 2.0998246669769287, validate: 2.3518755435943604\n",
      "Epoch 8/10. Iteration 42800/47167 Losses: train: 2.1419010162353516, validate: 2.3522045612335205\n",
      "Epoch 8/10. Iteration 42900/47167 Losses: train: 2.1404519081115723, validate: 2.352815628051758\n",
      "Epoch 8/10. Iteration 43000/47167 Losses: train: 2.14247465133667, validate: 2.3553106784820557\n",
      "Epoch 8/10. Iteration 43100/47167 Losses: train: 2.2407021522521973, validate: 2.3641304969787598\n",
      "Epoch 8/10. Iteration 43200/47167 Losses: train: 2.2684390544891357, validate: 2.364018201828003\n",
      "Epoch 8/10. Iteration 43300/47167 Losses: train: 2.2525668144226074, validate: 2.355360269546509\n",
      "Epoch 8/10. Iteration 43400/47167 Losses: train: 2.12579607963562, validate: 2.3646130561828613\n",
      "Epoch 8/10. Iteration 43500/47167 Losses: train: 2.1748061180114746, validate: 2.3669259548187256\n",
      "Epoch 8/10. Iteration 43600/47167 Losses: train: 2.12740159034729, validate: 2.377377510070801\n",
      "Epoch 8/10. Iteration 43700/47167 Losses: train: 2.191911220550537, validate: 2.356034994125366\n",
      "Epoch 8/10. Iteration 43800/47167 Losses: train: 2.1506969928741455, validate: 2.3528802394866943\n",
      "Epoch 8/10. Iteration 43900/47167 Losses: train: 2.2175064086914062, validate: 2.369412660598755\n",
      "Epoch 8/10. Iteration 44000/47167 Losses: train: 2.1937429904937744, validate: 2.3520219326019287\n",
      "Epoch 8/10. Iteration 44100/47167 Losses: train: 2.1915762424468994, validate: 2.3710131645202637\n",
      "Epoch 8/10. Iteration 44200/47167 Losses: train: 2.0761029720306396, validate: 2.3538687229156494\n",
      "Epoch 8/10. Iteration 44300/47167 Losses: train: 2.1859612464904785, validate: 2.353349208831787\n",
      "Epoch 8/10. Iteration 44400/47167 Losses: train: 2.327301263809204, validate: 2.357489585876465\n",
      "Epoch 8/10. Iteration 44500/47167 Losses: train: 2.1144165992736816, validate: 2.35892391204834\n",
      "Epoch 8/10. Iteration 44600/47167 Losses: train: 2.311119318008423, validate: 2.35783314704895\n",
      "Epoch 8/10. Iteration 44700/47167 Losses: train: 2.175973892211914, validate: 2.3527305126190186\n",
      "Epoch 8/10. Iteration 44800/47167 Losses: train: 2.1299121379852295, validate: 2.359354257583618\n",
      "Epoch 8/10. Iteration 44900/47167 Losses: train: 2.2652740478515625, validate: 2.3649466037750244\n",
      "Epoch 8/10. Iteration 45000/47167 Losses: train: 2.1194839477539062, validate: 2.356971025466919\n",
      "Epoch 8/10. Iteration 45100/47167 Losses: train: 2.133740186691284, validate: 2.3726611137390137\n",
      "Epoch 8/10. Iteration 45200/47167 Losses: train: 2.242802619934082, validate: 2.35066556930542\n",
      "Epoch 8/10. Iteration 45300/47167 Losses: train: 2.1645891666412354, validate: 2.3742218017578125\n",
      "Epoch 8/10. Iteration 45400/47167 Losses: train: 2.3090062141418457, validate: 2.361867904663086\n",
      "Epoch 8/10. Iteration 45500/47167 Losses: train: 2.203810691833496, validate: 2.355998992919922\n",
      "Epoch 8/10. Iteration 45600/47167 Losses: train: 2.2857563495635986, validate: 2.3517699241638184\n",
      "Epoch 8/10. Iteration 45700/47167 Losses: train: 2.129470109939575, validate: 2.3531084060668945\n",
      "Epoch 8/10. Iteration 45800/47167 Losses: train: 2.4218661785125732, validate: 2.3647000789642334\n",
      "Epoch 8/10. Iteration 45900/47167 Losses: train: 2.0672168731689453, validate: 2.3565564155578613\n",
      "Epoch 8/10. Iteration 46000/47167 Losses: train: 2.152596950531006, validate: 2.3666837215423584\n",
      "Epoch 8/10. Iteration 46100/47167 Losses: train: 2.3524701595306396, validate: 2.3730990886688232\n",
      "Epoch 8/10. Iteration 46200/47167 Losses: train: 2.1300437450408936, validate: 2.3628838062286377\n",
      "Epoch 8/10. Iteration 46300/47167 Losses: train: 2.2861838340759277, validate: 2.363577127456665\n",
      "Epoch 8/10. Iteration 46400/47167 Losses: train: 2.3138859272003174, validate: 2.368028163909912\n",
      "Epoch 8/10. Iteration 46500/47167 Losses: train: 2.1764914989471436, validate: 2.3779356479644775\n",
      "Epoch 8/10. Iteration 46600/47167 Losses: train: 2.2627477645874023, validate: 2.367551803588867\n",
      "Epoch 8/10. Iteration 46700/47167 Losses: train: 2.113323211669922, validate: 2.3586790561676025\n",
      "Epoch 8/10. Iteration 46800/47167 Losses: train: 2.2401061058044434, validate: 2.357166290283203\n",
      "Epoch 8/10. Iteration 46900/47167 Losses: train: 2.1666815280914307, validate: 2.358738899230957\n",
      "Epoch 8/10. Iteration 47000/47167 Losses: train: 2.103893756866455, validate: 2.3545329570770264\n",
      "Epoch 8/10. Iteration 47100/47167 Losses: train: 2.236900568008423, validate: 2.352750539779663\n",
      "Epoch 9/10. Iteration 100/47167 Losses: train: 2.127655506134033, validate: 2.3537213802337646\n",
      "Epoch 9/10. Iteration 200/47167 Losses: train: 2.2009057998657227, validate: 2.3638601303100586\n",
      "Epoch 9/10. Iteration 300/47167 Losses: train: 2.0896754264831543, validate: 2.3636627197265625\n",
      "Epoch 9/10. Iteration 400/47167 Losses: train: 2.1322124004364014, validate: 2.357085704803467\n",
      "Epoch 9/10. Iteration 500/47167 Losses: train: 2.144854784011841, validate: 2.360297918319702\n",
      "Epoch 9/10. Iteration 600/47167 Losses: train: 2.056910514831543, validate: 2.3661065101623535\n",
      "Epoch 9/10. Iteration 700/47167 Losses: train: 2.107327699661255, validate: 2.3729753494262695\n",
      "Epoch 9/10. Iteration 800/47167 Losses: train: 2.1160919666290283, validate: 2.3539741039276123\n",
      "Epoch 9/10. Iteration 900/47167 Losses: train: 2.0504953861236572, validate: 2.356311321258545\n",
      "Epoch 9/10. Iteration 1000/47167 Losses: train: 2.3499464988708496, validate: 2.35614013671875\n",
      "Epoch 9/10. Iteration 1100/47167 Losses: train: 2.075620651245117, validate: 2.364975929260254\n",
      "Epoch 9/10. Iteration 1200/47167 Losses: train: 2.1390738487243652, validate: 2.357316255569458\n",
      "Epoch 9/10. Iteration 1300/47167 Losses: train: 1.991927146911621, validate: 2.363485813140869\n",
      "Epoch 9/10. Iteration 1400/47167 Losses: train: 2.1897735595703125, validate: 2.3427841663360596\n",
      "Epoch 9/10. Iteration 1500/47167 Losses: train: 2.179419994354248, validate: 2.3619964122772217\n",
      "Epoch 9/10. Iteration 1600/47167 Losses: train: 2.168337345123291, validate: 2.363316059112549\n",
      "Epoch 9/10. Iteration 1700/47167 Losses: train: 1.88604736328125, validate: 2.3576595783233643\n",
      "Epoch 9/10. Iteration 1800/47167 Losses: train: 2.0901777744293213, validate: 2.3635549545288086\n",
      "Epoch 9/10. Iteration 1900/47167 Losses: train: 2.4090564250946045, validate: 2.3594679832458496\n",
      "Epoch 9/10. Iteration 2000/47167 Losses: train: 2.1487960815429688, validate: 2.368899345397949\n",
      "Epoch 9/10. Iteration 2100/47167 Losses: train: 2.1158087253570557, validate: 2.3554067611694336\n",
      "Epoch 9/10. Iteration 2200/47167 Losses: train: 2.2487237453460693, validate: 2.3670265674591064\n",
      "Epoch 9/10. Iteration 2300/47167 Losses: train: 2.2119507789611816, validate: 2.3623101711273193\n",
      "Epoch 9/10. Iteration 2400/47167 Losses: train: 2.4361464977264404, validate: 2.3635802268981934\n",
      "Epoch 9/10. Iteration 2500/47167 Losses: train: 2.0271685123443604, validate: 2.3778493404388428\n",
      "Epoch 9/10. Iteration 2600/47167 Losses: train: 2.2426483631134033, validate: 2.374467372894287\n",
      "Epoch 9/10. Iteration 2700/47167 Losses: train: 2.195528984069824, validate: 2.355708122253418\n",
      "Epoch 9/10. Iteration 2800/47167 Losses: train: 2.3641865253448486, validate: 2.3607983589172363\n",
      "Epoch 9/10. Iteration 2900/47167 Losses: train: 2.1152350902557373, validate: 2.3715221881866455\n",
      "Epoch 9/10. Iteration 3000/47167 Losses: train: 2.1085596084594727, validate: 2.364194393157959\n",
      "Epoch 9/10. Iteration 3100/47167 Losses: train: 2.041149377822876, validate: 2.3778295516967773\n",
      "Epoch 9/10. Iteration 3200/47167 Losses: train: 2.3634376525878906, validate: 2.370173215866089\n",
      "Epoch 9/10. Iteration 3300/47167 Losses: train: 2.2291340827941895, validate: 2.366126298904419\n",
      "Epoch 9/10. Iteration 3400/47167 Losses: train: 2.1082773208618164, validate: 2.3713715076446533\n",
      "Epoch 9/10. Iteration 3500/47167 Losses: train: 2.0406999588012695, validate: 2.3682925701141357\n",
      "Epoch 9/10. Iteration 3600/47167 Losses: train: 2.0924572944641113, validate: 2.3676059246063232\n",
      "Epoch 9/10. Iteration 3700/47167 Losses: train: 2.1967110633850098, validate: 2.3687877655029297\n",
      "Epoch 9/10. Iteration 3800/47167 Losses: train: 2.0993878841400146, validate: 2.359938859939575\n",
      "Epoch 9/10. Iteration 3900/47167 Losses: train: 1.9990290403366089, validate: 2.3644518852233887\n",
      "Epoch 9/10. Iteration 4000/47167 Losses: train: 2.0527868270874023, validate: 2.3637943267822266\n",
      "Epoch 9/10. Iteration 4100/47167 Losses: train: 2.1689252853393555, validate: 2.3662822246551514\n",
      "Epoch 9/10. Iteration 4200/47167 Losses: train: 2.040910243988037, validate: 2.3655130863189697\n",
      "Epoch 9/10. Iteration 4300/47167 Losses: train: 1.938092827796936, validate: 2.3503973484039307\n",
      "Epoch 9/10. Iteration 4400/47167 Losses: train: 2.1073596477508545, validate: 2.351375102996826\n",
      "Epoch 9/10. Iteration 4500/47167 Losses: train: 2.1677134037017822, validate: 2.360224485397339\n",
      "Epoch 9/10. Iteration 4600/47167 Losses: train: 1.951503038406372, validate: 2.348010778427124\n",
      "Epoch 9/10. Iteration 4700/47167 Losses: train: 2.221966505050659, validate: 2.3591556549072266\n",
      "Epoch 9/10. Iteration 4800/47167 Losses: train: 2.1579203605651855, validate: 2.3587005138397217\n",
      "Epoch 9/10. Iteration 4900/47167 Losses: train: 2.0930049419403076, validate: 2.3602752685546875\n",
      "Epoch 9/10. Iteration 5000/47167 Losses: train: 2.2476048469543457, validate: 2.358205795288086\n",
      "Epoch 9/10. Iteration 5100/47167 Losses: train: 2.298907518386841, validate: 2.3623764514923096\n",
      "Epoch 9/10. Iteration 5200/47167 Losses: train: 2.13079571723938, validate: 2.3584346771240234\n",
      "Epoch 9/10. Iteration 5300/47167 Losses: train: 1.94331955909729, validate: 2.3590617179870605\n",
      "Epoch 9/10. Iteration 5400/47167 Losses: train: 2.315596342086792, validate: 2.352341890335083\n",
      "Epoch 9/10. Iteration 5500/47167 Losses: train: 1.9690828323364258, validate: 2.3694913387298584\n",
      "Epoch 9/10. Iteration 5600/47167 Losses: train: 2.2046804428100586, validate: 2.364774703979492\n",
      "Epoch 9/10. Iteration 5700/47167 Losses: train: 2.1788923740386963, validate: 2.361022472381592\n",
      "Epoch 9/10. Iteration 5800/47167 Losses: train: 2.0838377475738525, validate: 2.349574327468872\n",
      "Epoch 9/10. Iteration 5900/47167 Losses: train: 2.198958158493042, validate: 2.363135576248169\n",
      "Epoch 9/10. Iteration 6000/47167 Losses: train: 2.1524481773376465, validate: 2.3700098991394043\n",
      "Epoch 9/10. Iteration 6100/47167 Losses: train: 2.3347890377044678, validate: 2.356475591659546\n",
      "Epoch 9/10. Iteration 6200/47167 Losses: train: 2.1351211071014404, validate: 2.357389211654663\n",
      "Epoch 9/10. Iteration 6300/47167 Losses: train: 2.209693193435669, validate: 2.3588383197784424\n",
      "Epoch 9/10. Iteration 6400/47167 Losses: train: 2.088326930999756, validate: 2.3562920093536377\n",
      "Epoch 9/10. Iteration 6500/47167 Losses: train: 2.152230978012085, validate: 2.361595630645752\n",
      "Epoch 9/10. Iteration 6600/47167 Losses: train: 2.2656617164611816, validate: 2.3571572303771973\n",
      "Epoch 9/10. Iteration 6700/47167 Losses: train: 2.0790770053863525, validate: 2.349323034286499\n",
      "Epoch 9/10. Iteration 6800/47167 Losses: train: 2.095778226852417, validate: 2.354018449783325\n",
      "Epoch 9/10. Iteration 6900/47167 Losses: train: 2.2758312225341797, validate: 2.3620364665985107\n",
      "Epoch 9/10. Iteration 7000/47167 Losses: train: 2.2707598209381104, validate: 2.3576154708862305\n",
      "Epoch 9/10. Iteration 7100/47167 Losses: train: 2.1534173488616943, validate: 2.3458023071289062\n",
      "Epoch 9/10. Iteration 7200/47167 Losses: train: 2.069411516189575, validate: 2.3516979217529297\n",
      "Epoch 9/10. Iteration 7300/47167 Losses: train: 2.209515333175659, validate: 2.353560447692871\n",
      "Epoch 9/10. Iteration 7400/47167 Losses: train: 2.2497246265411377, validate: 2.3634700775146484\n",
      "Epoch 9/10. Iteration 7500/47167 Losses: train: 2.4327728748321533, validate: 2.356041193008423\n",
      "Epoch 9/10. Iteration 7600/47167 Losses: train: 2.132660150527954, validate: 2.3687543869018555\n",
      "Epoch 9/10. Iteration 7700/47167 Losses: train: 2.2022788524627686, validate: 2.3537802696228027\n",
      "Epoch 9/10. Iteration 7800/47167 Losses: train: 2.0441832542419434, validate: 2.364424705505371\n",
      "Epoch 9/10. Iteration 7900/47167 Losses: train: 2.1219794750213623, validate: 2.360733985900879\n",
      "Epoch 9/10. Iteration 8000/47167 Losses: train: 2.080400228500366, validate: 2.361509084701538\n",
      "Epoch 9/10. Iteration 8100/47167 Losses: train: 2.008435010910034, validate: 2.3710744380950928\n",
      "Epoch 9/10. Iteration 8200/47167 Losses: train: 2.176337242126465, validate: 2.359513521194458\n",
      "Epoch 9/10. Iteration 8300/47167 Losses: train: 2.2617266178131104, validate: 2.357334852218628\n",
      "Epoch 9/10. Iteration 8400/47167 Losses: train: 2.108078718185425, validate: 2.3558316230773926\n",
      "Epoch 9/10. Iteration 8500/47167 Losses: train: 2.1573681831359863, validate: 2.363457441329956\n",
      "Epoch 9/10. Iteration 8600/47167 Losses: train: 2.0317234992980957, validate: 2.3679862022399902\n",
      "Epoch 9/10. Iteration 8700/47167 Losses: train: 2.000663995742798, validate: 2.372008800506592\n",
      "Epoch 9/10. Iteration 8800/47167 Losses: train: 2.100839138031006, validate: 2.3652453422546387\n",
      "Epoch 9/10. Iteration 8900/47167 Losses: train: 2.0090749263763428, validate: 2.350559711456299\n",
      "Epoch 9/10. Iteration 9000/47167 Losses: train: 2.235219717025757, validate: 2.3572726249694824\n",
      "Epoch 9/10. Iteration 9100/47167 Losses: train: 2.3346402645111084, validate: 2.3631505966186523\n",
      "Epoch 9/10. Iteration 9200/47167 Losses: train: 2.149169921875, validate: 2.3678066730499268\n",
      "Epoch 9/10. Iteration 9300/47167 Losses: train: 2.073554277420044, validate: 2.3628997802734375\n",
      "Epoch 9/10. Iteration 9400/47167 Losses: train: 2.2899560928344727, validate: 2.3583667278289795\n",
      "Epoch 9/10. Iteration 9500/47167 Losses: train: 2.155325412750244, validate: 2.3690621852874756\n",
      "Epoch 9/10. Iteration 9600/47167 Losses: train: 2.2351834774017334, validate: 2.363550901412964\n",
      "Epoch 9/10. Iteration 9700/47167 Losses: train: 2.164680242538452, validate: 2.3766579627990723\n",
      "Epoch 9/10. Iteration 9800/47167 Losses: train: 2.0819404125213623, validate: 2.3743507862091064\n",
      "Epoch 9/10. Iteration 9900/47167 Losses: train: 2.136540412902832, validate: 2.3742218017578125\n",
      "Epoch 9/10. Iteration 10000/47167 Losses: train: 2.3357834815979004, validate: 2.3725924491882324\n",
      "Epoch 9/10. Iteration 10100/47167 Losses: train: 2.2920236587524414, validate: 2.365675449371338\n",
      "Epoch 9/10. Iteration 10200/47167 Losses: train: 2.1933822631835938, validate: 2.3597893714904785\n",
      "Epoch 9/10. Iteration 10300/47167 Losses: train: 2.146320343017578, validate: 2.3639414310455322\n",
      "Epoch 9/10. Iteration 10400/47167 Losses: train: 2.1616270542144775, validate: 2.369718551635742\n",
      "Epoch 9/10. Iteration 10500/47167 Losses: train: 2.002814769744873, validate: 2.366513729095459\n",
      "Epoch 9/10. Iteration 10600/47167 Losses: train: 2.1125965118408203, validate: 2.365466833114624\n",
      "Epoch 9/10. Iteration 10700/47167 Losses: train: 2.1503961086273193, validate: 2.3676910400390625\n",
      "Epoch 9/10. Iteration 10800/47167 Losses: train: 2.238145112991333, validate: 2.3667445182800293\n",
      "Epoch 9/10. Iteration 10900/47167 Losses: train: 2.2809481620788574, validate: 2.351977825164795\n",
      "Epoch 9/10. Iteration 11000/47167 Losses: train: 2.1624481678009033, validate: 2.3697423934936523\n",
      "Epoch 9/10. Iteration 11100/47167 Losses: train: 2.1975176334381104, validate: 2.3637313842773438\n",
      "Epoch 9/10. Iteration 11200/47167 Losses: train: 2.236274480819702, validate: 2.3644113540649414\n",
      "Epoch 9/10. Iteration 11300/47167 Losses: train: 2.3584117889404297, validate: 2.3615944385528564\n",
      "Epoch 9/10. Iteration 11400/47167 Losses: train: 2.0258190631866455, validate: 2.3623998165130615\n",
      "Epoch 9/10. Iteration 11500/47167 Losses: train: 2.0284786224365234, validate: 2.369279384613037\n",
      "Epoch 9/10. Iteration 11600/47167 Losses: train: 2.1126163005828857, validate: 2.355888605117798\n",
      "Epoch 9/10. Iteration 11700/47167 Losses: train: 2.124582529067993, validate: 2.3644115924835205\n",
      "Epoch 9/10. Iteration 11800/47167 Losses: train: 2.1669678688049316, validate: 2.3635146617889404\n",
      "Epoch 9/10. Iteration 11900/47167 Losses: train: 2.091110944747925, validate: 2.3501224517822266\n",
      "Epoch 9/10. Iteration 12000/47167 Losses: train: 2.2286031246185303, validate: 2.3513879776000977\n",
      "Epoch 9/10. Iteration 12100/47167 Losses: train: 2.122227907180786, validate: 2.3530666828155518\n",
      "Epoch 9/10. Iteration 12200/47167 Losses: train: 2.011120080947876, validate: 2.351609706878662\n",
      "Epoch 9/10. Iteration 12300/47167 Losses: train: 2.284679412841797, validate: 2.358368158340454\n",
      "Epoch 9/10. Iteration 12400/47167 Losses: train: 2.2156972885131836, validate: 2.3415305614471436\n",
      "Epoch 9/10. Iteration 12500/47167 Losses: train: 2.0426440238952637, validate: 2.353386640548706\n",
      "Epoch 9/10. Iteration 12600/47167 Losses: train: 2.1845946311950684, validate: 2.3584237098693848\n",
      "Epoch 9/10. Iteration 12700/47167 Losses: train: 2.203935384750366, validate: 2.36464262008667\n",
      "Epoch 9/10. Iteration 12800/47167 Losses: train: 2.2226312160491943, validate: 2.354238748550415\n",
      "Epoch 9/10. Iteration 12900/47167 Losses: train: 2.1179916858673096, validate: 2.3583426475524902\n",
      "Epoch 9/10. Iteration 13000/47167 Losses: train: 2.2077226638793945, validate: 2.3683040142059326\n",
      "Epoch 9/10. Iteration 13100/47167 Losses: train: 2.189121723175049, validate: 2.3623859882354736\n",
      "Epoch 9/10. Iteration 13200/47167 Losses: train: 2.2234952449798584, validate: 2.357140064239502\n",
      "Epoch 9/10. Iteration 13300/47167 Losses: train: 2.2040905952453613, validate: 2.365128755569458\n",
      "Epoch 9/10. Iteration 13400/47167 Losses: train: 2.014047861099243, validate: 2.362915277481079\n",
      "Epoch 9/10. Iteration 13500/47167 Losses: train: 2.1201353073120117, validate: 2.360092878341675\n",
      "Epoch 9/10. Iteration 13600/47167 Losses: train: 2.1378960609436035, validate: 2.360872983932495\n",
      "Epoch 9/10. Iteration 13700/47167 Losses: train: 2.1088225841522217, validate: 2.352320909500122\n",
      "Epoch 9/10. Iteration 13800/47167 Losses: train: 2.0018882751464844, validate: 2.3588221073150635\n",
      "Epoch 9/10. Iteration 13900/47167 Losses: train: 2.0202486515045166, validate: 2.3711466789245605\n",
      "Epoch 9/10. Iteration 14000/47167 Losses: train: 2.1772611141204834, validate: 2.3639864921569824\n",
      "Epoch 9/10. Iteration 14100/47167 Losses: train: 2.177934408187866, validate: 2.3588674068450928\n",
      "Epoch 9/10. Iteration 14200/47167 Losses: train: 2.098400354385376, validate: 2.352489471435547\n",
      "Epoch 9/10. Iteration 14300/47167 Losses: train: 2.1468961238861084, validate: 2.364403247833252\n",
      "Epoch 9/10. Iteration 14400/47167 Losses: train: 2.1890711784362793, validate: 2.3657138347625732\n",
      "Epoch 9/10. Iteration 14500/47167 Losses: train: 2.2161378860473633, validate: 2.3609602451324463\n",
      "Epoch 9/10. Iteration 14600/47167 Losses: train: 2.2472681999206543, validate: 2.3661651611328125\n",
      "Epoch 9/10. Iteration 14700/47167 Losses: train: 2.242396831512451, validate: 2.3548049926757812\n",
      "Epoch 9/10. Iteration 14800/47167 Losses: train: 2.1298394203186035, validate: 2.346848249435425\n",
      "Epoch 9/10. Iteration 14900/47167 Losses: train: 2.29068922996521, validate: 2.346909999847412\n",
      "Epoch 9/10. Iteration 15000/47167 Losses: train: 2.2185475826263428, validate: 2.3600733280181885\n",
      "Epoch 9/10. Iteration 15100/47167 Losses: train: 2.1666977405548096, validate: 2.3554089069366455\n",
      "Epoch 9/10. Iteration 15200/47167 Losses: train: 2.104905605316162, validate: 2.3554210662841797\n",
      "Epoch 9/10. Iteration 15300/47167 Losses: train: 2.126659393310547, validate: 2.3557944297790527\n",
      "Epoch 9/10. Iteration 15400/47167 Losses: train: 2.1364240646362305, validate: 2.348393201828003\n",
      "Epoch 9/10. Iteration 15500/47167 Losses: train: 2.0501534938812256, validate: 2.364020586013794\n",
      "Epoch 9/10. Iteration 15600/47167 Losses: train: 2.154245138168335, validate: 2.354498863220215\n",
      "Epoch 9/10. Iteration 15700/47167 Losses: train: 2.1842658519744873, validate: 2.355030059814453\n",
      "Epoch 9/10. Iteration 15800/47167 Losses: train: 2.1166603565216064, validate: 2.3541011810302734\n",
      "Epoch 9/10. Iteration 15900/47167 Losses: train: 1.9959936141967773, validate: 2.3585638999938965\n",
      "Epoch 9/10. Iteration 16000/47167 Losses: train: 2.13315749168396, validate: 2.3691117763519287\n",
      "Epoch 9/10. Iteration 16100/47167 Losses: train: 2.052224636077881, validate: 2.3688337802886963\n",
      "Epoch 9/10. Iteration 16200/47167 Losses: train: 2.354386568069458, validate: 2.3692386150360107\n",
      "Epoch 9/10. Iteration 16300/47167 Losses: train: 1.9570808410644531, validate: 2.362034559249878\n",
      "Epoch 9/10. Iteration 16400/47167 Losses: train: 2.1164095401763916, validate: 2.3638229370117188\n",
      "Epoch 9/10. Iteration 16500/47167 Losses: train: 2.032827377319336, validate: 2.352888822555542\n",
      "Epoch 9/10. Iteration 16600/47167 Losses: train: 2.313588857650757, validate: 2.354541301727295\n",
      "Epoch 9/10. Iteration 16700/47167 Losses: train: 2.1359171867370605, validate: 2.3684914112091064\n",
      "Epoch 9/10. Iteration 16800/47167 Losses: train: 2.323742628097534, validate: 2.365954875946045\n",
      "Epoch 9/10. Iteration 16900/47167 Losses: train: 2.1743226051330566, validate: 2.3722994327545166\n",
      "Epoch 9/10. Iteration 17000/47167 Losses: train: 2.07211971282959, validate: 2.360095739364624\n",
      "Epoch 9/10. Iteration 17100/47167 Losses: train: 2.0156078338623047, validate: 2.3496816158294678\n",
      "Epoch 9/10. Iteration 17200/47167 Losses: train: 2.222153663635254, validate: 2.3680715560913086\n",
      "Epoch 9/10. Iteration 17300/47167 Losses: train: 2.1525585651397705, validate: 2.3562018871307373\n",
      "Epoch 9/10. Iteration 17400/47167 Losses: train: 2.3129525184631348, validate: 2.3560943603515625\n",
      "Epoch 9/10. Iteration 17500/47167 Losses: train: 1.9911420345306396, validate: 2.354886531829834\n",
      "Epoch 9/10. Iteration 17600/47167 Losses: train: 2.2356491088867188, validate: 2.3471310138702393\n",
      "Epoch 9/10. Iteration 17700/47167 Losses: train: 2.059027910232544, validate: 2.3541362285614014\n",
      "Epoch 9/10. Iteration 17800/47167 Losses: train: 2.136826992034912, validate: 2.363462209701538\n",
      "Epoch 9/10. Iteration 17900/47167 Losses: train: 2.3436577320098877, validate: 2.3556361198425293\n",
      "Epoch 9/10. Iteration 18000/47167 Losses: train: 2.2809810638427734, validate: 2.3597400188446045\n",
      "Epoch 9/10. Iteration 18100/47167 Losses: train: 2.2077975273132324, validate: 2.3475801944732666\n",
      "Epoch 9/10. Iteration 18200/47167 Losses: train: 2.1706206798553467, validate: 2.347839593887329\n",
      "Epoch 9/10. Iteration 18300/47167 Losses: train: 2.1886212825775146, validate: 2.3523523807525635\n",
      "Epoch 9/10. Iteration 18400/47167 Losses: train: 2.289865016937256, validate: 2.347738027572632\n",
      "Epoch 9/10. Iteration 18500/47167 Losses: train: 2.046616554260254, validate: 2.346165180206299\n",
      "Epoch 9/10. Iteration 18600/47167 Losses: train: 2.038588762283325, validate: 2.3485724925994873\n",
      "Epoch 9/10. Iteration 18700/47167 Losses: train: 2.118812084197998, validate: 2.355170488357544\n",
      "Epoch 9/10. Iteration 18800/47167 Losses: train: 2.0762009620666504, validate: 2.3479528427124023\n",
      "Epoch 9/10. Iteration 18900/47167 Losses: train: 2.2319226264953613, validate: 2.3611559867858887\n",
      "Epoch 9/10. Iteration 19000/47167 Losses: train: 2.131160259246826, validate: 2.355135917663574\n",
      "Epoch 9/10. Iteration 19100/47167 Losses: train: 1.9963526725769043, validate: 2.366610050201416\n",
      "Epoch 9/10. Iteration 19200/47167 Losses: train: 2.074659585952759, validate: 2.36484956741333\n",
      "Epoch 9/10. Iteration 19300/47167 Losses: train: 2.3014540672302246, validate: 2.355607509613037\n",
      "Epoch 9/10. Iteration 19400/47167 Losses: train: 2.0962510108947754, validate: 2.3468592166900635\n",
      "Epoch 9/10. Iteration 19500/47167 Losses: train: 2.2136504650115967, validate: 2.3554022312164307\n",
      "Epoch 9/10. Iteration 19600/47167 Losses: train: 2.021735429763794, validate: 2.3515186309814453\n",
      "Epoch 9/10. Iteration 19700/47167 Losses: train: 2.048833131790161, validate: 2.355689287185669\n",
      "Epoch 9/10. Iteration 19800/47167 Losses: train: 2.3448827266693115, validate: 2.349546432495117\n",
      "Epoch 9/10. Iteration 19900/47167 Losses: train: 2.212627649307251, validate: 2.3442130088806152\n",
      "Epoch 9/10. Iteration 20000/47167 Losses: train: 2.3030006885528564, validate: 2.357645034790039\n",
      "Epoch 9/10. Iteration 20100/47167 Losses: train: 2.3325023651123047, validate: 2.347654104232788\n",
      "Epoch 9/10. Iteration 20200/47167 Losses: train: 2.017774820327759, validate: 2.3436920642852783\n",
      "Epoch 9/10. Iteration 20300/47167 Losses: train: 2.052993059158325, validate: 2.352428913116455\n",
      "Epoch 9/10. Iteration 20400/47167 Losses: train: 2.0253844261169434, validate: 2.3589284420013428\n",
      "Epoch 9/10. Iteration 20500/47167 Losses: train: 1.9527041912078857, validate: 2.3582210540771484\n",
      "Epoch 9/10. Iteration 20600/47167 Losses: train: 2.0857176780700684, validate: 2.346376657485962\n",
      "Epoch 9/10. Iteration 20700/47167 Losses: train: 1.9965718984603882, validate: 2.3653459548950195\n",
      "Epoch 9/10. Iteration 20800/47167 Losses: train: 2.2124452590942383, validate: 2.356684923171997\n",
      "Epoch 9/10. Iteration 20900/47167 Losses: train: 2.1467247009277344, validate: 2.353123903274536\n",
      "Epoch 9/10. Iteration 21000/47167 Losses: train: 2.2482523918151855, validate: 2.3506479263305664\n",
      "Epoch 9/10. Iteration 21100/47167 Losses: train: 2.1782867908477783, validate: 2.3446388244628906\n",
      "Epoch 9/10. Iteration 21200/47167 Losses: train: 2.143151044845581, validate: 2.346402883529663\n",
      "Epoch 9/10. Iteration 21300/47167 Losses: train: 2.1728479862213135, validate: 2.3435583114624023\n",
      "Epoch 9/10. Iteration 21400/47167 Losses: train: 2.108830451965332, validate: 2.342839241027832\n",
      "Epoch 9/10. Iteration 21500/47167 Losses: train: 2.2581217288970947, validate: 2.3461670875549316\n",
      "Epoch 9/10. Iteration 21600/47167 Losses: train: 2.0935935974121094, validate: 2.3451120853424072\n",
      "Epoch 9/10. Iteration 21700/47167 Losses: train: 2.2337229251861572, validate: 2.3531947135925293\n",
      "Epoch 9/10. Iteration 21800/47167 Losses: train: 2.26647686958313, validate: 2.344838857650757\n",
      "Epoch 9/10. Iteration 21900/47167 Losses: train: 2.0155627727508545, validate: 2.347367525100708\n",
      "Epoch 9/10. Iteration 22000/47167 Losses: train: 2.1542437076568604, validate: 2.3460326194763184\n",
      "Epoch 9/10. Iteration 22100/47167 Losses: train: 2.216535806655884, validate: 2.3511550426483154\n",
      "Epoch 9/10. Iteration 22200/47167 Losses: train: 2.320518732070923, validate: 2.344999313354492\n",
      "Epoch 9/10. Iteration 22300/47167 Losses: train: 2.093010187149048, validate: 2.3409688472747803\n",
      "Epoch 9/10. Iteration 22400/47167 Losses: train: 2.132288694381714, validate: 2.3477492332458496\n",
      "Epoch 9/10. Iteration 22500/47167 Losses: train: 2.073357105255127, validate: 2.344538927078247\n",
      "Epoch 9/10. Iteration 22600/47167 Losses: train: 2.0449271202087402, validate: 2.343564748764038\n",
      "Epoch 9/10. Iteration 22700/47167 Losses: train: 2.169804573059082, validate: 2.341628313064575\n",
      "Epoch 9/10. Iteration 22800/47167 Losses: train: 2.1192283630371094, validate: 2.3424975872039795\n",
      "Epoch 9/10. Iteration 22900/47167 Losses: train: 1.9700392484664917, validate: 2.3417584896087646\n",
      "Epoch 9/10. Iteration 23000/47167 Losses: train: 2.164696216583252, validate: 2.3386549949645996\n",
      "Epoch 9/10. Iteration 23100/47167 Losses: train: 2.176631450653076, validate: 2.3458611965179443\n",
      "Epoch 9/10. Iteration 23200/47167 Losses: train: 2.1100199222564697, validate: 2.3568315505981445\n",
      "Epoch 9/10. Iteration 23300/47167 Losses: train: 2.166640043258667, validate: 2.3480935096740723\n",
      "Epoch 9/10. Iteration 23400/47167 Losses: train: 2.181462287902832, validate: 2.347280263900757\n",
      "Epoch 9/10. Iteration 23500/47167 Losses: train: 2.1792964935302734, validate: 2.3413074016571045\n",
      "Epoch 9/10. Iteration 23600/47167 Losses: train: 1.9901692867279053, validate: 2.33804988861084\n",
      "Epoch 9/10. Iteration 23700/47167 Losses: train: 2.0748870372772217, validate: 2.346453905105591\n",
      "Epoch 9/10. Iteration 23800/47167 Losses: train: 2.175952434539795, validate: 2.3485021591186523\n",
      "Epoch 9/10. Iteration 23900/47167 Losses: train: 2.211320161819458, validate: 2.3404674530029297\n",
      "Epoch 9/10. Iteration 24000/47167 Losses: train: 2.4402472972869873, validate: 2.3434395790100098\n",
      "Epoch 9/10. Iteration 24100/47167 Losses: train: 2.120025634765625, validate: 2.348975419998169\n",
      "Epoch 9/10. Iteration 24200/47167 Losses: train: 2.310917615890503, validate: 2.3623995780944824\n",
      "Epoch 9/10. Iteration 24300/47167 Losses: train: 2.060218334197998, validate: 2.3527462482452393\n",
      "Epoch 9/10. Iteration 24400/47167 Losses: train: 2.1541178226470947, validate: 2.3436524868011475\n",
      "Epoch 9/10. Iteration 24500/47167 Losses: train: 2.200920343399048, validate: 2.3574166297912598\n",
      "Epoch 9/10. Iteration 24600/47167 Losses: train: 2.2119970321655273, validate: 2.341456651687622\n",
      "Epoch 9/10. Iteration 24700/47167 Losses: train: 2.2076988220214844, validate: 2.352341890335083\n",
      "Epoch 9/10. Iteration 24800/47167 Losses: train: 2.1044349670410156, validate: 2.346409559249878\n",
      "Epoch 9/10. Iteration 24900/47167 Losses: train: 2.0762245655059814, validate: 2.3467681407928467\n",
      "Epoch 9/10. Iteration 25000/47167 Losses: train: 2.0288844108581543, validate: 2.3452014923095703\n",
      "Epoch 9/10. Iteration 25100/47167 Losses: train: 2.197782516479492, validate: 2.3497426509857178\n",
      "Epoch 9/10. Iteration 25200/47167 Losses: train: 2.139965534210205, validate: 2.3543436527252197\n",
      "Epoch 9/10. Iteration 25300/47167 Losses: train: 2.1491758823394775, validate: 2.3554370403289795\n",
      "Epoch 9/10. Iteration 25400/47167 Losses: train: 2.075443983078003, validate: 2.3549792766571045\n",
      "Epoch 9/10. Iteration 25500/47167 Losses: train: 2.1158361434936523, validate: 2.3532378673553467\n",
      "Epoch 9/10. Iteration 25600/47167 Losses: train: 2.226010799407959, validate: 2.3601930141448975\n",
      "Epoch 9/10. Iteration 25700/47167 Losses: train: 2.2378714084625244, validate: 2.3476357460021973\n",
      "Epoch 9/10. Iteration 25800/47167 Losses: train: 2.2393317222595215, validate: 2.353760242462158\n",
      "Epoch 9/10. Iteration 25900/47167 Losses: train: 2.1664865016937256, validate: 2.351414442062378\n",
      "Epoch 9/10. Iteration 26000/47167 Losses: train: 2.0236668586730957, validate: 2.3498198986053467\n",
      "Epoch 9/10. Iteration 26100/47167 Losses: train: 2.075512409210205, validate: 2.346076011657715\n",
      "Epoch 9/10. Iteration 26200/47167 Losses: train: 2.042285680770874, validate: 2.3419413566589355\n",
      "Epoch 9/10. Iteration 26300/47167 Losses: train: 2.0735936164855957, validate: 2.3485536575317383\n",
      "Epoch 9/10. Iteration 26400/47167 Losses: train: 2.262885093688965, validate: 2.351149082183838\n",
      "Epoch 9/10. Iteration 26500/47167 Losses: train: 2.18418025970459, validate: 2.3395447731018066\n",
      "Epoch 9/10. Iteration 26600/47167 Losses: train: 2.2246546745300293, validate: 2.3549423217773438\n",
      "Epoch 9/10. Iteration 26700/47167 Losses: train: 2.0866098403930664, validate: 2.3410563468933105\n",
      "Epoch 9/10. Iteration 26800/47167 Losses: train: 2.0638389587402344, validate: 2.345872640609741\n",
      "Epoch 9/10. Iteration 26900/47167 Losses: train: 2.2403626441955566, validate: 2.349884510040283\n",
      "Epoch 9/10. Iteration 27000/47167 Losses: train: 2.1134979724884033, validate: 2.3623769283294678\n",
      "Epoch 9/10. Iteration 27100/47167 Losses: train: 2.180079221725464, validate: 2.34731388092041\n",
      "Epoch 9/10. Iteration 27200/47167 Losses: train: 2.2984261512756348, validate: 2.3494839668273926\n",
      "Epoch 9/10. Iteration 27300/47167 Losses: train: 2.2111282348632812, validate: 2.355630397796631\n",
      "Epoch 9/10. Iteration 27400/47167 Losses: train: 2.109178066253662, validate: 2.3490822315216064\n",
      "Epoch 9/10. Iteration 27500/47167 Losses: train: 2.057699203491211, validate: 2.3568825721740723\n",
      "Epoch 9/10. Iteration 27600/47167 Losses: train: 2.1707425117492676, validate: 2.355803966522217\n",
      "Epoch 9/10. Iteration 27700/47167 Losses: train: 2.134699821472168, validate: 2.343493938446045\n",
      "Epoch 9/10. Iteration 27800/47167 Losses: train: 2.1012227535247803, validate: 2.3514962196350098\n",
      "Epoch 9/10. Iteration 27900/47167 Losses: train: 2.071681022644043, validate: 2.346208095550537\n",
      "Epoch 9/10. Iteration 28000/47167 Losses: train: 2.3052637577056885, validate: 2.353707790374756\n",
      "Epoch 9/10. Iteration 28100/47167 Losses: train: 2.4371120929718018, validate: 2.3489410877227783\n",
      "Epoch 9/10. Iteration 28200/47167 Losses: train: 2.2609431743621826, validate: 2.352903127670288\n",
      "Epoch 9/10. Iteration 28300/47167 Losses: train: 2.2876169681549072, validate: 2.3397152423858643\n",
      "Epoch 9/10. Iteration 28400/47167 Losses: train: 2.1410233974456787, validate: 2.3461856842041016\n",
      "Epoch 9/10. Iteration 28500/47167 Losses: train: 2.1315431594848633, validate: 2.3451483249664307\n",
      "Epoch 9/10. Iteration 28600/47167 Losses: train: 2.0235884189605713, validate: 2.3427441120147705\n",
      "Epoch 9/10. Iteration 28700/47167 Losses: train: 2.133268356323242, validate: 2.349275588989258\n",
      "Epoch 9/10. Iteration 28800/47167 Losses: train: 2.055022716522217, validate: 2.3525009155273438\n",
      "Epoch 9/10. Iteration 28900/47167 Losses: train: 2.2116706371307373, validate: 2.3449018001556396\n",
      "Epoch 9/10. Iteration 29000/47167 Losses: train: 2.076777696609497, validate: 2.3320202827453613\n",
      "Epoch 9/10. Iteration 29100/47167 Losses: train: 2.0469796657562256, validate: 2.342499017715454\n",
      "Epoch 9/10. Iteration 29200/47167 Losses: train: 2.3056416511535645, validate: 2.3328487873077393\n",
      "Epoch 9/10. Iteration 29300/47167 Losses: train: 2.0887911319732666, validate: 2.337453603744507\n",
      "Epoch 9/10. Iteration 29400/47167 Losses: train: 1.9668421745300293, validate: 2.3405120372772217\n",
      "Epoch 9/10. Iteration 29500/47167 Losses: train: 2.175053358078003, validate: 2.351315498352051\n",
      "Epoch 9/10. Iteration 29600/47167 Losses: train: 2.113844156265259, validate: 2.3542590141296387\n",
      "Epoch 9/10. Iteration 29700/47167 Losses: train: 2.1673996448516846, validate: 2.357452392578125\n",
      "Epoch 9/10. Iteration 29800/47167 Losses: train: 2.18965220451355, validate: 2.351536512374878\n",
      "Epoch 9/10. Iteration 29900/47167 Losses: train: 2.190547227859497, validate: 2.3425557613372803\n",
      "Epoch 9/10. Iteration 30000/47167 Losses: train: 2.2029778957366943, validate: 2.3394744396209717\n",
      "Epoch 9/10. Iteration 30100/47167 Losses: train: 2.1372711658477783, validate: 2.3362197875976562\n",
      "Epoch 9/10. Iteration 30200/47167 Losses: train: 2.2689261436462402, validate: 2.3444290161132812\n",
      "Epoch 9/10. Iteration 30300/47167 Losses: train: 2.2182791233062744, validate: 2.3419253826141357\n",
      "Epoch 9/10. Iteration 30400/47167 Losses: train: 2.2143940925598145, validate: 2.330672025680542\n",
      "Epoch 9/10. Iteration 30500/47167 Losses: train: 2.2852752208709717, validate: 2.329641580581665\n",
      "Epoch 9/10. Iteration 30600/47167 Losses: train: 1.9956809282302856, validate: 2.3372857570648193\n",
      "Epoch 9/10. Iteration 30700/47167 Losses: train: 2.086409330368042, validate: 2.3307809829711914\n",
      "Epoch 9/10. Iteration 30800/47167 Losses: train: 2.1415657997131348, validate: 2.3461084365844727\n",
      "Epoch 9/10. Iteration 30900/47167 Losses: train: 2.246309518814087, validate: 2.3471243381500244\n",
      "Epoch 9/10. Iteration 31000/47167 Losses: train: 2.3269524574279785, validate: 2.3469040393829346\n",
      "Epoch 9/10. Iteration 31100/47167 Losses: train: 2.1309683322906494, validate: 2.3481884002685547\n",
      "Epoch 9/10. Iteration 31200/47167 Losses: train: 2.135333776473999, validate: 2.3438503742218018\n",
      "Epoch 9/10. Iteration 31300/47167 Losses: train: 2.1758785247802734, validate: 2.3464810848236084\n",
      "Epoch 9/10. Iteration 31400/47167 Losses: train: 2.367643356323242, validate: 2.3497154712677\n",
      "Epoch 9/10. Iteration 31500/47167 Losses: train: 2.2029478549957275, validate: 2.330479621887207\n",
      "Epoch 9/10. Iteration 31600/47167 Losses: train: 2.0579090118408203, validate: 2.339754343032837\n",
      "Epoch 9/10. Iteration 31700/47167 Losses: train: 2.0705392360687256, validate: 2.337329387664795\n",
      "Epoch 9/10. Iteration 31800/47167 Losses: train: 2.155287981033325, validate: 2.351062297821045\n",
      "Epoch 9/10. Iteration 31900/47167 Losses: train: 2.2041146755218506, validate: 2.3562495708465576\n",
      "Epoch 9/10. Iteration 32000/47167 Losses: train: 2.2994916439056396, validate: 2.333219051361084\n",
      "Epoch 9/10. Iteration 32100/47167 Losses: train: 2.067471504211426, validate: 2.344215154647827\n",
      "Epoch 9/10. Iteration 32200/47167 Losses: train: 2.089785099029541, validate: 2.342935800552368\n",
      "Epoch 9/10. Iteration 32300/47167 Losses: train: 2.039463520050049, validate: 2.34741473197937\n",
      "Epoch 9/10. Iteration 32400/47167 Losses: train: 2.2462196350097656, validate: 2.3417000770568848\n",
      "Epoch 9/10. Iteration 32500/47167 Losses: train: 2.1372621059417725, validate: 2.3565566539764404\n",
      "Epoch 9/10. Iteration 32600/47167 Losses: train: 1.985089659690857, validate: 2.3373208045959473\n",
      "Epoch 9/10. Iteration 32700/47167 Losses: train: 2.1416797637939453, validate: 2.3542747497558594\n",
      "Epoch 9/10. Iteration 32800/47167 Losses: train: 2.2404582500457764, validate: 2.357133150100708\n",
      "Epoch 9/10. Iteration 32900/47167 Losses: train: 2.306572198867798, validate: 2.3433501720428467\n",
      "Epoch 9/10. Iteration 33000/47167 Losses: train: 2.2537243366241455, validate: 2.3469467163085938\n",
      "Epoch 9/10. Iteration 33100/47167 Losses: train: 2.3044538497924805, validate: 2.352587938308716\n",
      "Epoch 9/10. Iteration 33200/47167 Losses: train: 2.1438851356506348, validate: 2.357726573944092\n",
      "Epoch 9/10. Iteration 33300/47167 Losses: train: 2.3195106983184814, validate: 2.3409457206726074\n",
      "Epoch 9/10. Iteration 33400/47167 Losses: train: 2.020092248916626, validate: 2.35054349899292\n",
      "Epoch 9/10. Iteration 33500/47167 Losses: train: 2.2916409969329834, validate: 2.3499698638916016\n",
      "Epoch 9/10. Iteration 33600/47167 Losses: train: 2.2180583477020264, validate: 2.354238271713257\n",
      "Epoch 9/10. Iteration 33700/47167 Losses: train: 2.191896438598633, validate: 2.346067190170288\n",
      "Epoch 9/10. Iteration 33800/47167 Losses: train: 2.215879440307617, validate: 2.343242645263672\n",
      "Epoch 9/10. Iteration 33900/47167 Losses: train: 2.1960055828094482, validate: 2.3459737300872803\n",
      "Epoch 9/10. Iteration 34000/47167 Losses: train: 2.056391477584839, validate: 2.3386685848236084\n",
      "Epoch 9/10. Iteration 34100/47167 Losses: train: 2.2978785037994385, validate: 2.3360159397125244\n",
      "Epoch 9/10. Iteration 34200/47167 Losses: train: 2.0770719051361084, validate: 2.3378045558929443\n",
      "Epoch 9/10. Iteration 34300/47167 Losses: train: 2.051588535308838, validate: 2.355609178543091\n",
      "Epoch 9/10. Iteration 34400/47167 Losses: train: 2.1543731689453125, validate: 2.349224090576172\n",
      "Epoch 9/10. Iteration 34500/47167 Losses: train: 2.0763542652130127, validate: 2.339384078979492\n",
      "Epoch 9/10. Iteration 34600/47167 Losses: train: 1.99318528175354, validate: 2.3400893211364746\n",
      "Epoch 9/10. Iteration 34700/47167 Losses: train: 2.1896588802337646, validate: 2.3453445434570312\n",
      "Epoch 9/10. Iteration 34800/47167 Losses: train: 2.17319917678833, validate: 2.342006206512451\n",
      "Epoch 9/10. Iteration 34900/47167 Losses: train: 2.2745769023895264, validate: 2.347086191177368\n",
      "Epoch 9/10. Iteration 35000/47167 Losses: train: 2.318748950958252, validate: 2.341015577316284\n",
      "Epoch 9/10. Iteration 35100/47167 Losses: train: 2.0181009769439697, validate: 2.3575479984283447\n",
      "Epoch 9/10. Iteration 35200/47167 Losses: train: 2.256561279296875, validate: 2.347907781600952\n",
      "Epoch 9/10. Iteration 35300/47167 Losses: train: 1.9906922578811646, validate: 2.3298850059509277\n",
      "Epoch 9/10. Iteration 35400/47167 Losses: train: 2.250901222229004, validate: 2.3373801708221436\n",
      "Epoch 9/10. Iteration 35500/47167 Losses: train: 2.3579812049865723, validate: 2.331601619720459\n",
      "Epoch 9/10. Iteration 35600/47167 Losses: train: 2.1380178928375244, validate: 2.3431472778320312\n",
      "Epoch 9/10. Iteration 35700/47167 Losses: train: 2.2107152938842773, validate: 2.335662603378296\n",
      "Epoch 9/10. Iteration 35800/47167 Losses: train: 2.2361068725585938, validate: 2.351480722427368\n",
      "Epoch 9/10. Iteration 35900/47167 Losses: train: 2.030322313308716, validate: 2.3441648483276367\n",
      "Epoch 9/10. Iteration 36000/47167 Losses: train: 2.2698869705200195, validate: 2.3502118587493896\n",
      "Epoch 9/10. Iteration 36100/47167 Losses: train: 2.1508522033691406, validate: 2.330394744873047\n",
      "Epoch 9/10. Iteration 36200/47167 Losses: train: 2.2714881896972656, validate: 2.3352346420288086\n",
      "Epoch 9/10. Iteration 36300/47167 Losses: train: 2.2911434173583984, validate: 2.3387792110443115\n",
      "Epoch 9/10. Iteration 36400/47167 Losses: train: 2.2998805046081543, validate: 2.3474130630493164\n",
      "Epoch 9/10. Iteration 36500/47167 Losses: train: 2.1636154651641846, validate: 2.3424715995788574\n",
      "Epoch 9/10. Iteration 36600/47167 Losses: train: 2.213876962661743, validate: 2.3429360389709473\n",
      "Epoch 9/10. Iteration 36700/47167 Losses: train: 2.225529193878174, validate: 2.3494787216186523\n",
      "Epoch 9/10. Iteration 36800/47167 Losses: train: 2.1713509559631348, validate: 2.3379647731781006\n",
      "Epoch 9/10. Iteration 36900/47167 Losses: train: 2.1251487731933594, validate: 2.3291051387786865\n",
      "Epoch 9/10. Iteration 37000/47167 Losses: train: 2.108242988586426, validate: 2.3449323177337646\n",
      "Epoch 9/10. Iteration 37100/47167 Losses: train: 2.121549367904663, validate: 2.33441424369812\n",
      "Epoch 9/10. Iteration 37200/47167 Losses: train: 2.129859209060669, validate: 2.348322629928589\n",
      "Epoch 9/10. Iteration 37300/47167 Losses: train: 2.364408016204834, validate: 2.3241944313049316\n",
      "Epoch 9/10. Iteration 37400/47167 Losses: train: 2.3537497520446777, validate: 2.3523285388946533\n",
      "Epoch 9/10. Iteration 37500/47167 Losses: train: 2.4066758155822754, validate: 2.340184450149536\n",
      "Epoch 9/10. Iteration 37600/47167 Losses: train: 2.1250710487365723, validate: 2.3362984657287598\n",
      "Epoch 9/10. Iteration 37700/47167 Losses: train: 2.292511224746704, validate: 2.347799777984619\n",
      "Epoch 9/10. Iteration 37800/47167 Losses: train: 2.314136028289795, validate: 2.3337337970733643\n",
      "Epoch 9/10. Iteration 37900/47167 Losses: train: 2.1641764640808105, validate: 2.347254991531372\n",
      "Epoch 9/10. Iteration 38000/47167 Losses: train: 2.203562021255493, validate: 2.3401904106140137\n",
      "Epoch 9/10. Iteration 38100/47167 Losses: train: 1.9758319854736328, validate: 2.338705539703369\n",
      "Epoch 9/10. Iteration 38200/47167 Losses: train: 2.2237486839294434, validate: 2.3431615829467773\n",
      "Epoch 9/10. Iteration 38300/47167 Losses: train: 2.2180869579315186, validate: 2.345346212387085\n",
      "Epoch 9/10. Iteration 38400/47167 Losses: train: 2.0643231868743896, validate: 2.3381526470184326\n",
      "Epoch 9/10. Iteration 38500/47167 Losses: train: 2.3033103942871094, validate: 2.3283779621124268\n",
      "Epoch 9/10. Iteration 38600/47167 Losses: train: 2.2474865913391113, validate: 2.3422670364379883\n",
      "Epoch 9/10. Iteration 38700/47167 Losses: train: 2.2183542251586914, validate: 2.3306376934051514\n",
      "Epoch 9/10. Iteration 38800/47167 Losses: train: 2.003331184387207, validate: 2.344557762145996\n",
      "Epoch 9/10. Iteration 38900/47167 Losses: train: 2.222519874572754, validate: 2.343975782394409\n",
      "Epoch 9/10. Iteration 39000/47167 Losses: train: 2.221942901611328, validate: 2.3340981006622314\n",
      "Epoch 9/10. Iteration 39100/47167 Losses: train: 2.114421844482422, validate: 2.3194026947021484\n",
      "Epoch 9/10. Iteration 39200/47167 Losses: train: 2.057981252670288, validate: 2.338020086288452\n",
      "Epoch 9/10. Iteration 39300/47167 Losses: train: 2.2522504329681396, validate: 2.346353769302368\n",
      "Epoch 9/10. Iteration 39400/47167 Losses: train: 2.1664605140686035, validate: 2.3393876552581787\n",
      "Epoch 9/10. Iteration 39500/47167 Losses: train: 2.140315532684326, validate: 2.347724676132202\n",
      "Epoch 9/10. Iteration 39600/47167 Losses: train: 2.223618507385254, validate: 2.337674379348755\n",
      "Epoch 9/10. Iteration 39700/47167 Losses: train: 2.0168473720550537, validate: 2.350121021270752\n",
      "Epoch 9/10. Iteration 39800/47167 Losses: train: 2.2942399978637695, validate: 2.3381357192993164\n",
      "Epoch 9/10. Iteration 39900/47167 Losses: train: 2.065701723098755, validate: 2.3397057056427\n",
      "Epoch 9/10. Iteration 40000/47167 Losses: train: 2.0373306274414062, validate: 2.3308334350585938\n",
      "Epoch 9/10. Iteration 40100/47167 Losses: train: 2.0997397899627686, validate: 2.3480539321899414\n",
      "Epoch 9/10. Iteration 40200/47167 Losses: train: 2.145005702972412, validate: 2.3519887924194336\n",
      "Epoch 9/10. Iteration 40300/47167 Losses: train: 2.2707765102386475, validate: 2.3503241539001465\n",
      "Epoch 9/10. Iteration 40400/47167 Losses: train: 2.370115280151367, validate: 2.341813087463379\n",
      "Epoch 9/10. Iteration 40500/47167 Losses: train: 2.166173219680786, validate: 2.3370614051818848\n",
      "Epoch 9/10. Iteration 40600/47167 Losses: train: 2.081763744354248, validate: 2.3391005992889404\n",
      "Epoch 9/10. Iteration 40700/47167 Losses: train: 2.2496519088745117, validate: 2.332655668258667\n",
      "Epoch 9/10. Iteration 40800/47167 Losses: train: 1.9904357194900513, validate: 2.3390817642211914\n",
      "Epoch 9/10. Iteration 40900/47167 Losses: train: 2.1731390953063965, validate: 2.3364639282226562\n",
      "Epoch 9/10. Iteration 41000/47167 Losses: train: 2.0868773460388184, validate: 2.333674669265747\n",
      "Epoch 9/10. Iteration 41100/47167 Losses: train: 2.0829036235809326, validate: 2.34181809425354\n",
      "Epoch 9/10. Iteration 41200/47167 Losses: train: 2.260601282119751, validate: 2.3446900844573975\n",
      "Epoch 9/10. Iteration 41300/47167 Losses: train: 2.084613084793091, validate: 2.3410580158233643\n",
      "Epoch 9/10. Iteration 41400/47167 Losses: train: 2.0701959133148193, validate: 2.3326284885406494\n",
      "Epoch 9/10. Iteration 41500/47167 Losses: train: 2.2235751152038574, validate: 2.340111255645752\n",
      "Epoch 9/10. Iteration 41600/47167 Losses: train: 2.3023135662078857, validate: 2.3436203002929688\n",
      "Epoch 9/10. Iteration 41700/47167 Losses: train: 2.216160297393799, validate: 2.3409292697906494\n",
      "Epoch 9/10. Iteration 41800/47167 Losses: train: 2.3411948680877686, validate: 2.3326809406280518\n",
      "Epoch 9/10. Iteration 41900/47167 Losses: train: 2.154282569885254, validate: 2.3499040603637695\n",
      "Epoch 9/10. Iteration 42000/47167 Losses: train: 2.2721805572509766, validate: 2.346870183944702\n",
      "Epoch 9/10. Iteration 42100/47167 Losses: train: 2.1529946327209473, validate: 2.3371167182922363\n",
      "Epoch 9/10. Iteration 42200/47167 Losses: train: 2.395345687866211, validate: 2.336840867996216\n",
      "Epoch 9/10. Iteration 42300/47167 Losses: train: 2.2634968757629395, validate: 2.33642315864563\n",
      "Epoch 9/10. Iteration 42400/47167 Losses: train: 1.99549400806427, validate: 2.3344383239746094\n",
      "Epoch 9/10. Iteration 42500/47167 Losses: train: 2.136077642440796, validate: 2.335800886154175\n",
      "Epoch 9/10. Iteration 42600/47167 Losses: train: 2.1541943550109863, validate: 2.3417699337005615\n",
      "Epoch 9/10. Iteration 42700/47167 Losses: train: 2.095400094985962, validate: 2.339359760284424\n",
      "Epoch 9/10. Iteration 42800/47167 Losses: train: 2.1334633827209473, validate: 2.3357393741607666\n",
      "Epoch 9/10. Iteration 42900/47167 Losses: train: 2.080942153930664, validate: 2.347886323928833\n",
      "Epoch 9/10. Iteration 43000/47167 Losses: train: 2.2058064937591553, validate: 2.3440847396850586\n",
      "Epoch 9/10. Iteration 43100/47167 Losses: train: 2.2500858306884766, validate: 2.343045949935913\n",
      "Epoch 9/10. Iteration 43200/47167 Losses: train: 2.185258150100708, validate: 2.3427340984344482\n",
      "Epoch 9/10. Iteration 43300/47167 Losses: train: 2.202664375305176, validate: 2.3329415321350098\n",
      "Epoch 9/10. Iteration 43400/47167 Losses: train: 2.164053440093994, validate: 2.3417038917541504\n",
      "Epoch 9/10. Iteration 43500/47167 Losses: train: 2.1955323219299316, validate: 2.3436005115509033\n",
      "Epoch 9/10. Iteration 43600/47167 Losses: train: 2.337658405303955, validate: 2.3349735736846924\n",
      "Epoch 9/10. Iteration 43700/47167 Losses: train: 1.989180088043213, validate: 2.3423190116882324\n",
      "Epoch 9/10. Iteration 43800/47167 Losses: train: 2.05897855758667, validate: 2.332742691040039\n",
      "Epoch 9/10. Iteration 43900/47167 Losses: train: 2.199411630630493, validate: 2.3405065536499023\n",
      "Epoch 9/10. Iteration 44000/47167 Losses: train: 2.026524543762207, validate: 2.344449996948242\n",
      "Epoch 9/10. Iteration 44100/47167 Losses: train: 2.174673080444336, validate: 2.340907096862793\n",
      "Epoch 9/10. Iteration 44200/47167 Losses: train: 2.315016508102417, validate: 2.344358205795288\n",
      "Epoch 9/10. Iteration 44300/47167 Losses: train: 2.1720123291015625, validate: 2.3435587882995605\n",
      "Epoch 9/10. Iteration 44400/47167 Losses: train: 2.1373250484466553, validate: 2.346714496612549\n",
      "Epoch 9/10. Iteration 44500/47167 Losses: train: 2.2357940673828125, validate: 2.3494198322296143\n",
      "Epoch 9/10. Iteration 44600/47167 Losses: train: 2.159132957458496, validate: 2.3337533473968506\n",
      "Epoch 9/10. Iteration 44700/47167 Losses: train: 2.097219228744507, validate: 2.3489696979522705\n",
      "Epoch 9/10. Iteration 44800/47167 Losses: train: 2.1303586959838867, validate: 2.34198260307312\n",
      "Epoch 9/10. Iteration 44900/47167 Losses: train: 2.1199305057525635, validate: 2.3375048637390137\n",
      "Epoch 9/10. Iteration 45000/47167 Losses: train: 2.1334307193756104, validate: 2.3506252765655518\n",
      "Epoch 9/10. Iteration 45100/47167 Losses: train: 2.12835431098938, validate: 2.34089994430542\n",
      "Epoch 9/10. Iteration 45200/47167 Losses: train: 2.1231331825256348, validate: 2.3405566215515137\n",
      "Epoch 9/10. Iteration 45300/47167 Losses: train: 2.1376585960388184, validate: 2.3408334255218506\n",
      "Epoch 9/10. Iteration 45400/47167 Losses: train: 1.9854505062103271, validate: 2.3482048511505127\n",
      "Epoch 9/10. Iteration 45500/47167 Losses: train: 2.200021505355835, validate: 2.355213165283203\n",
      "Epoch 9/10. Iteration 45600/47167 Losses: train: 2.231443166732788, validate: 2.360255479812622\n",
      "Epoch 9/10. Iteration 45700/47167 Losses: train: 2.1359832286834717, validate: 2.337339401245117\n",
      "Epoch 9/10. Iteration 45800/47167 Losses: train: 2.382211446762085, validate: 2.348134756088257\n",
      "Epoch 9/10. Iteration 45900/47167 Losses: train: 2.3422904014587402, validate: 2.3467934131622314\n",
      "Epoch 9/10. Iteration 46000/47167 Losses: train: 2.0267603397369385, validate: 2.342270612716675\n",
      "Epoch 9/10. Iteration 46100/47167 Losses: train: 2.20656156539917, validate: 2.339794397354126\n",
      "Epoch 9/10. Iteration 46200/47167 Losses: train: 2.098789691925049, validate: 2.349011182785034\n",
      "Epoch 9/10. Iteration 46300/47167 Losses: train: 2.239248514175415, validate: 2.343808650970459\n",
      "Epoch 9/10. Iteration 46400/47167 Losses: train: 2.142329454421997, validate: 2.335578441619873\n",
      "Epoch 9/10. Iteration 46500/47167 Losses: train: 2.125680685043335, validate: 2.3408050537109375\n",
      "Epoch 9/10. Iteration 46600/47167 Losses: train: 2.349656105041504, validate: 2.3524036407470703\n",
      "Epoch 9/10. Iteration 46700/47167 Losses: train: 2.231367826461792, validate: 2.3479554653167725\n",
      "Epoch 9/10. Iteration 46800/47167 Losses: train: 2.1862337589263916, validate: 2.3519790172576904\n",
      "Epoch 9/10. Iteration 46900/47167 Losses: train: 2.272918701171875, validate: 2.3374807834625244\n",
      "Epoch 9/10. Iteration 47000/47167 Losses: train: 2.154945135116577, validate: 2.348330020904541\n",
      "Epoch 9/10. Iteration 47100/47167 Losses: train: 2.2542102336883545, validate: 2.334228515625\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.cuda.empty_cache()\n",
    "model = TransformetGenerator(dim, len(word_to_id_dict), len(word_to_id_dict), bptt, embs=emb_tensor).to(device)\n",
    "optimizer = opt.Adam(model.parameters())\n",
    "def loss(res, b):\n",
    "  x = b[:, 1:].view(b.shape[0], b.shape[1] - 1, -1).to(device)\n",
    "  from_ones = torch.gather(res[:, :-1, :], 2, x)\n",
    "  los = (- torch.sum((1 - from_ones).log()) + torch.sum((1 - res[:, :-1, :]).log()))\n",
    "  err = - los / (torch.tensor(b.shape[0] * (b.shape[1] - 1) * res.shape[2]).to(device)) - torch.sum(from_ones.log()) / b.shape[0] / (b.shape[1] - 1)\n",
    "  return err\n",
    "\n",
    "train_losses, validate_losses, test_loss = learn(device, model, loss, 10, 100, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "gcjhLQdLNy-9",
    "outputId": "7a66d4ba-d58c-4db1-fcba-fbf9660fcb80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood by iteration.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f336d05acc0>,\n",
       " <matplotlib.lines.Line2D at 0x7f336d2be828>]"
      ]
     },
     "execution_count": 132,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAD4CAYAAAD2FnFTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQIiISFb/H7b1gWbIrCgOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Likelihood by iteration.\")\n",
    "plt.yscale('log')\n",
    "plt.plot(range(0, len(train_losses)), train_losses, 'r',\n",
    "         range(0, len(validate_losses)), validate_losses, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "IdxEeKZ135QU",
    "outputId": "cd917125-69bc-4d24-878e-2df692b4ad5a"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "words = 40\n",
    "ph = \"\"\n",
    "tr = 64\n",
    "print(ph, end=' ')\n",
    "sent = [word_to_id_dict[x] for x in \"<sos> {}\".format(ph).split()]\n",
    "for _ in range(words):\n",
    "  x = np.array(sent[-bptt + 1::] + [0]).reshape(1, min(len(sent) + 1, bptt))\n",
    "  x = torch.from_numpy(x).to(device)\n",
    "  probs = sorted(list(enumerate(model.forward(x, x)[0][-2].cpu().detach().numpy().tolist())), key=lambda x: x[1], reverse=True)[:tr]\n",
    "  ids = [x[0] for x in probs]\n",
    "  ps = [x[1] for x in probs]\n",
    "  tr = max(2, tr // 2)\n",
    "\n",
    "  word_id = random.choices(ids, ps)[0]\n",
    "\n",
    "  word = id_to_word_dict[word_id]\n",
    "  #if word_id != 0:\n",
    "  print(word, end=' ')\n",
    "  sent.append(word_id)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Generate_starts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
