{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/annikura/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import datetime as dt\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from enum import Enum\n",
    "from random import choices\n",
    "from random import randint\n",
    "import json\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_jokes_clean.json', 'r') as file:\n",
    "    json_jokes = json.load(file)['posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_to_tokens(text):\n",
    "    text = (' '.join(text.split(r'\\n'))).lower()\n",
    "    \n",
    "    while True:\n",
    "        new_s = re.sub(\"\\\\.\\\\.+\", \".\", text)\n",
    "        if new_s == text:\n",
    "            break\n",
    "        text = new_s\n",
    "    \n",
    "    word = ''\n",
    "    number = ''\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    is_whitespace = re.compile('\\s')\n",
    "    for sym in text:\n",
    "        if sym.isalpha():\n",
    "            if number:\n",
    "                tokens.append(number)\n",
    "                number = ''\n",
    "            word += sym\n",
    "            continue\n",
    "        if sym.isnumeric():\n",
    "            if word:\n",
    "                tokens.append(word)\n",
    "                word = ''\n",
    "            number += sym\n",
    "            continue\n",
    "        if number:\n",
    "                tokens.append(number)\n",
    "        if word:\n",
    "                tokens.append(word)\n",
    "        number, word = '', ''\n",
    "        if is_whitespace.match(sym) or sym == r'\\\\' or sym == \"'\":\n",
    "            continue\n",
    "        tokens.append(sym)\n",
    "    if number:\n",
    "                tokens.append(number)\n",
    "    if word:\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'hope',\n",
       " 'you',\n",
       " 're',\n",
       " 'all',\n",
       " 'getting',\n",
       " 'your',\n",
       " 'walter',\n",
       " 'cronkite',\n",
       " 'jokes',\n",
       " 'in',\n",
       " 'order',\n",
       " '.',\n",
       " 'he',\n",
       " 's',\n",
       " 'next',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_to_tokens(\"I hope you're all getting your Walter Cronkite jokes in order...... \\nHe's next.. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = defaultdict(int)\n",
    "\n",
    "for joke in json_jokes:\n",
    "    if len(joke['title']) + len(joke['text']) < 100:\n",
    "        text = process_to_tokens(joke['title'] + \" \" + joke['text'])\n",
    "        for word in text:\n",
    "            if word not in english_stopwords and word.isalpha() and len(word) > 4:\n",
    "                words[word] += 1        \n",
    "tags = dict(list(sorted(words.items(), key=lambda x: x[1], reverse=True))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes = []\n",
    "\n",
    "archive = {} \n",
    "\n",
    "words = defaultdict(int)\n",
    "\n",
    "for joke in json_jokes:\n",
    "    if len(joke['title']) + len(joke['text']) < 300 and \"edit :\" not in joke['text']:\n",
    "        text = process_to_tokens(joke['title'] + \" \" + joke['text'])\n",
    "        archive[' '.join(text)] = joke\n",
    "        jokes.append((joke['tag'], text))\n",
    "        for tag in tags:\n",
    "            if tag in text:\n",
    "                jokes.append((tag, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    def __init__(self):\n",
    "        self.cnt = 0\n",
    "        self.d = {}\n",
    "        self.rev_d = {}\n",
    "    \n",
    "    def add_tokens(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token not in self.d:\n",
    "                self.d[token] = self.cnt\n",
    "                self.rev_d[self.cnt] = token\n",
    "                self.cnt += 1\n",
    "                \n",
    "    def get_id(self, token):\n",
    "        return self.d[token]\n",
    "    \n",
    "    def get_token(self, i):\n",
    "        return self.rev_d[i]\n",
    "    \n",
    "    def get_cnt(self):\n",
    "        return self.cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary()\n",
    "\n",
    "for _, joke in jokes:\n",
    "    dictionary.add_tokens(joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25243"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.get_cnt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialTokens(Enum):\n",
    "    START = 0,\n",
    "    END = 1\n",
    "    \n",
    "    \n",
    "class UniGrammGenerator:\n",
    "    def __init__(self):\n",
    "        self.table = defaultdict(lambda: defaultdict(int))\n",
    "        self.cnt = 0\n",
    "        \n",
    "    def generate_next_word(self, context, _):\n",
    "        possible_words = self.table[context]\n",
    "        return choices(list(possible_words.keys()), weights=list(possible_words.values()))\n",
    "        \n",
    "    def generate(self, context):\n",
    "        text = []\n",
    "        while(text[-1] != SpecialTokens.END):\n",
    "            new_word = self.generate_next_word(context, None)\n",
    "            text += new_word\n",
    "        return \" \".join(text[:-1])\n",
    "    \n",
    "    \n",
    "    def get_prob(self, context, last_n):\n",
    "        return self.cnt\n",
    "    \n",
    "    def add_ngram(self, context, ngram):\n",
    "        self.table[context][ngram] += 1\n",
    "        self.cnt += 1\n",
    "    \n",
    "    def add_ngrams(self, context, ngrams):\n",
    "        for ngram in ngrams:\n",
    "            self.add_ngram(context, ngram)\n",
    "    \n",
    "    def learn_one_text(self, context, text):\n",
    "        tokens = text + [SpecialTokens.END]\n",
    "        self.add_ngrams(context, tokens)\n",
    "    \n",
    "    def learn(self, data):\n",
    "        for context, text in data:\n",
    "            self.learn_one_text(context, text)\n",
    "            \n",
    "    \n",
    "class NGrammGenerator:\n",
    "    def __init__(self, N):\n",
    "        assert N > 1\n",
    "        self.N = N\n",
    "        self.table = defaultdict(lambda: defaultdict(int))\n",
    "        self.context_table = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "        self.context_sums = defaultdict(lambda: defaultdict(int))\n",
    "        self.context_weights = defaultdict(int)\n",
    "        self.dict = set()\n",
    "        self.d = Dictionary()\n",
    "        \n",
    "        self.worse = NGrammGenerator(N - 1) if N > 2 else UniGrammGenerator()\n",
    "        \n",
    "    def get_prob(self, context, last_n):\n",
    "        return self.context_sums[context][last_n]\n",
    "        \n",
    "    def generate_next_word(self, context, last_n):\n",
    "        context_size = self.context_sums[context][last_n] * 1000 // self.N\n",
    "        wider_size = self.worse.get_prob(context, last_n[:-1])\n",
    "        \n",
    "        if context_size + wider_size - 1 <= 0:\n",
    "            return self.worse.generate_next_word(context, last_n[:-1])\n",
    "        r = randint(0, context_size + wider_size - 1)\n",
    "        if r >= context_size:\n",
    "            return self.worse.generate_next_word(context, last_n[:-1])\n",
    "        possible_words = self.context_table[context][last_n]\n",
    "        return choices(list(possible_words.keys()), weights=list(possible_words.values()))\n",
    "        \n",
    "    def generate(self, context):\n",
    "        text = [SpecialTokens.START]\n",
    "        while(text[-1] != SpecialTokens.END):\n",
    "            last_n = text[-self.N + 1:][::-1]\n",
    "            last_n = last_n + (self.N - 1 - len(last_n)) * [None]\n",
    "            while True:\n",
    "                new_word = self.generate_next_word(context, tuple(last_n))\n",
    "                if new_word[0] == SpecialTokens.END or new_word[0].isalpha() or len(text) == 1 or text[-1].isalpha():\n",
    "                    break\n",
    "            text += new_word\n",
    "        return \" \".join(text[1:-1])\n",
    "    \n",
    "    def add_ngram(self, context, ngram):\n",
    "        self.table[ngram[1:]][ngram[0]] += 1\n",
    "        self.context_table[context][ngram[1:]][ngram[0]] += 1\n",
    "        self.context_sums[context][ngram[1:]] += 1\n",
    "        self.context_weights[context] += 1\n",
    "    \n",
    "    def add_ngrams(self, context, ngrams):\n",
    "        for ngram in ngrams:\n",
    "            self.add_ngram(context, ngram)\n",
    "    \n",
    "    def learn_one_text(self, context, text):\n",
    "        for word in text:\n",
    "            self.dict.add(word)\n",
    "        tokens = [SpecialTokens.START] + text + [SpecialTokens.END]\n",
    "        ngrams = []\n",
    "        for i in range(self.N):\n",
    "            ngrams.append([None] * i + tokens)\n",
    "        self.add_ngrams(context, zip(*ngrams))\n",
    "    \n",
    "    def learn(self, data):\n",
    "        self.worse.learn(data)\n",
    "        for context, text in data:\n",
    "            self.d.add_tokens(text)\n",
    "            self.learn_one_text(context, text)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = NGrammGenerator(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.learn(jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([None, 'jokes', 'would', 'difference', 'black', 'women', 'walks', 'never', 'people', 'always', 'favorite'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.context_table.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a black man walking along carrying a tv down the street . when we see a black guy walks into a bar . a black guy a mexican and a jew walk in to a bar the bartender sees the parrot and says , she got that from me , my wife on the other side .\n",
      "21 so my friend and i are walking down the street . when we see a black man across the road carrying a large , expensive looking television . i look over to my friend and say , \" holy shit dude ! is that yours ? \" and he says , \" no . mine is back home cleaning my shoes . \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '101iqt',\n",
       " 'tag': None,\n",
       " 'score': 17,\n",
       " 'title': 'So my friend and I are walking down the street...',\n",
       " 'text': 'when we see a black man across the road carrying a large, expensive looking television. I look over to my friend and say, \"Holy shit dude! Is that yours?\" And he says, \"No. Mine is back home cleaning my shoes.\"'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = 0\n",
    "\n",
    "while dist < 6:\n",
    "    joke = generator.generate('black')\n",
    "    f_joke = set(process_to_tokens(joke))\n",
    "    closest = ''\n",
    "    dist = 100000000\n",
    "\n",
    "    for _, j in jokes:\n",
    "        j_s = set(j)\n",
    "        d = min(len(j_s) + len(f_joke) - 2 * len(j_s.intersection(f_joke)), len(f_joke) - len(j_s.intersection(f_joke)))\n",
    "        if dist > d:\n",
    "            dist = d\n",
    "            closest = ' '.join(j)\n",
    "        \n",
    "print(joke)\n",
    "print(dist, closest)\n",
    "archive[closest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None:i hate the fucking eagles , man !\n",
      "None:how many mosquitoes does it take to change a lightbulb ? two , but i came here to get into ? the time between when i get home .\n",
      "None:why does waldo wear stripes ? because he was looking for .\n",
      "None:there â€™ s a new wheelchair party forming but it doesn t have any patients\n",
      "black:a black hole was telling me a story . it sucked me right in .\n",
      "None:my girlfriend left me i came home to find that complex branching pathways have been cut into his field ?\n",
      "None:i rang my boss and asked how he was proud that his son had sex . he was trying to get a comb from a policeman pulls him over . the stoned driver waits for it to burn some calories .\n",
      "never:why should you never trust an atom they make up everything .\n",
      "None:best asian joke im writing a speech for best friend s 21 st he is asian points for immaturity and vulgarity must be short help me r / askreddit / comments / skeh 8 i m funny i started my new job , my boss offered me a nice stable job . i want to go into rehab yesterday . he said , no hablo ingles .\n",
      "None:why are black people good at basketball ? because it had no guts !\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    name = choices(list(generator.context_weights.keys()), list(generator.context_weights.values()))[0]\n",
    "    print(\"{}:{}\".format(name, generator.generate(name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
